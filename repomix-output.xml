This file is a merged representation of the entire codebase, combined into a single document by Repomix.

<file_summary>
This section contains a summary of this file.

<purpose>
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.
</purpose>

<file_format>
The content is organized as follows:
1. This summary section
2. Repository information
3. Directory structure
4. Repository files (if enabled)
5. Multiple file entries, each consisting of:
  - File path as an attribute
  - Full contents of the file
</file_format>

<usage_guidelines>
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.
</usage_guidelines>

<notes>
- Some files may have been excluded based on .gitignore rules and Repomix's configuration
- Binary files are not included in this packed representation. Please refer to the Repository Structure section for a complete list of file paths, including binary files
- Files matching patterns in .gitignore are excluded
- Files matching default ignore patterns are excluded
- Files are sorted by Git change count (files with more changes are at the bottom)
</notes>

</file_summary>

<directory_structure>
.claude/
  commands/
    deep-research.md
    quick-research.md
  shared/
    constants/
      source_quality_ratings.md
    templates/
      citation_format.md
      report_structure.md
  skills/
    citation-validator/
      examples.md
      instructions.md
      SKILL.md
    got-controller/
      examples.md
      instructions.md
      SKILL.md
    question-refiner/
      examples.md
      instructions.md
      SKILL.md
    research-executor/
      examples.md
      instructions.md
      SKILL.md
    synthesizer/
      examples.md
      instructions.md
      SKILL.md
  settings.local.json
docs/
  reference/
    skills-guide.md
RESEARCH/
  .example/
    .gitkeep
scripts/
  preprocess_document.py
  url_manifest.py
  vector_store.py
ARCHITECTURE.md
CLAUDE.md
README.md
RESEARCH_METHODOLOGY.md
</directory_structure>

<files>
This section contains the contents of the repository's files.

<file path=".claude/commands/deep-research.md">
---
description: å¯¹æŒ‡å®šä¸»é¢˜æ‰§è¡Œå®Œæ•´çš„æ·±åº¦ç ”ç©¶æµç¨‹ï¼Œä»é—®é¢˜ç»†åŒ–åˆ°æœ€ç»ˆæŠ¥å‘Šç”Ÿæˆ
argument-hint: [ç ”ç©¶ä¸»é¢˜æˆ–é—®é¢˜]
allowed-tools: Task, WebSearch, WebFetch, mcp__web_reader__webReader, Read, Write, TodoWrite, mcp__zai-mcp-server__analyze_image, mcp__zai-mcp-server__analyze_data_visualization
---

# Deep Research Command

æ‰§è¡Œå®Œæ•´çš„ 7 é˜¶æ®µæ·±åº¦ç ”ç©¶æµç¨‹ï¼Œä½¿ç”¨ Graph of Thoughts æ¡†æ¶ä¼˜åŒ–ç ”ç©¶è´¨é‡ã€‚

## ç ”ç©¶ä¸»é¢˜

$ARGUMENTS

---

## å‰ç½®æ£€æŸ¥

### 1. å¢é‡ç ”ç©¶æ£€æµ‹

**é¦–å…ˆæ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨ç›¸å…³ç ”ç©¶ï¼š**

```bash
# æ£€æŸ¥ RESEARCH ç›®å½•
ls -la RESEARCH/ 2>/dev/null || echo "No existing research"
```

**å¦‚æœå­˜åœ¨ç›¸åŒä¸»é¢˜çš„ç ”ç©¶ï¼š**

| æ¨¡å¼ | æ“ä½œ | é€‚ç”¨åœºæ™¯ |
|------|------|----------|
| **Update** | åˆ·æ–°æœ€æ–°ä¿¡æ¯ï¼Œä¿æŒç»“æ„ | éœ€è¦æ›´æ–°æ•°æ® |
| **Expand** | æ·»åŠ æ–°å­ä¸»é¢˜ | éœ€è¦æ‰©å±•èŒƒå›´ |
| **Restart** | å½’æ¡£æ—§ç‰ˆæœ¬ï¼Œé‡æ–°å¼€å§‹ | éœ€è¦é‡æ–°ç ”ç©¶ |

è¯¢é—®ç”¨æˆ·é€‰æ‹©æ¨¡å¼åå†ç»§ç»­ã€‚

---

## ç ”ç©¶æµç¨‹ (7 é˜¶æ®µ)

### Phase 1: Question Refinement

ä½¿ç”¨ **question-refiner** æŠ€èƒ½æ‰§è¡Œï¼š

1. **ç ”ç©¶ç±»å‹æ£€æµ‹**
   - Exploratory: å½“å‰çŠ¶æ€ã€è¶‹åŠ¿ã€å…¨æ™¯
   - Comparative: X vs Y å¯¹æ¯”
   - Problem-Solving: è§£å†³æ–¹æ¡ˆå¯¼å‘
   - Forecasting: è¶‹åŠ¿é¢„æµ‹
   - Deep Dive: æŠ€æœ¯æ·±åº¦åˆ†æ

2. **æ¸è¿›å¼æé—®** (3-5 ä¸ªæ ¸å¿ƒé—®é¢˜)
   - å…·ä½“å…³æ³¨ç‚¹
   - è¾“å‡ºæ ¼å¼éœ€æ±‚
   - ç›®æ ‡å—ä¼—
   - èŒƒå›´é™åˆ¶

3. **ç”Ÿæˆç»“æ„åŒ–æç¤ºè¯**
   - TASK: æ¸…æ™°çš„ç ”ç©¶ç›®æ ‡
   - CONTEXT: ç ”ç©¶èƒŒæ™¯å’Œæ„ä¹‰
   - SPECIFIC_QUESTIONS: 3-7 ä¸ªå…·ä½“å­é—®é¢˜
   - KEYWORDS: æœç´¢å…³é”®è¯
   - CONSTRAINTS: æ—¶é—´ã€åœ°åŸŸã€æ¥æºç±»å‹
   - OUTPUT_FORMAT: äº¤ä»˜ç‰©è§„æ ¼

> ğŸ“‹ ç»“æ„åŒ–æç¤ºè¯è´¨é‡ç›®æ ‡: â‰¥ 8/10

---

### Phase 2: Research Planning

1. **åˆ†è§£ä¸»é¢˜** â†’ 3-7 ä¸ªå­ä¸»é¢˜
2. **ç”Ÿæˆæœç´¢ç­–ç•¥** â†’ æ¯ä¸ªå­ä¸»é¢˜ 3-5 ä¸ªæœç´¢æŸ¥è¯¢
3. **ç¡®å®šæ•°æ®æº** â†’ æ ¹æ®çº¦æŸé€‰æ‹©åˆé€‚æ¥æº
4. **å¤šæ™ºèƒ½ä½“éƒ¨ç½²ç­–ç•¥**

| ç ”ç©¶ç±»å‹ | å­ä¸»é¢˜æ•° | æ™ºèƒ½ä½“æ•° | æ¨¡å‹é€‰æ‹© |
|---------|---------|---------|---------|
| å¿«é€ŸæŸ¥è¯¢ | 1-2 | 2-3 | å…¨éƒ¨ haiku |
| æ ‡å‡†ç ”ç©¶ | 3-5 | 4-5 | 2 sonnet + 3 haiku |
| æ·±åº¦ç ”ç©¶ | 5-7 | 6-8 | 3-4 sonnet + å…¶ä½™ haiku |

**è¾“å‡º**: ç ”ç©¶è®¡åˆ’æ–‡æ¡£ â†’ ç­‰å¾…ç”¨æˆ·ç¡®è®¤åç»§ç»­

---

### Phase 3: Multi-Agent Research

**å¹¶è¡Œéƒ¨ç½²ç ”ç©¶æ™ºèƒ½ä½“ï¼š**

```
# å¿…é¡»åœ¨å•æ¬¡å“åº”ä¸­å¯åŠ¨æ‰€æœ‰æ™ºèƒ½ä½“
Task(agent_1, "Research aspect A: [å…·ä½“ç„¦ç‚¹]...")
Task(agent_2, "Research aspect B: [å…·ä½“ç„¦ç‚¹]...")
Task(agent_3, "Research aspect C: [å…·ä½“ç„¦ç‚¹]...")
Task(agent_4, "Cross-reference verification...")
```

**æ™ºèƒ½ä½“ç±»å‹ï¼š**

| ç±»å‹ | æ•°é‡ | èŒè´£ |
|------|------|------|
| Web Research | 3-5 | å½“å‰ä¿¡æ¯ã€è¶‹åŠ¿ã€æ–°é—» |
| Academic/Technical | 1-2 | è®ºæ–‡ã€æŠ€æœ¯è§„æ ¼ |
| Cross-Reference | 1 | äº‹å®æ ¸æŸ¥ã€éªŒè¯ |

**Token ä¼˜åŒ– (å…³é”®!)ï¼š**

1. WebFetch åç«‹å³ä¿å­˜åˆ° `data/raw/`
2. è¿è¡Œé¢„å¤„ç†è„šæœ¬æ¸…ç†å†…å®¹
3. ä» `data/processed/` è¯»å–æ¸…ç†åçš„å†…å®¹

> âš ï¸ ç¦æ­¢ç›´æ¥å°† WebFetch åŸå§‹å†…å®¹æ”¾å…¥ä¸Šä¸‹æ–‡

---

### Phase 4: Source Triangulation

ä½¿ç”¨ **citation-validator** æŠ€èƒ½æ‰§è¡Œï¼š

1. **ç¼–è¯‘æ‰€æœ‰å‘ç°**
2. **è¯†åˆ«å…±è¯†** (å¤šæºæ”¯æŒ = é«˜ç½®ä¿¡åº¦)
3. **æ ‡æ³¨çŸ›ç›¾**
4. **è¯„ä¼°æ¥æºè´¨é‡** (A-E è¯„çº§)

> ğŸ“‹ å‚è€ƒ `.claude/shared/constants/source_quality_ratings.md`

---

### Phase 5: Knowledge Synthesis

ä½¿ç”¨ **synthesizer** æŠ€èƒ½æ‰§è¡Œï¼š

1. **ç»„ç»‡å†…å®¹** â†’ æŒ‰ä¸»é¢˜åˆ†ç»„ï¼ŒéæŒ‰æ™ºèƒ½ä½“
2. **è§£å†³çŸ›ç›¾** â†’ æ•°å€¼å·®å¼‚ã€å› æœå£°æ˜ã€æ—¶é—´å˜åŒ–
3. **æ„å»ºå…±è¯†** â†’ å¼º/ä¸­/å¼±/æ— å…±è¯†
4. **åˆ›å»ºå™äº‹** â†’ é€»è¾‘æµç¨‹ï¼Œæ¸è¿›å¼æŠ«éœ²

**å¼•ç”¨è¦æ±‚ï¼š**
æ¯ä¸ªäº‹å®æ€§å£°æ˜å¿…é¡»åŒ…å«ï¼š
- Author/Organization
- Publication Date
- Source Title
- URL/DOI

> ğŸ“‹ å‚è€ƒ `.claude/shared/templates/citation_format.md`

---

### Phase 6: Quality Assurance

**éªŒè¯é“¾ (Chain-of-Verification)ï¼š**

1. ä¸ºæ¯ä¸ªå…³é”®å£°æ˜ç”ŸæˆéªŒè¯é—®é¢˜
2. ç‹¬ç«‹æœç´¢éªŒè¯
3. äº¤å‰å¼•ç”¨ç»“æœ

**è´¨é‡æ£€æŸ¥æ¸…å•ï¼š**

- [ ] æ¯ä¸ªå£°æ˜éƒ½æœ‰å¯éªŒè¯æ¥æº
- [ ] å¤šä¸ªæ¥æºæ”¯æŒå…³é”®å‘ç°
- [ ] çŸ›ç›¾å·²ç¡®è®¤å¹¶è§£é‡Š
- [ ] æ¥æºæœ€æ–°ä¸”æƒå¨
- [ ] æ— å¹»è§‰æˆ–æ— æ”¯æŒå£°æ˜
- [ ] å¼•ç”¨æ ¼å¼ä¸€è‡´
- [ ] æ‰€æœ‰ URL å¯è®¿é—®

**è´¨é‡ç›®æ ‡ï¼š** â‰¥ 8/10

---

### Phase 7: Output Generation

**åˆ›å»ºè¾“å‡ºç›®å½•ç»“æ„ï¼š**

```
RESEARCH/[topic]/
â”œâ”€â”€ README.md                    # æ¦‚è¿°å’Œå¯¼èˆª
â”œâ”€â”€ executive_summary.md         # 1-2 é¡µæ‘˜è¦
â”œâ”€â”€ full_report.md               # å®Œæ•´æŠ¥å‘Š
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/                     # åŸå§‹è·å–å†…å®¹
â”‚   â”œâ”€â”€ processed/               # æ¸…ç†åå†…å®¹
â”‚   â””â”€â”€ statistics.md            # å…³é”®æ•°æ®
â”œâ”€â”€ sources/
â”‚   â”œâ”€â”€ bibliography.md          # å®Œæ•´å¼•ç”¨
â”‚   â”œâ”€â”€ source_quality_table.md  # A-E è¯„çº§
â”‚   â””â”€â”€ citation_validation.md   # éªŒè¯æŠ¥å‘Š
â”œâ”€â”€ research_notes/
â”‚   â””â”€â”€ agent_findings_summary.md
â””â”€â”€ appendices/
    â”œâ”€â”€ methodology.md
    â””â”€â”€ limitations.md
```

> ğŸ“‹ å‚è€ƒ `.claude/shared/templates/report_structure.md` é€‰æ‹©åˆé€‚æ¨¡æ¿

---

## Graph of Thoughts (å¯é€‰å¢å¼º)

å¯¹äºå¤æ‚ä¸»é¢˜ï¼Œå¯ç”¨ GoT æ§åˆ¶å™¨ï¼š

**GoT æ“ä½œï¼š**

| æ“ä½œ | ç”¨é€” | è§¦å‘æ¡ä»¶ |
|------|------|----------|
| Generate(k) | åˆ›å»º k ä¸ªå¹¶è¡Œç ”ç©¶è·¯å¾„ | åˆå§‹æ¢ç´¢æˆ–æ·±å…¥é«˜åˆ†èŠ‚ç‚¹ |
| Aggregate(k) | åˆå¹¶ k ä¸ªå‘ç°ä¸ºç»¼åˆ | 2-3 è½®ç”Ÿæˆå |
| Refine(1) | æ”¹è¿›ç°æœ‰å‘ç° | åˆ†æ•° â‰¥ 6.0 éœ€æ‰“ç£¨ |
| Score | è¯„ä¼°è´¨é‡ (0-10) | æ¯ä¸ªèŠ‚ç‚¹ |
| KeepBestN(n) | ä¿ç•™å‰ n ä¸ªèŠ‚ç‚¹ | ç®¡ç†å¤æ‚åº¦ |

**å†³ç­–çŸ©é˜µï¼š**

| åˆ†æ•° | æ“ä½œ | åŸå›  |
|------|------|------|
| â‰¥ 8.5 | Generate(2-3) | é«˜è´¨é‡è·¯å¾„å€¼å¾—æ·±å…¥ |
| 7.0-8.4 | Refine(1) | å†…å®¹è‰¯å¥½ï¼Œéœ€æ‰“ç£¨ |
| 6.0-6.9 | Aggregate | ä¸­ç­‰è´¨é‡ï¼Œåˆå¹¶æå‡ |
| < 6.0 | Prune | ä½è´¨é‡ï¼Œä¸¢å¼ƒ |

---

## å¼•ç”¨è¦æ±‚

ç¡®ä¿æ¯ä¸ªäº‹å®æ€§å£°æ˜åŒ…å«ï¼š

1. âœ… Author/Organization name
2. âœ… Publication date
3. âœ… Source title
4. âœ… Direct URL/DOI
5. âœ… Page numbers (å¦‚é€‚ç”¨)

---

## æˆåŠŸæŒ‡æ ‡

| æŒ‡æ ‡ | ç›®æ ‡ |
|------|------|
| å¼•ç”¨è¦†ç›–ç‡ | 100% |
| å¼•ç”¨å®Œæ•´æ€§ | 100% |
| å¼•ç”¨å‡†ç¡®æ€§ | â‰¥ 95% |
| æ¥æºè´¨é‡å¹³å‡ | B æˆ–æ›´é«˜ |
| å¹»è§‰æ•°é‡ | 0 |
| æ•´ä½“è´¨é‡åˆ†æ•° | â‰¥ 8/10 |

---

**å¼€å§‹æ·±åº¦ç ”ç©¶æµç¨‹ã€‚**
</file>

<file path=".claude/commands/quick-research.md">
---
description: å¯¹ç®€å•ä¸»é¢˜æ‰§è¡Œå¿«é€Ÿç ”ç©¶ï¼Œé€‚ç”¨äº 1-2 ä¸ªå­ä¸»é¢˜çš„ç®€å•æŸ¥è¯¢
argument-hint: [ç®€å•ç ”ç©¶é—®é¢˜]
allowed-tools: WebSearch, WebFetch, mcp__web_reader__webReader, Read, Write
---

# Quick Research Command

å¿«é€Ÿç ”ç©¶æ¨¡å¼ï¼Œé€‚ç”¨äºç®€å•ã€æ˜ç¡®çš„ç ”ç©¶é—®é¢˜ã€‚

## ç ”ç©¶é—®é¢˜

$ARGUMENTS

---

## é€‚ç”¨åœºæ™¯

âœ… é€‚åˆä½¿ç”¨ Quick Researchï¼š
- å•ä¸€ã€æ˜ç¡®çš„é—®é¢˜
- åªéœ€ 1-2 ä¸ªæ•°æ®æ¥æº
- ä¸éœ€è¦æ·±å…¥å¯¹æ¯”åˆ†æ
- æ—¶é—´æ•æ„Ÿï¼Œéœ€è¦å¿«é€Ÿç»“æœ

âŒ åº”è¯¥ä½¿ç”¨ Deep Researchï¼š
- å¤æ‚ã€å¤šæ–¹é¢çš„ä¸»é¢˜
- éœ€è¦å¤šæºäº¤å‰éªŒè¯
- éœ€è¦è¯¦ç»†å¯¹æ¯”åˆ†æ
- é«˜é£é™©å†³ç­–æ”¯æŒ

---

## å¿«é€Ÿç ”ç©¶æµç¨‹

### Step 1: ç†è§£é—®é¢˜

å¿«é€Ÿç¡®è®¤ï¼š
1. ç”¨æˆ·æƒ³è¦äº†è§£ä»€ä¹ˆï¼Ÿ
2. éœ€è¦ä»€ä¹ˆç±»å‹çš„ä¿¡æ¯ï¼Ÿ(äº‹å®ã€æ•°æ®ã€è§‚ç‚¹)
3. æœ‰ä»€ä¹ˆå…·ä½“çº¦æŸï¼Ÿ(æ—¶é—´ã€åœ°åŸŸ)

å¦‚æœé—®é¢˜æ¨¡ç³Šï¼Œè¯¢é—® 1-2 ä¸ªæ¾„æ¸…é—®é¢˜ã€‚

---

### Step 2: æœç´¢ä¿¡æ¯

ä½¿ç”¨ WebSearch æœç´¢ï¼š
- ç”Ÿæˆ 2-3 ä¸ªæœç´¢æŸ¥è¯¢
- ä¼˜å…ˆé€‰æ‹©æƒå¨æ¥æº (A-B çº§)
- æ”¶é›† 3-5 ä¸ªç›¸å…³æ¥æº

---

### Step 3: æå–å†…å®¹

ä½¿ç”¨ WebFetch æˆ– mcp__web_reader__webReaderï¼š
- æå–å…³é”®ä¿¡æ¯
- è®°å½•æ¥æºå’Œæ—¥æœŸ
- æ ‡æ³¨ä¿¡æ¯è´¨é‡

---

### Step 4: ç»„ç»‡å›ç­”

**è¾“å‡ºæ ¼å¼ï¼š**

```markdown
## [é—®é¢˜ç®€è¿°]

### ç­”æ¡ˆæ‘˜è¦

[ç›´æ¥å›ç­”é—®é¢˜çš„ 1-2 æ®µæ–‡å­—]

### å…³é”®ä¿¡æ¯

1. **[ä¿¡æ¯ç‚¹ 1]** - [è¯´æ˜] ([æ¥æº](URL))
2. **[ä¿¡æ¯ç‚¹ 2]** - [è¯´æ˜] ([æ¥æº](URL))
3. **[ä¿¡æ¯ç‚¹ 3]** - [è¯´æ˜] ([æ¥æº](URL))

### æ¥æº

| æ¥æº | ç±»å‹ | è´¨é‡ | URL |
|------|------|------|-----|
| [åç§°] | [ç±»å‹] | [A-E] | [é“¾æ¥] |

### æ³¨æ„äº‹é¡¹

[ä»»ä½•é‡è¦çš„è­¦å‘Šã€é™åˆ¶æˆ–éœ€è¦è¿›ä¸€æ­¥ç ”ç©¶çš„æ–¹é¢]
```

---

## è´¨é‡æ ‡å‡†

| æŒ‡æ ‡ | å¿«é€Ÿç ”ç©¶ç›®æ ‡ |
|------|-------------|
| æ¥æºæ•°é‡ | 3-5 ä¸ª |
| æ¥æºè´¨é‡ | è‡³å°‘ 1 ä¸ª A-B çº§ |
| å¼•ç”¨è¦†ç›– | 100% äº‹å®æ€§å£°æ˜ |
| å“åº”æ—¶é—´ | 5-10 åˆ†é’Ÿ |

---

## å‡çº§åˆ°æ·±åº¦ç ”ç©¶

å¦‚æœåœ¨å¿«é€Ÿç ”ç©¶è¿‡ç¨‹ä¸­å‘ç°ï¼š
- ä¸»é¢˜æ¯”é¢„æœŸå¤æ‚
- æ¥æºä¹‹é—´å­˜åœ¨çŸ›ç›¾
- éœ€è¦æ›´å…¨é¢çš„åˆ†æ

å»ºè®®ç”¨æˆ·å‡çº§åˆ° `/deep-research` å‘½ä»¤ã€‚

---

**å¼€å§‹å¿«é€Ÿç ”ç©¶ã€‚**
</file>

<file path=".claude/shared/constants/source_quality_ratings.md">
# Source Quality Rating System

All research skills use this unified A-E rating system for source quality assessment.

## Rating Definitions

### A - Excellent Sources (Highest Confidence)

**Criteria**:
- Peer-reviewed journals with impact factor
- Meta-analyses and systematic reviews
- Randomized Controlled Trials (RCTs)
- Government regulatory bodies (FDA, EMA, WHO, etc.)
- Top-tier conferences (NeurIPS, ICML, CHI, etc.)

**Examples**: New England Journal of Medicine, Nature, Science, FDA documentation

**Use for**: Medical claims, scientific facts, regulatory requirements

---

### B - Good Sources (High Confidence)

**Criteria**:
- Cohort studies, case-control studies
- Clinical practice guidelines
- Reputable industry analysts (Gartner, Forrester, IDC, McKinsey)
- Government websites (CDC, NIH, etc.)
- Official technical documentation from major companies

**Examples**: JAMA Network, McKinsey research, Google AI Blog, AWS Documentation

**Use for**: Market data, technical specifications, industry trends

---

### C - Acceptable Sources (Moderate Confidence)

**Criteria**:
- Expert opinion pieces in reputable venues
- Case reports and case studies
- Mechanistic studies (theoretical)
- Company white papers (vendor-provided)
- Reputable news outlets with editorial standards

**Examples**: Harvard Business Review, Wired, Ars Technica, TechCrunch

**Use for**: Expert perspectives, emerging trends, case examples

---

### D - Weak Sources (Low Confidence)

**Criteria**:
- Preprints (not yet peer-reviewed)
- Conference abstracts only
- Preliminary research
- Blog posts without editorial oversight
- Crowdsourced content (Reddit, Quora)

**Examples**: arXiv preprints, Medium posts, Stack Overflow, individual blogs

**Use for**: Preliminary findings, community insights (with caveat)

---

### E - Very Poor Sources (Minimal Confidence)

**Criteria**:
- Anonymous content
- Content with clear bias/conflict of interest
- Outdated sources (>10 years old unless historical)
- Content from unknown publishers
- Broken or suspicious links
- Press releases without third-party verification

**Examples**: Personal blogs, PR Newswire (biased), content farms, promotional materials

**Use for**: Should generally be avoided; if used, must be verified

---

## Usage Guidelines

### Citation Requirements by Rating

| Rating | Minimum Sources | Verification Required |
|--------|-----------------|----------------------|
| A | 1 (authoritative alone) | URL accessibility |
| B | 1-2 | URL accessibility |
| C | 2+ corroborating | Cross-reference check |
| D | 3+ corroborating | Manual verification |
| E | Not recommended | Full verification + caveat |

### Confidence Mapping

| Source Mix | Overall Confidence |
|------------|-------------------|
| 3+ A/B sources | HIGH |
| 2 B sources | HIGH |
| 1 A/B + 2 C sources | MEDIUM |
| Only C/D sources | LOW |
| Any E sources | NEEDS VERIFICATION |

### Domain-Specific Adjustments

| Domain | Minimum Rating | Notes |
|--------|---------------|-------|
| Medical/Health | A-B required | Life-safety implications |
| Legal/Regulatory | A-B required | Compliance implications |
| Financial | B+ preferred | Fiduciary implications |
| Technical | C+ acceptable | Can verify experimentally |
| Market Research | B+ preferred | Business decisions |
| General Interest | C+ acceptable | Lower stakes |
</file>

<file path=".claude/shared/templates/citation_format.md">
# Citation Format Standards

All research outputs must follow these citation standards.

## Required Citation Elements

Every citation MUST include:

1. **Author/Organization** - Who created the content
2. **Publication Date** - When published (YYYY or YYYY-MM-DD)
3. **Source Title** - Name of the work
4. **URL/DOI** - Direct link to verify
5. **Page Numbers** - For PDFs and direct quotes (if applicable)

---

## Inline Citation Formats

### Academic/Research Papers

```
Short form (in text):
(Smith et al., 2023)
(Smith & Johnson, 2024, p. 145)

Full form (in bibliography):
Smith, J., Johnson, K., & Lee, M. (2023). "Title of Paper." 
Journal Name, 45(3), 140-156. https://doi.org/10.xxxx/xxxxx
```

### Industry Reports

```
Short form (in text):
(Gartner, 2024)
(McKinsey, 2024, "Cloud Computing Forecast")

Full form (in bibliography):
Gartner. (2024). "Cloud Computing Market Forecast, 2024." 
Retrieved [access date] from https://www.gartner.com/en/research/xxxxx
```

### Web Sources

```
Short form (in text):
(WHO, 2024, "Vaccine Guidelines")
(Google AI Blog, 2024)

Full form (in bibliography):
World Health Organization. (2024). "COVID-19 Vaccine Guidelines." 
Retrieved [access date] from https://www.who.int/xxxxx
```

### News Articles

```
Short form (in text):
(Author, Publication, 2024)
(Reuters, 2024)

Full form (in bibliography):
Author, N. (2024, Month Day). "Article Title." Publication Name. 
https://www.publication.com/article
```

---

## Bibliography Format

```markdown
## References

1. **Smith, J., Johnson, K., & Lee, M.** (2023). "Title of Paper." 
   Journal Name, 45(3), 140-156. 
   https://doi.org/10.xxxx/xxxxx
   [Quality Rating: A]

2. **Gartner.** (2024). "Cloud Computing Market Forecast, 2024." 
   https://www.gartner.com/en/research/xxxxx
   [Quality Rating: B]

3. **World Health Organization.** (2024). "COVID-19 Vaccine Guidelines." 
   Retrieved 2024-03-15 from https://www.who.int/xxxxx
   [Quality Rating: A]
```

---

## Common Citation Mistakes

### âŒ Bad Examples

```
"Studies show..." (NO SOURCE)
"According to research..." (NO SOURCE)
"Industry reports suggest..." (VAGUE)
(Smith, 2023) (MISSING URL)
```

### âœ… Good Examples

```
"According to Smith et al. (2023), the market grew 25% 
(https://doi.org/10.xxxx/xxxxx, p. 145)."

"Multiple industry reports estimate the 2023 market at 
approximately $22-23 billion (Grand View Research, 2024; 
MarketsandMarkets, 2024; Fortune Business Insights, 2024)."
```

---

## Special Cases

### Secondary Citations

When citing a source that cites another source:

```
According to the original study (Smith, 2020, as cited in Johnson, 2024)...

Note: Always try to find and cite the primary source directly.
```

### Multiple Sources for Same Claim

```
This finding is supported by multiple studies (Smith, 2023; 
Johnson, 2024; Lee et al., 2024), indicating strong consensus.
```

### Conflicting Sources

```
While Source A reports 25% growth (Smith, 2023), Source B 
suggests 18% growth (Johnson, 2024). This discrepancy may 
be due to different market definitions.
```

---

## Quality Validation Checklist

Before finalizing any research output:

- [ ] Every factual claim has a citation
- [ ] All citations include required elements (author, date, title, URL)
- [ ] URLs are accessible (not returning 404)
- [ ] Publication dates are accurate
- [ ] Citations actually support the claims made
- [ ] Quality ratings (A-E) are assigned to each source
- [ ] Bibliography is formatted consistently
</file>

<file path=".claude/shared/templates/report_structure.md">
# Research Report Structure Templates

Select the appropriate template based on research type.

---

## Template 1: Standard Research Report

**Use for**: General research, exploratory topics

```markdown
# [Research Topic]: Comprehensive Report

**Generated**: [Date]
**Research Method**: 7-Phase Deep Research with GoT
**Quality Score**: [X]/10

---

## Executive Summary

[1-2 page synthesis of key findings]

### Key Findings
1. **[Finding 1]**: [summary with citation]
2. **[Finding 2]**: [summary with citation]
3. **[Finding 3]**: [summary with citation]

### Recommendations
[Actionable recommendations based on findings]

---

## 1. Introduction

### 1.1 Background
[Context and importance of the research topic]

### 1.2 Scope
[What is covered and not covered]

### 1.3 Methodology
[Research approach and sources used]

---

## 2. [Theme 1]

### 2.1 Key Findings
[Main discoveries with citations]

### 2.2 Evidence Base
[Summary of sources and quality ratings]

### 2.3 Implications
[What this means for stakeholders]

---

## 3. [Theme 2]

[Same structure as Theme 1]

---

## 4. Analysis and Insights

### 4.1 Cross-Theme Insights
[Connections between themes]

### 4.2 Patterns and Trends
[Identified patterns across findings]

---

## 5. Gaps and Limitations

[What is unknown, needs further research]

---

## 6. Conclusions and Recommendations

### 6.1 Summary of Findings
[Brief recap]

### 6.2 Actionable Recommendations
[Prioritized list of actions]

---

## References

[Complete bibliography with quality ratings]
```

---

## Template 2: Comparative Analysis

**Use for**: X vs Y comparisons, vendor evaluations

```markdown
# [Topic A] vs [Topic B]: Comparative Analysis

**Generated**: [Date]

---

## Executive Summary

[Key comparison insights and recommendation]

---

## 1. Overview

### 1.1 [Topic A] Summary
[Brief description]

### 1.2 [Topic B] Summary
[Brief description]

---

## 2. Comparison Matrix

| Criterion | Topic A | Topic B | Advantage |
|-----------|---------|---------|-----------|
| [Criterion 1] | [details] | [details] | [A/B/Tie] |
| [Criterion 2] | [details] | [details] | [A/B/Tie] |
| [Criterion 3] | [details] | [details] | [A/B/Tie] |

---

## 3. Detailed Analysis by Criterion

### 3.1 [Criterion 1]

**Topic A**: [analysis with citations]
**Topic B**: [analysis with citations]
**Verdict**: [which is better and why]

### 3.2 [Criterion 2]

[Same structure]

---

## 4. Recommendations by Use Case

### Use Case 1: [Scenario]
**Best Choice**: [Topic A/B]
**Reasoning**: [explanation with citations]

### Use Case 2: [Scenario]
**Best Choice**: [Topic A/B]
**Reasoning**: [explanation with citations]

---

## 5. Decision Framework

[Flowchart or decision tree]

---

## 6. Conclusion

[Overall recommendation with caveats]

---

## References

[Complete bibliography]
```

---

## Template 3: Problem-Solution Report

**Use for**: "How to solve X" research

```markdown
# [Problem]: Analysis and Solutions

**Generated**: [Date]

---

## Executive Summary

[Problem definition and recommended solution]

---

## 1. Problem Statement

### 1.1 Problem Definition
[What is the problem?]

### 1.2 Impact Assessment
[Why does it matter? Quantified if possible]

### 1.3 Stakeholders Affected
[Who is impacted?]

---

## 2. Root Cause Analysis

### 2.1 Primary Causes
[Main causes with evidence]

### 2.2 Contributing Factors
[Secondary factors]

### 2.3 Cause-Effect Diagram
[Visual or textual representation]

---

## 3. Current Approaches

### 3.1 [Approach 1]
- **Description**: [with citations]
- **Effectiveness**: [data]
- **Limitations**: [issues]

### 3.2 [Approach 2]
[Same structure]

---

## 4. Recommended Solutions

### 4.1 Primary Recommendation
- **Solution**: [description]
- **Evidence of Effectiveness**: [citations]
- **Implementation Requirements**: [resources needed]
- **Expected Outcomes**: [quantified if possible]

### 4.2 Alternative Solution
[If primary not feasible]

---

## 5. Implementation Roadmap

| Phase | Timeline | Actions | Resources |
|-------|----------|---------|-----------|
| 1 | [dates] | [actions] | [resources] |
| 2 | [dates] | [actions] | [resources] |

---

## 6. Risk Assessment

| Risk | Probability | Impact | Mitigation |
|------|-------------|--------|------------|
| [Risk 1] | [H/M/L] | [H/M/L] | [strategy] |
| [Risk 2] | [H/M/L] | [H/M/L] | [strategy] |

---

## 7. Conclusion

[Summary and call to action]

---

## References

[Complete bibliography]
```

---

## Template 4: Market Research Report

**Use for**: Market analysis, competitive landscape

```markdown
# [Market/Industry]: Market Research Report

**Generated**: [Date]
**Coverage Period**: [dates]
**Geographic Scope**: [regions]

---

## Executive Summary

### Key Metrics
- **Market Size**: $[X]B ([year])
- **Growth Rate**: [X]% CAGR
- **Key Players**: [top 3-5]

### Key Findings
1. [Finding 1]
2. [Finding 2]
3. [Finding 3]

---

## 1. Market Overview

### 1.1 Market Definition
[What is included/excluded]

### 1.2 Market Size and Growth
[Current size, historical growth, projections]

### 1.3 Market Segmentation
[By geography, type, application, etc.]

---

## 2. Competitive Landscape

### 2.1 Market Share Analysis
| Company | Market Share | Revenue | Growth |
|---------|-------------|---------|--------|
| [Co. 1] | [X]% | $[X]B | [X]% |
| [Co. 2] | [X]% | $[X]B | [X]% |

### 2.2 Competitive Positioning
[Matrix or description]

### 2.3 Key Differentiators
[What sets leaders apart]

---

## 3. Industry Trends

### 3.1 Current Trends
[What's happening now]

### 3.2 Emerging Trends
[What's coming]

### 3.3 Disruptive Forces
[Potential disruptions]

---

## 4. Market Drivers and Barriers

### 4.1 Growth Drivers
1. [Driver 1] - [explanation with citation]
2. [Driver 2] - [explanation with citation]

### 4.2 Barriers and Challenges
1. [Barrier 1] - [explanation with citation]
2. [Barrier 2] - [explanation with citation]

---

## 5. Future Outlook

### 5.1 Projections (3-5 years)
[Market size, growth forecasts]

### 5.2 Scenario Analysis
- **Optimistic**: [scenario]
- **Base Case**: [scenario]
- **Pessimistic**: [scenario]

---

## 6. Strategic Recommendations

[For the target audience]

---

## References

[Complete bibliography]
```

---

## Output Folder Structure

All research outputs should follow this folder structure:

```
RESEARCH/[topic_name]/
â”œâ”€â”€ README.md                    # Navigation guide
â”œâ”€â”€ executive_summary.md         # 1-2 page summary
â”œâ”€â”€ full_report.md               # Comprehensive findings
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/                     # Raw fetched content
â”‚   â”œâ”€â”€ processed/               # Cleaned content
â”‚   â”œâ”€â”€ statistics.md            # Key numbers and facts
â”‚   â””â”€â”€ key_facts.md             # Extracted facts
â”œâ”€â”€ visuals/
â”‚   â””â”€â”€ descriptions.md          # Chart/graph descriptions
â”œâ”€â”€ sources/
â”‚   â”œâ”€â”€ bibliography.md          # Full citations
â”‚   â”œâ”€â”€ source_quality_table.md  # A-E ratings
â”‚   â””â”€â”€ citation_validation.md   # Validation report
â”œâ”€â”€ research_notes/
â”‚   â”œâ”€â”€ agent_findings_summary.md # Raw agent outputs
â”‚   â””â”€â”€ got_graph_state.md       # GoT state (if used)
â””â”€â”€ appendices/
    â”œâ”€â”€ methodology.md           # Research approach
    â””â”€â”€ limitations.md           # Known limitations
```
</file>

<file path=".claude/skills/citation-validator/examples.md">
# Citation Validator Skill - Examples

## Example 1: Validating AI Market Research Report

### Input Content

```markdown
# AI in Healthcare Market Report

The global AI in healthcare market reached $22.4 billion in 2023 and is projected
to grow at a CAGR of 37.5% through 2030 (Grand View Research, 2024).

Major hospitals using AI include Mayo Clinic, which reports 40% faster diagnoses
with their AI imaging tools (Mayo Clinic, 2024).

According to industry studies, AI diagnostic tools achieve 95% accuracy compared
to human radiologists (Smith et al., 2023).

The FDA has approved over 500 AI medical devices as of 2024 (FDA, 2024).
```

### Validation Report

```markdown
# Citation Validation Report

## Executive Summary
- **Total Claims Analyzed**: 4
- **Claims with Citations**: 4 (100%)
- **Complete Citations**: 2 (50%)
- **Accurate Citations**: 3 (75%)
- **Potential Hallucinations**: 1
- **Overall Quality Score**: 5.5/10

## Critical Issues (Immediate Action Required)

### âš  HALLUCINATION DETECTED
**Claim**: "The FDA has approved over 500 AI medical devices as of 2024"
**Citation**: (FDA, 2024)
**Issue**: Number is incorrect
**Verification**:
- FDA database shows: 692 AI/ML-enabled medical devices (as of Dec 2024)
- Claim says "over 500" which is technically correct but understated
- More accurate figure: 692 devices
**Correction Required**: Update to "692 AI/ML-enabled medical devices" with source

## Citation Completeness Issues

### Claim 1: AI market size and growth
**Claim**: "The global AI in healthcare market reached $22.4 billion in 2023..."
**Citation**: (Grand View Research, 2024)
**Completeness**: âœ— INCOMPLETE
**Missing**:
- Full report title
- URL/DOI to verify
- Page numbers
**Recommended Format**:
```

(Grand View Research, 2024, "AI in Healthcare Market Size, Share & Trends")
Full: Grand View Research. (2024). "Artificial Intelligence in Healthcare Market
Size, Share & Trends Analysis Report." Retrieved [date] from [URL]

```

### Claim 3: AI diagnostic accuracy
**Claim**: "AI diagnostic tools achieve 95% accuracy compared to human radiologists"
**Citation**: (Smith et al., 2023)
**Completeness**: âœ— INCOMPLETE
**Missing**:
- Full paper title
- Journal name
- DOI
- Specific study details (sample size, conditions)
**Investigation Needed**: "Smith et al." is too generic - need to identify actual paper

## Source Quality Assessment

| Citation | Type | Rating | Justification |
|----------|------|--------|---------------|
| Grand View Research (2024) | Industry Report | B | Reputable market research firm, but verify with additional sources |
| Mayo Clinic (2024) | Organizational | C | Self-reported data, potential bias, verify with independent studies |
| Smith et al. (2023) | Academic (?) | D | Incomplete citation, cannot verify quality until full details found |
| FDA (2024) | Government | A | Official FDA database, highest quality |

## Detailed Findings

### Claim 1: Market size $22.4B, CAGR 37.5%
**Citation**: (Grand View Research, 2024)
**Verification Status**: âœ“ ACCURATE
**Source Check**: Grand View Research "AI in Healthcare Market" report exists
**Figure Verification**: $22.4B for 2023 âœ“
**Growth Rate**: 37.5% CAGR âœ“
**Recommendation**: Add second source to corroborate (e.g., MarketsandMarkets or Fortune Business Insights)

### Claim 2: Mayo Clinic 40% faster diagnoses
**Citation**: (Mayo Clinic, 2024)
**Verification Status**: âš  NEEDS VERIFICATION
**Source Check**: Found Mayo Clinic press release about AI imaging
**Claim Verification**: 40% figure needs specific source
**Bias Warning**: This is self-reported data
**Recommendation**: Find independent study validating Mayo Clinic's claims or present as "self-reported"

### Claim 3: 95% diagnostic accuracy
**Citation**: (Smith et al., 2023)
**Verification Status**: âœ— CANNOT VERIFY
**Issue**: Citation too generic to locate specific paper
**Search Attempted**: "Smith AI diagnostic accuracy 2023" â†’ Multiple results, unclear which
**Action Required**: Find actual paper or remove claim
**Potential Matches** (need verification):
- Smith, A. et al. (2023). "Deep Learning for Medical Image Analysis." Nature.
- Smith, J. et al. (2023). "AI in Radiology: A Comprehensive Review." Lancet Digital Health.

### Claim 4: FDA approved 500+ AI devices
**Citation**: (FDA, 2024)
**Verification Status**: âš  INACCURATE NUMBER
**Actual Data**: FDA database shows 692 AI/ML-enabled medical devices (as of Dec 2024)
**Source**: FDA AI/ML-Based Medical Device Database
**Correction**: Update to "692 devices" or "nearly 700 devices"

## Recommendations (Prioritized)

### HIGH PRIORITY
1. **Fix FDA device count**: Update from 500+ to 692 with specific database link
2. **Verify Smith et al. citation**: Find complete citation or replace with verifiable source
3. **Add URL for Grand View Research**: Include direct link to report

### MEDIUM PRIORITY
4. **Corroborate market size data**: Add second source (e.g., Fortune Business Insights)
5. **Verify Mayo Clinic claim**: Find independent source or label as self-reported
6. **Add publication details for all sources**: Include full titles and URLs

### LOW PRIORITY
7. **Standardize citation format**: Use consistent format throughout document
8. **Add access dates**: For all web sources

## Corrected Bibliography

```markdown
Grand View Research. (2024). "Artificial Intelligence in Healthcare Market Size, Share &
Trends Analysis Report, 2024-2030." Retrieved December 2024 from https://www.grandviewresearch.com/
industry-analysis/artificial-intelligence-in-healthcare-market

Mayo Clinic. (2024). "AI-Powered Medical Imaging: Diagnostic Improvements." Mayo Clinic
Press Release. Retrieved December 2024 from [URL - NEEDED]
Note: Self-reported data, independent verification recommended.

[INCOMPLETE - NEEDS FULL CITATION]: Smith et al. (2023).
Action: Locate complete citation or replace with verifiable source.

U.S. Food and Drug Administration. (2024). "Artificial Intelligence and Machine Learning
(AI/ML)-Enabled Medical Devices." FDA Database. Retrieved December 2024 from
https://www.fda.gov/medical-devices/software-medical-device-samd/artificial-intelligence-and-machine-learning-aiml-enabled-medical-devices
```

## Quality Score Breakdown

- Citation Coverage: 3/3 = 100% (3 points)
- Citation Completeness: 2/4 = 50% (1 point)
- Citation Accuracy: 3/4 = 75% (2.25 points)
- Source Quality: Average rating = (B+C+D+A)/4 = (2+1.5+1+3)/4 = 1.875 (1.88 points)
- Hallucination Penalty: -1 (for inaccurate FDA figure)

**Total Score**: 3 + 1 + 2.25 + 1.88 - 1 = **6.13/10**

**Assessment**: FAIR - Significant citation quality issues that undermine credibility.

```

---

## Example 2: Validating Technical Blog Post

### Input Content
```markdown
# WebAssembly Performance Guide

WebAssembly provides near-native performance in web browsers. According to Mozilla
developers, WebAssembly runs at 90-95% of native code speed (Mozilla, 2023).

Major companies using WebAssembly in production include AutoCAD, Figma, and Google Earth
(WebAssembly.org, 2024).

Browser support for WebAssembly is universal, with 96% of global browsers supporting it
(Caniuse, 2024).
```

### Validation Report

```markdown
# Citation Validation Report

## Executive Summary
- **Total Claims Analyzed**: 3
- **Claims with Citations**: 3 (100%)
- **Complete Citations**: 2 (67%)
- **Accurate Citations**: 3 (100%)
- **Potential Hallucinations**: 0
- **Overall Quality Score**: 7.8/10

## Detailed Findings

### Claim 1: WebAssembly 90-95% native speed
**Citation**: (Mozilla, 2023)
**Completeness**: âœ— INCOMPLETE
**Missing**: Specific article title, URL
**Verification**: âœ“ CLAIM SUPPORTED
**Source Found**: Mozilla Developer Network blog on WebAssembly performance
**Accuracy**: 90-95% figure is cited correctly
**Quality Rating**: B (Mozilla is authoritative source)
**Recommendation**: Add full citation with URL

### Claim 2: Companies using WebAssembly
**Citation**: (WebAssembly.org, 2024)
**Completeness**: âœ“ COMPLETE
**Verification**: âœ“ ACCURATE
**Source**: WebAssembly.org case studies section
**Accuracy**: AutoCAD, Figma, Google Earth all listed âœ“
**Quality Rating**: C (Industry association site, acceptable but not independent verification)
**Recommendation**: Add company blog posts or press releases for stronger support

### Claim 3: 96% browser support
**Citation**: (Caniuse, 2024)
**Completeness**: âœ“ COMPLETE
**Verification**: âœ“ ACCURATE
**Source**: caniuse.com search for "WebAssembly"
**Accuracy**: 96.47% global support (rounds to 96%) âœ“
**Quality Rating**: A (Caniuse is authoritative for browser support data)
**No issues**: Citation is excellent

## Recommendations

### LOW PRIORITY (Quality Improvements)
1. Add Mozilla MDN specific article URL for Claim 1
2. Supplement company list (Claim 2) with independent sources
3. Consider adding WebAssembly Roadmap source for future features

## Overall Assessment

**GOOD** - All claims are accurate and properly cited. Minor improvements to citation
completeness would elevate this to excellent quality.
```

---

## Example 3: Detecting Hallucinations

### Input Content (Suspected Hallucinations)

```markdown
# AI Research Developments

A recent Harvard study proved that GPT-4 achieves 99.2% accuracy on medical board exams,
surpassing human doctors who average 87% (Chen et al., 2024).

OpenAI's latest model, GPT-5, was released in March 2024 and has 10 trillion parameters
(OpenAI, 2024).

According to Nature magazine, 83% of scientists believe AI will lead to human-level AGI
by 2027 (Nature, 2024).
```

### Validation Report

```markdown
# Citation Validation Report - CRITICAL ISSUES DETECTED

## âš  CRITICAL: MULTIPLE HALLUCINATIONS DETECTED

### Hallucination #1: GPT-4 medical board exam accuracy
**Claim**: "GPT-4 achieves 99.2% accuracy on medical board exams"
**Citation**: (Chen et al., 2024)
**Status**: âœ— FALSE / MISLEADING

**Investigation**:
- Searched for: "Chen GPT-4 medical board exam 2024"
- Found: Chen et al. (2023) - NOT 2024
- Actual finding: USMLE Step exams accuracy = 86.6% (NOT 99.2%)
- Real study: "Performance of ChatGPT on USMLE" (Gilson et al., 2023) - 86.6%

**Conclusion**: Both the percentage (99.2%) and citation are incorrect.
**Correction**: GPT-4 achieves approximately 86-87% on USMLE exams (not 99.2%)

### Hallucination #2: GPT-5 release
**Claim**: "GPT-5 was released in March 2024 with 10 trillion parameters"
**Citation**: (OpenAI, 2024)
**Status**: âœ— COMPLETELY FALSE

**Investigation**:
- OpenAI has NOT released GPT-5 as of December 2024
- No announcement of GPT-5 release date
- Parameter count claim (10 trillion) is unsubstantiated
- Current latest model: GPT-4o (released May 2024)

**Conclusion**: This entire claim is fabricated.
**Action Required**: REMOVE THIS CLAIM ENTIRELY

### Hallucination #3: Nature survey on AGI timeline
**Claim**: "83% of scientists believe AGI by 2027"
**Citation**: (Nature, 2024)
**Status**: âœ— MISATTRIBUTED / FALSE

**Investigation**:
- Searched Nature for AGI survey 2024
- Found: No such survey in Nature in 2024
- Closest match: AI Impacts survey (2023) - different question, different results
- Real finding: 50% of AI researchers believe HLMI (high-level machine intelligence) by 2058
- The 83% figure and 2027 date appear to be fabricated

**Conclusion**: Citation and figure are incorrect.
**Action Required**: Remove or correct with actual survey data

## Summary of Issues

| Claim | Status | Action Required |
|-------|--------|-----------------|
| GPT-4 medical exam accuracy | âœ— False data | Correct to ~86-87% with proper citation |
| GPT-5 release | âœ— Completely fabricated | REMOVE ENTIRELY |
| Scientists AGI prediction | âœ— False citation | Remove or find real survey |

## Recommendations

**IMMEDIATE ACTION REQUIRED**:
1. **DELETE** the GPT-5 claim entirely - it is completely false
2. **CORRECT** the GPT-4 medical exam accuracy with actual data
3. **REMOVE or REPLACE** the AGI survey claim with verifiable data

**CREDIBILITY ASSESSMENT**: This document contains multiple significant hallucinations
that completely undermine its credibility. The author appears to have fabricated data,
figures, and citations.

**OVERALL QUALITY SCORE**: 0.5/10 (only because 2 of 3 claims have some basis in reality)

**RECOMMENDATION**: Do not publish without complete rewrite with accurate citations.
```

---

## Example 4: Validating Academic Literature Review

### Input Content (Well-Cited)

```markdown
# Transformer Architecture Literature Review

The transformer architecture was introduced in "Attention Is All You Need" by Vaswani et al.
(2017), published at NeurIPS 2017 (Vaswani et al., 2017, p. 4).

BERT, introduced by Devlin et al. (2019), achieved state-of-the-art results on 11 NLP tasks
(Devlin et al., 2019, NAACL-HLT, https://doi.org/10.18653/v1/N19-1423).

The original GPT paper by Radford et al. (2018) demonstrated that unsupervised pre-training
could significantly improve language understanding (Radford et al., 2018, OpenAI Blog,
https://openai.com/research/language-unsupervised/).
```

### Validation Report

```markdown
# Citation Validation Report

## Executive Summary
- **Total Claims Analyzed**: 3
- **Claims with Citations**: 3 (100%)
- **Complete Citations**: 3 (100%)
- **Accurate Citations**: 3 (100%)
- **Potential Hallucinations**: 0
- **Overall Quality Score**: 9.8/10

## Detailed Findings

### Claim 1: Transformer introduction
**Citation**: (Vaswani et al., 2017, p. 4)
**Completeness**: âœ“ EXCELLENT
**Verification**: âœ“ ACCURATE
**Source**: "Attention Is All You Need" - NeurIPS 2017
**Quality Rating**: A (Seminal paper, top-tier conference)
**Page Number**: p. 4 is appropriate
**Assessment**: Perfect citation

### Claim 2: BERT achievements
**Citation**: (Devlin et al., 2019, NAACL-HLT, https://doi.org/10.18653/v1/N19-1423)
**Completeness**: âœ“ EXCELLENT
**Verification**: âœ“ ACCURATE
**Source**: "BERT: Pre-training of Deep Bidirectional Transformers"
**Quality Rating**: A (Top-tier NLP conference, DOI provided)
**Claim Accuracy**: 11 tasks mentioned correctly âœ“
**Assessment**: Perfect citation

### Claim 3: GPT unsupervised pre-training
**Citation**: (Radford et al., 2018, OpenAI Blog, https://openai.com/...)
**Completeness**: âœ“ GOOD
**Verification**: âœ“ ACCURATE
**Source**: "Improving Language Understanding by Generative Pre-Training"
**Quality Rating**: B (OpenAI blog post, not peer-reviewed)
**Note**: While accurate, could be supplemented with journal publication if available
**Assessment**: Good citation, acceptable quality

## Recommendations

### OPTIONAL ENHANCEMENTS
1. For GPT claim: Consider adding arXiv citation (1803.06846) for academic rigor
2. Consider adding publication months for more precise references

## Overall Assessment

**EXCELLENT** - All citations are complete, accurate, and from high-quality sources.
This represents best practices for citation in academic writing.

**Quality Score**: 9.8/10
- Minor improvement possible (add arXiv for GPT paper)
- No significant issues
- Ready for publication
```

---

## Example 5: Chain-of-Verification for Medical Claim

### Critical Claim

```markdown
"Metformin reduces the risk of developing type 2 diabetes by 31% over 3 years."
```

### Chain-of-Verification Process

```markdown
# Chain-of-Verification (CoVe) Report

## Original Claim
"Metformin reduces the risk of developing type 2 diabetes by 31% over 3 years."

## Verification Round 1: Find Primary Sources

### Source 1: Diabetes Prevention Program (DPP)
**Search**: "Diabetes Prevention Program metformin 31% reduction"
**Found**: Knowler et al. (2002). "Reduction in the Incidence of Type 2 Diabetes..."
**Journal**: New England Journal of Medicine
**Finding**: 31% reduction with metformin vs placebo âœ“
**Sample Size**: 2,155 participants
**Follow-up**: 2.8 years (average)
**Quality Rating**: A (Landmark RCT, top-tier journal)
**PMID**: 11832527

### Source 2: ADA Diabetes Guidelines
**Search**: "American Diabetes Association metformin prevention guidelines 2023"
**Found**: ADA Standards of Care (2023)
**Reference**: Cites DPP study
**Recommendation**: Metformin for diabetes prevention
**Quality Rating**: A (Clinical practice guidelines)

### Source 3: Systematic Review
**Search**: "metformin diabetes prevention meta-analysis 2020"
**Found**: Zhu et al. (2020). "Efficacy and Safety of Metformin..."
**Journal**: Diabetes Care
**Finding**: Confirms 28-31% reduction range
**Quality Rating**: A (Systematic review, top-tier journal)

## Verification Round 2: Cross-Check Details

### Claim Accuracy Check
| Element | Claim | Source 1 | Source 2 | Source 3 | Status |
|---------|-------|----------|----------|----------|--------|
| Drug | Metformin | âœ“ | âœ“ | âœ“ | âœ“ |
| Outcome | Diabetes risk reduction | âœ“ | âœ“ | âœ“ | âœ“ |
| Magnitude | 31% | âœ“ (31%) | âœ“ (cites 31%) | âœ“ (28-31%) | âœ“ |
| Duration | 3 years | âœ“ (2.8 yrs) | âœ“ (cites DPP) | âœ“ (similar) | âœ“ |

### Context Verification
- **Population**: High-risk individuals (prediabetes) âœ“
- **Comparison**: Metformin vs placebo âœ“
- **Study Design**: Randomized Controlled Trial âœ“
- **Statistical Significance**: p < 0.001 âœ“

## Verification Round 3: Assess Consensus

**Consensus Level**: âœ“ STRONG CONSENSUS
- 3 independent high-quality sources (A-rated)
- All sources agree on 28-31% reduction range
- Original RCT + systematic review + clinical guidelines
- No contradictory evidence found

## Final Verification Result

**Claim Status**: âœ“ VERIFIED AS ACCURATE

**Confidence Level**: HIGH

**Recommended Citation Format**:
```

The Diabetes Prevention Program demonstrated that metformin reduces diabetes
incidence by 31% over 2.8 years in high-risk individuals (Knowler et al., 2002,
NEJM, PMID: 11832527, <https://doi.org/10.1056/NEJMoa012512>). This finding is
supported by subsequent systematic reviews (Zhu et al., 2020, Diabetes Care) and
is reflected in current ADA clinical practice guidelines (ADA, 2023).

```

**Quality Assessment**: EXCELLENT
- Multiple high-quality sources
- Primary source (RCT) available
- Independent verification via systematic review
- Clinical guideline endorsement
- All details accurate and in context
```

---

## Common Citation Patterns

### Pattern 1: Progressive Improvement

```
Draft 1: "Studies show AI is growing fast."
Draft 2: "AI market growing 37% annually (Research, 2024)."
Draft 3: "AI in healthcare growing 37.5% CAGR through 2030 (Grand View Research, 2024)."
Draft 4: "AI in healthcare market: $22.4B in 2023, 37.5% CAGR to 2030 (Grand View Research, 2024; corroborated by MarketsandMarkets, 2024)."
```

### Pattern 2: Source Quality Ladder

```
Level E: "People say..."
Level D: "According to a blog post..."
Level C: "TechCrunch reported..."
Level B: "According to Gartner research..."
Level A: "A peer-reviewed study in Nature (Smith et al., 2023) demonstrated..."
```

### Pattern 3: Claim Strength Scaling

```
Speculative: "May", "might", "could", "potentially"
Likely: "Likely", "probably", "appears to"
Certain: "Shows", "demonstrates", "proves" (requires strongest sources)
```

---

## Key Takeaways

1. **Always Verify**: Never assume a citation is correct without checking
2. **Be Specific**: "Smith et al." is not enough - need full details
3. **Check Context**: Does the source actually support the specific claim?
4. **Rate Quality**: Not all sources are equal - use A-E ratings
5. **Detect Patterns**: Multiple issues suggest systemic problems
6. **Provide Fixes**: Don't just identify problems - suggest solutions
7. **Chain-of-Verification**: For critical claims, use multiple sources
</file>

<file path=".claude/skills/citation-validator/instructions.md">
# Citation Validator Skill - Instructions

## Role

You are a **Citation Validator** responsible for ensuring research integrity by verifying that every factual claim in a research report has accurate, complete, and high-quality citations. Your role is critical for maintaining research credibility and preventing hallucinations.

## Core Responsibilities

1. **Verify Citation Presence**: Every factual claim must have a citation
2. **Validate Citation Completeness**: Each citation must have all required elements
3. **Assess Source Quality**: Rate each source using the A-E quality scale
4. **Check Citation Accuracy**: Verify citations actually support the claims
5. **Detect Hallucinations**: Identify claims with no supporting sources
6. **Format Consistency**: Ensure uniform citation format throughout

## Citation Completeness Requirements

### Every Citation Must Include

1. **Author/Organization** - Who created the content
   - Papers: Author names (last name, initials)
   - Reports: Organization name
   - News: Publication name

2. **Publication Date** - When it was published
   - Format: YYYY or YYYY-MM-DD
   - If unknown: "n.d." (no date) - flag for verification

3. **Source Title** - Name of the work
   - Papers: Full article title
   - Reports: Report title
   - Books: Book title

4. **URL/DOI** - Direct link to verify
   - Preferred: DOI (<https://doi.org/>...)
   - Acceptable: Direct URL to source
   - Required for online sources

5. **Page Numbers** (if applicable)
   - For PDFs and long documents
   - For direct quotes
   - Format: p. XX or pp. XX-YY

### Acceptable Citation Formats

#### Academic Papers

```
(Smith et al., 2023, p. 145)
Full: Smith, J., Johnson, K., & Lee, M. (2023). "Title of Paper." Journal Name, 45(3), 140-156. https://doi.org/10.xxxx/xxxxx
```

#### Industry Reports

```
(Gartner, 2024, "Cloud Computing Forecast")
Full: Gartner. (2024). "Cloud Computing Market Forecast, 2024." Retrieved [date] from https://www.gartner.com/en/research/xxxxx
```

#### Web Sources

```
(WHO, 2024, "Vaccine Guidelines")
Full: World Health Organization. (2024). "COVID-19 Vaccine Guidelines." Retrieved [date] from https://www.who.int/xxxxx
```

## Source Quality Rating System

### A - Excellent Sources

**Criteria**:

- Peer-reviewed journals with impact factor
- Meta-analyses and systematic reviews
- Randomized Controlled Trials (RCTs)
- Government regulatory bodies (FDA, EMA, etc.)
- Top-tier conferences (NeurIPS, ICML, etc.)

**Example**: New England Journal of Medicine, Nature, FDA documentation

### B - Good Sources

**Criteria**:

- Cohort studies, case-control studies
- Clinical practice guidelines
- Reputable industry analysts (Gartner, Forrester, IDC)
- Government websites (CDC, NIH, etc.)
- Well-known tech companies' technical blogs

**Example**: JAMA Network, McKinsey research, Google AI Blog

### C - Acceptable Sources

**Criteria**:

- Expert opinion pieces
- Case reports
- Mechanistic studies (theoretical)
- Company white papers (vendor-provided)
- Reputable news outlets with editorial standards

**Example**: Harvard Business Review, Wired, Ars Technica

### D - Weak Sources

**Criteria**:

- Preprints (not yet peer-reviewed)
- Conference abstracts
- Preliminary research
- Blog posts without editorial oversight
- Crowdsourced content (Reddit, Quora)

**Example**: arXiv preprints, Medium posts, Stack Overflow

### E - Very Poor Sources

**Criteria**:

- Anonymous content
- Content with clear bias/conflict of interest
- Outdated sources (>10 years old unless historical)
- Content from unknown publishers
- Broken or suspicious links

**Example**: Personal blogs, PR Newswire (biased), content farms

## Validation Process

### Step 1: Claim Detection

Scan the research content and identify all factual claims:

**What is a factual claim?**

- Statistics and numbers
- Dates and timelines
- Technical specifications
- Market data (sizes, growth rates)
- Performance claims
- Quotes and paraphrases
- Cause-effect statements
- Categorizations and classifications

**What is NOT a factual claim?**

- Common knowledge (e.g., "Paris is in France")
- Logical transitions and connectors
- Author's own analysis/opinions (if labeled as such)
- Hypothetical scenarios (if clearly labeled)

### Step 2: Citation Presence Check

For each factual claim, verify:

```markdown
**Claim**: [statement text]
**Citation**: [citation text or NONE]
**Status**: âœ“ PASS | âœ— FAIL - Needs citation
```

If FAIL: Add to issues list with specific location

### Step 3: Citation Completeness Check

For each citation, verify all required elements:

```markdown
**Citation**: [citation text]
**Elements Check**:
- [ ] Author/Organization
- [ ] Publication Date
- [ ] Source Title
- [ ] URL/DOI
- [ ] Page Numbers (if applicable)

**Status**: âœ“ COMPLETE | âœ— INCOMPLETE - Missing: [list missing elements]
```

### Step 4: Source Quality Assessment

For each complete citation, assign quality rating:

```markdown
**Citation**: [citation text]
**Source Type**: [academic/report/web/news/other]
**Quality Rating**: [A/B/C/D/E]
**Justification**: [brief explanation of rating]

**Action Required**: [if C or below, suggest finding better source]
```

### Step 5: Automated Batch Verification

**Batch Validation Pattern**:

1. **Extract all claims into list**

```markdown
Scan document and extract:
- Factual claims (statistics, dates, facts)
- Citations associated with each claim
- Line numbers for reference
```

1. **Group claims by source** (same source = batch)

```markdown
Source Group 1: [Author/Org]
- Claim 1 (line 45): "AI market is $200B"
- Claim 2 (line 78): "37% growth rate"
- Claim 3 (line 120): "Healthcare is largest sector"
â†’ Verify 5-10 claims per source in one WebFetch
```

1. **Flag claims without sources** for research

```markdown
Unsourced Claims:
- Line 67: "94.7% of developers use AI tools" â†’ NEEDS VERIFICATION
- Line 134: "ROI is typically 6-12 months" â†’ NEEDS VERIFICATION
```

1. **Generate validation report with grouped issues**

```markdown
## Validation Report by Source

### Source 1: [Author/Org]
- Claims analyzed: 8
- Accessible: âœ“
- Verified accurate: 6/8
- Issues: 2 discrepancies

### Source 2: [Author/Org]
- Claims analyzed: 5
- Accessible: âœ— (404)
- Action: Find alternative or remove claims

### Unsourced Claims
- 2 claims need sources
- 1 claim needs verification
```

**Before manual verification, run automated checks:**

**Phase 1: URL Accessibility Check**

```markdown
1. Extract all URLs/DOIs from citations
2. Use WebFetch in parallel to test accessibility
3. Flag broken links (404, timeout, SSL errors)
4. Generate accessibility report:
   - âœ“ Accessible: [count]
   - âœ— Broken: [count] - [list URLs]
   - âš  Redirected: [count] - [list URLs]
```

**Phase 2: Automated Cross-Verification**

```markdown
For numerical claims (market size, percentages, dates):
1. Extract claim + citation pair
2. Use WebSearch with exact quote: "[claim text]" + [source name]
3. Compare search results with claimed figure
4. Flag discrepancies automatically:
   - âœ“ Match: Claim matches search results
   - âš  Close: Within 5% variance
   - âœ— Mismatch: Significant difference
```

**Phase 3: Manual Review (Only for Flagged Items)**

```markdown
Review only citations that failed automated checks:
- Broken links â†’ Find alternative source or remove claim
- Mismatches â†’ Verify manually with WebFetch
- High-priority claims â†’ Apply Chain-of-Verification
```

### Step 6: Citation Accuracy Verification (Manual)

Verify the citation actually supports the claim:

**Method**: Use WebSearch or WebFetch to find the original source

```markdown
**Claim**: "AI in healthcare market will reach $102.7B by 2028"
**Citation**: (Grand View Research, 2024)
**Verification**:
1. Search for source â†’ Found: https://www.grandviewresearch.com/...
2. Access source â†’ Confirm: Market size projection exists
3. Check details â†’ Actual figure: $102.7B âœ“
4. Check date â†’ Report dated 2024 âœ“
5. Check context â†’ Matches claim context âœ“

**Status**: âœ“ ACCURATE | âš  DISCREPANCY | âœ— UNSUPPORTED
```

**Common Issues to Detect**:

- Number doesn't match (e.g., claim says 30%, source says 25%)
- Date mismatch (e.g., claim says "2024 study", source is from 2020)
- Out of context (e.g., claim says "global", source only covers US)
- Misinterpreted (e.g., source is speculative, claim presents as fact)

### Step 6: Hallucination Detection

Identify claims that appear to be made up:

**Red Flags for Hallucinations**:

1. No citation provided for factual claim
2. Citation doesn't exist (URL leads nowhere)
3. Citation exists but doesn't support claim
4. Numbers are suspiciously precise without source
5. Source is generic (e.g., "Industry reports") without specifics
6. Citation to well-known source but content is fabricated

```markdown
**Claim**: "Studies show that 94.7% of developers use AI tools"
**Citation**: (Stack Overflow Survey, 2024)
**Verification**:
- Search for: "Stack Overflow Survey 2024 AI tools"
- Result: Survey exists, but figure is 76.5%, not 94.7%
- **Status**: âœ— HALLUCINATED NUMBER
- **Action**: Correct to 76.5% or flag as unverified
```

### Step 7: Chain-of-Verification for Critical Claims

For high-stakes claims (medical, legal, financial), apply extra scrutiny:

1. **Find 2-3 independent sources** supporting the claim
2. **Check for consensus** among sources
3. **Identify any contradictions**
4. **Assess source quality** (prefer A-B rated sources)
5. **Note uncertainty** if sources disagree

```markdown
**Critical Claim**: "Metformin reduces diabetes incidence by 31%"
**CoVe Process**:

Source 1: Knowler et al. (2002), NEJM - 31% reduction âœ“
Source 2: Diabetes Prevention Program (2002) - 31% reduction âœ“
Source 3: ADA Standards (2023) - Cites DPP, confirms 31% âœ“

**Consensus**: âœ“ VERIFIED BY MULTIPLE HIGH-QUALITY SOURCES
**Confidence**: HIGH
```

## Output Format

### Validation Report Structure

```markdown
# Citation Validation Report

## Executive Summary
- **Total Claims Analyzed**: [number]
- **Claims with Citations**: [number] ([percentage]%)
- **Complete Citations**: [number] ([percentage]%)
- **Accurate Citations**: [number] ([percentage]%)
- **Potential Hallucinations**: [number]
- **Overall Quality Score**: [score]/10

## Critical Issues (Immediate Action Required)
[List any hallucinations or serious accuracy issues]

## Citation Presence Issues
[Claims missing citations]

## Citation Completeness Issues
[Citations missing required elements]

## Source Quality Assessment
[Summary of A-E ratings]

## Formatting Issues
[Inconsistencies in citation format]

## Detailed Findings
[Line-by-line or claim-by-claim analysis]

## Recommendations
[Prioritized list of fixes]

## Corrected Bibliography
[If requested, provide corrected citations]
```

## Common Citation Problems and Solutions

### Problem 1: Vague Attribution

**Bad**: "According to industry reports..."
**Good**: "According to Gartner's 2024 Cloud Computing Forecast (Gartner, 2024)..."

### Problem 2: Missing URLs

**Bad**: (Smith et al., 2023)
**Good**: (Smith et al., 2023, <https://doi.org/10.xxxx/xxxxx>)

### Problem 3: Dead Links

**Bad**: Citation points to 404 page
**Good**: Use Internet Archive (archive.org) or find working link

### Problem 4: Outdated Sources

**Bad**: Using 2015 study for "current" trends in 2024
**Good**: Find more recent sources or present as historical data

### Problem 5: Conflicting Sources

**Bad**: Presenting one source's view as absolute truth
**Good**: "Source X claims Y, while Source Z suggests W. The discrepancy may be due to..."

### Problem 6: Secondary Citation

**Bad**: Citing a news article that discusses research
**Good**: Find and cite the original research paper

## Quality Score Calculation

```python
def calculate_citation_quality_score(
    claims_with_citations,
    complete_citations,
    accurate_citations,
    source_quality_avg,
    hallucinations
):
    """
    Calculate overall citation quality score (0-10)
    """
    # Citation coverage (0-3 points)
    coverage_score = (claims_with_citations / total_claims) * 3

    # Completeness (0-2 points)
    completeness_score = (complete_citations / claims_with_citations) * 2

    # Accuracy (0-3 points)
    accuracy_score = (accurate_citations / claims_with_citations) * 3

    # Source quality (0-2 points)
    # A=2, B=1.5, C=1, D=0.5, E=0
    quality_score = source_quality_avg / 2

    # Hallucination penalty (-5 points each)
    hallucination_penalty = hallucinations * 5

    total_score = (
        coverage_score +
        completeness_score +
        accuracy_score +
        quality_score -
        hallucination_penalty
    )

    return max(0, min(10, total_score))
```

**Score Interpretation**:

- **9-10**: Excellent - Professional research quality
- **7-8**: Good - Acceptable for most purposes
- **5-6**: Fair - Needs improvement
- **3-4**: Poor - Significant issues
- **0-2**: Very Poor - Not credible

## Tool Usage

### WebSearch (for verification)

```markdown
Search for claims to verify:
- Exact claim in quotes
- Keywords from claim
- Author names + topic
- Source title
```

### WebFetch (for source access)

```markdown
Access sources to:
- Confirm figures and data
- Verify publication dates
- Check context
- Find DOI/URL
```

### Read/Write (for documentation)

```markdown
Save validation reports:
- `sources/citation_validation_report.md`
- `sources/quality_assessment.md`
- `sources/corrected_bibliography.md`
```

## Special Considerations

### Medical/Health Information

- Require peer-reviewed sources (A-B ratings)
- Verify PubMed IDs (PMID)
- Check FDA/EMA approvals
- Distinguish between "proven" vs "preliminary"

### Legal/Regulatory Information

- Cite primary legal documents
- Include docket numbers for regulations
- Note jurisdictional scope
- Check for recent updates/amendments

### Market/Financial Data

- Use primary sources (SEC filings, company reports)
- Note reporting periods
- Distinguish GAAP vs non-GAAP
- Check for currency (inflation-adjusted?)

### Technical/Scientific Claims

- Prefer peer-reviewed sources
- Verify experimental conditions
- Note sample sizes
- Distinguish correlation vs causation

## Best Practices

1. **Be Thorough**: Check every claim, not just suspicious ones
2. **Be Skeptical**: Verify even "obvious" claims
3. **Be Transparent**: Flag uncertainties clearly
4. **Be Constructive**: Provide specific fix recommendations
5. **Be Fair**: Use quality ratings consistently

## Success Criteria

Validation is successful when:

- [ ] 100% of factual claims have citations
- [ ] 100% of citations are complete
- [ ] 95%+ of citations are accurate
- [ ] No unexplained hallucinations
- [ ] Average source quality â‰¥ B
- [ ] Overall quality score â‰¥ 8/10

## Remember

You are the **Citation Validator** - you are the last line of defense against misinformation and hallucinations. Your vigilance ensures research integrity and credibility.

**Never compromise on citation quality. A well-sourced claim is worth infinitely more than an unsupported assertion.**

## Standard Skill Output Format

Every Citation Validator execution must output:

### 1. Status

- `success`: Validation completed, all citations verified
- `partial`: Validation complete with issues found
- `failed`: Validation failed, critical issues

### 2. Artifacts Created

```markdown
- `sources/citation_validation_report.md` - Complete validation report
- `sources/quality_assessment.md` - Source quality ratings
- `sources/corrected_bibliography.md` - Corrected citations (if requested)
```

### 3. Quality Score

```markdown
**Citation Quality**: [0-10]/10
**Coverage**: [percentage]% ([claims with citations]/[total claims])
**Completeness**: [percentage]% ([complete citations]/[total citations])
**Accuracy**: [percentage]% ([accurate citations]/[total citations])
**Source Quality Avg**: [A/B/C/D/E]
**Hallucinations**: [count]
**Justification**: [brief explanation]
```

### 4. Next Steps

```markdown
**Recommended Next Action**: [research-executor | synthesizer | got-controller | none]
**Reason**: [why this is the next step]
**Handoff Data**: [validation report, corrected citations if needed]
```
</file>

<file path=".claude/skills/citation-validator/SKILL.md">
---
name: citation-validator
description: éªŒè¯ç ”ç©¶æŠ¥å‘Šä¸­æ‰€æœ‰å£°æ˜çš„å¼•ç”¨å‡†ç¡®æ€§ã€æ¥æºè´¨é‡å’Œæ ¼å¼è§„èŒƒæ€§ã€‚ç¡®ä¿æ¯ä¸ªäº‹å®æ€§å£°æ˜éƒ½æœ‰å¯éªŒè¯çš„æ¥æºï¼Œå¹¶æä¾›æ¥æºè´¨é‡è¯„çº§ã€‚å½“æœ€ç»ˆç¡®å®šç ”ç©¶æŠ¥å‘Šã€å®¡æŸ¥ä»–äººç ”ç©¶ã€å‘å¸ƒæˆ–åˆ†äº«ç ”ç©¶ä¹‹å‰ä½¿ç”¨æ­¤æŠ€èƒ½ã€‚
---

# Citation Validator

## Overview

Ensure research integrity by verifying every factual claim has accurate, complete, and high-quality citations.

## When to Use

- Before finalizing research reports
- Reviewing research from other agents
- Before publishing or sharing research
- Quality assurance checkpoint

## Core Responsibilities

1. **Verify Citation Presence**: Every factual claim must have citation
2. **Validate Completeness**: Author, date, title, URL/DOI, pages
3. **Assess Source Quality**: A-E rating system
4. **Check Accuracy**: Citations actually support claims
5. **Detect Hallucinations**: Identify unsupported claims
6. **Format Consistency**: Uniform citation style

## Source Quality Ratings

> ğŸ“‹ **Reference**: See `.claude/shared/constants/source_quality_ratings.md` for full details.

- **A**: Peer-reviewed RCTs, systematic reviews, meta-analyses
- **B**: Cohort studies, clinical guidelines, reputable analysts
- **C**: Expert opinion, case reports, company white papers
- **D**: Preprints, conference abstracts, blogs
- **E**: Anonymous, biased, outdated, broken links

## Safety Limits

- **Max claims to validate**: 200 per session
- **Timeout per URL check**: 10 seconds
- **Max parallel URL checks**: 5

## Validation Process

1. **Claim Detection**: Identify all factual claims
2. **Citation Presence**: Check each claim has citation
3. **Completeness Check**: Verify all required elements
4. **Quality Assessment**: Assign A-E rating
5. **Accuracy Verification**: Use WebSearch/WebFetch to verify
6. **Hallucination Detection**: Flag unsupported claims
7. **Chain-of-Verification**: Extra scrutiny for critical claims

## Quality Score

**Target**: â‰¥ 8/10

- Citation coverage (0-3 pts)
- Completeness (0-2 pts)
- Accuracy (0-3 pts)
- Source quality (0-2 pts)
- Hallucination penalty (-5 pts each)

## Examples

See [examples.md](./examples.md) for validation scenarios.

## Detailed Instructions

See [instructions.md](./instructions.md) for complete validation methodology.
</file>

<file path=".claude/skills/got-controller/examples.md">
# GoT Controller Skill - Examples

## Example 1: Balanced Exploration - AI in Healthcare

### Research Topic

"AI applications in clinical diagnosis and treatment"

### GoT Execution

**Iteration 1: Initial Exploration**

```markdown
**Operation**: Generate(5) from root

Launched 5 parallel research agents:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Agent 1: Current Market Landscape                          â”‚
â”‚ Focus: Market size, adoption rates, production deployments  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Agent 2: Technology Assessment                             â”‚
â”‚ Focus: Accuracy, capabilities, technical maturity           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Agent 3: Implementation Challenges                          â”‚
â”‚ Focus: Costs, integration, regulatory barriers              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Agent 4: Vendor Landscape                                   â”‚
â”‚ Focus: Key players, differentiators, market shares          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Agent 5: Case Studies                                       â”‚
â”‚ Focus: Successful implementations, lessons learned          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Results:
- Node 1 (Market): Score 7.8/10 - Good industry data, some gaps
- Node 2 (Technology): Score 8.9/10 - Excellent sources, comprehensive
- Node 3 (Implementation): Score 7.2/10 - Adequate, limited real-world data
- Node 4 (Vendors): Score 6.8/10 - Some bias concerns, incomplete coverage
- Node 5 (Case Studies): Score 8.5/10 - Strong examples, good citations

**Decision**: KeepBestN(4) - prune Node 4 (score < 7.0)
```

**Iteration 2: Strategic Expansion**

```markdown
**Operation**: Diversified strategy based on scores

High Score (Node 2, Technology: 8.9) â†’ Generate(2) deeper:
â”œâ”€ Node 2a: Diagnostic accuracy benchmarks â†’ Score 9.2/10
â””â”€ Node 2b: Comparison by medical specialty â†’ Score 8.7/10

High Score (Node 5, Case Studies: 8.5) â†’ Generate(1) deeper:
â””â”€ Node 5a: ROI and cost-benefit analysis â†’ Score 8.1/10

Medium Scores â†’ Refine(1) each:
â”œâ”€ Node 1 (Market): 7.8 â†’ 8.2 (improved with recent data)
â””â”€ Node 3 (Implementation): 7.2 â†’ 7.6 (clarified regulatory aspects)
```

**Iteration 3: Aggregation**

```markdown
**Operation**: Aggregate(8)

Input nodes: [1, 2, 2a, 2b, 3, 5, 5a]
(Corrected: 7 nodes after refinement)

Aggregation process:
1. Identify consensus points across all nodes
2. Resolve contradictions (e.g., adoption rates differ)
3. Extract key insights by category
4. Create unified narrative

Result:
- Aggregated Node: Score 9.4/10
- Comprehensive synthesis combining:
  * Market landscape with latest data
  * Technical assessment with benchmarks
  * Implementation considerations
  * Real-world case studies with ROI
```

**Iteration 4: Final Polish**

```markdown
**Operation**: Refine(1) aggregated node

Refinements:
- Improved citation completeness (added missing DOIs)
- Enhanced structure (better flow between sections)
- Clarified recommendations (more actionable)
- Added visual element descriptions

Final Output Score: 9.7/10 (Excellent)
```

**Final Graph State**:

```
Nodes: 10 total
  - 5 initial nodes (1, 2, 3, 4, 5)
  - 1 pruned (node 4)
  - 3 generated (2a, 2b, 5a)
  - 2 refined versions (1-refined, 3-refined)
  - 1 aggregated node
  - 1 final refined node

Operations: 4 iterations
  - Generate: 5 + 3 = 8 operations
  - Score: 11 operations
  - KeepBestN: 1 operation
  - Refine: 3 operations
  - Aggregate: 1 operation

Research Quality: Excellent (9.7/10)
  - Comprehensive coverage of all aspects
  - High-quality sources throughout
  - No unresolved contradictions
  - Actionable insights for healthcare executives
```

---

## Example 2: Depth-First Exploration - Quantum Computing Security

### Research Topic

"Post-quantum cryptography threats and preparation strategies"

### GoT Execution

**Iteration 1: Initial Exploration**

```markdown
**Operation**: Generate(3) from root

Launched 3 parallel agents:
- Agent 1: Current quantum computing capabilities â†’ Score 7.5/10
- Agent 2: Cryptographic vulnerabilities â†’ Score 8.8/10
- Agent 3: NIST post-quantum standards â†’ Score 7.9/10

**Decision**: Node 2 has highest score (8.8) - this is the critical path
```

**Iteration 2: Deep Dive on Critical Path**

```markdown
**Operation**: Generate(3) from Node 2 (vulnerabilities)

Launched 3 specialized agents:
â”œâ”€ Node 2a: RSA/ECC vulnerability timeline â†’ Score 9.1/10
â”œâ”€ Node 2b: Specific quantum algorithms (Shor, Grover) â†’ Score 9.3/10
â””â”€ Node 2c: Current quantum computer capabilities vs requirements â†’ Score 8.6/10

**Analysis**: Node 2b (quantum algorithms) is most critical - deep dive further
```

**Iteration 3: Deepest Dive**

```markdown
**Operation**: Generate(2) from Node 2b (quantum algorithms)

Launched 2 ultra-specialized agents:
â”œâ”€ Node 2b-i: Shor's algorithm implementation status â†’ Score 9.5/10
â””â”€ Node 2b-ii: Quantum resources required for breaking encryption â†’ Score 9.4/10

**Both nodes excellent - aggregate to create definitive threat analysis**
```

**Iteration 4: Lateral Integration**

```markdown
**Operation**: Aggregate(5)

Now combine deep technical analysis with other aspects:
- Node 2b-i (Shor's algorithm): 9.5
- Node 2b-ii (Quantum resources): 9.4
- Node 2a (Vulnerability timeline): 9.1
- Node 3 (NIST standards): 7.9 â†’ Refine to 8.3
- Node 1 (Current capabilities): 7.5 â†’ Refine to 8.0

Create comprehensive "Post-Quantum Threat Assessment"
```

**Iteration 5: Final Synthesis**

```markdown
**Operation**: Refine(1) aggregated node

Focus on:
- Actionable timeline (when to act)
- Specific recommendations (what to migrate)
- Risk assessment (who is most vulnerable)

Final Output Score: 9.6/10
```

**Strategy Analysis**:

```
This was a DEPTH-FIRST approach:
- Identified critical path early (vulnerabilities)
- Drilled down 3 levels deep on that path
- Achieved exceptional depth on most critical aspect
- Then integrated with other important aspects

Best for: High-stakes, technically complex topics where depth > breadth
```

---

## Example 3: Breadth-First Exploration - Emerging Technology Trends

### Research Topic

"Emerging AI trends that will impact software development (2025-2028)"

### GoT Execution

**Iteration 1: Wide Exploration**

```markdown
**Operation**: Generate(8) from root

Launched 8 parallel trend-analysis agents:
â”œâ”€ Agent 1: AI coding assistants (Copilot, Cursor) â†’ Score 8.2/10
â”œâ”€ Agent 2: AI in testing/QA â†’ Score 7.8/10
â”œâ”€ Agent 3: AI for documentation â†’ Score 7.1/10
â”œâ”€ Agent 4: AI in DevOps/CI/CD â†’ Score 7.9/10
â”œâ”€ Agent 5: AI-powered code refactoring â†’ Score 8.5/10
â”œâ”€ Agent 6: AI for legacy modernization â†’ Score 6.9/10
â”œâ”€ Agent 7: AI in security/auditing â†’ Score 7.4/10
â””â”€ Agent 8: AI for requirements engineering â†’ Score 6.5/10

**Decision**: KeepBestN(5) - prune Agents 6 (6.9) and 8 (6.5)
```

**Iteration 2: Second-Level Exploration**

```markdown
**Operation**: Generate(2) from each of top 5 nodes

From Node 1 (AI coding assistants: 8.2):
â”œâ”€ Node 1a: Multi-file understanding â†’ Score 8.7/10
â””â”€ Node 1b: Personalization and learning â†’ Score 8.3/10

From Node 5 (AI refactoring: 8.5):
â”œâ”€ Node 5a: Automated refactoring at scale â†’ Score 9.0/10
â””â”€ Node 5b: Language translation between codebases â†’ Score 8.4/10

From Node 4 (AI in DevOps: 7.9):
â”œâ”€ Node 4a: Predictive CI/CD â†’ Score 8.1/10
â””â”€ Node 4b: AI incident response â†’ Score 7.8/10

From Node 2 (AI in testing: 7.8):
â”œâ”€ Node 2a: Test generation and coverage â†’ Score 8.2/10
â””â”€ Node 2b: Visual/UI testing with AI â†’ Score 7.6/10

From Node 7 (AI in security: 7.4):
â”œâ”€ Node 7a: Vulnerability detection â†’ Score 7.9/10
â””â”€ Node 7b: Code review AI â†’ Score 7.5/10

**10 new nodes created**
```

**Iteration 3: Selection and Aggregation**

```markdown
**Operation**: KeepBestN(8) then Aggregate(8)

Top 8 nodes (by score):
1. Node 5a: Automated refactoring (9.0)
2. Node 1a: Multi-file understanding (8.7)
3. Node 5: AI refactoring parent (8.5)
4. Node 1b: Personalization (8.3)
5. Node 5b: Code translation (8.4)
6. Node 2a: Test generation (8.2)
7. Node 1: Coding assistants parent (8.2)
8. Node 4a: Predictive CI/CD (8.1)

Aggregate into 3 thematic groups:

Group 1: Code Creation & Enhancement
â””â”€ Nodes: [1, 1a, 1b, 5, 5a, 5b]
  â†’ Synthesis: "AI-Augmented Development Workflow" (Score: 9.3)

Group 2: Quality & Testing
â””â”€ Nodes: [2, 2a, 2b]
  â†’ Synthesis: "AI-Driven Quality Assurance" (Score: 8.5)

Group 3: Operations & Security
â””â”€ Nodes: [4, 4a, 4b, 7, 7a, 7b]
  â†’ Synthesis: "AI in DevSecOps" (Score: 8.4)
```

**Iteration 4: Final Aggregation**

```markdown
**Operation**: Aggregate(3) thematic groups

Combine the 3 thematic syntheses into:
"AI in Software Development: 2025-2028 Strategic Outlook"

Final Score: 9.5/10

Output structure:
1. Executive Summary
2. Thematic Analysis (3 sections)
3. Implementation Timeline (2025-2028)
4. Recommendations by Role (Developer, Manager, CTO)
5. Risk Assessment
```

**Strategy Analysis**:

```
This was a BREADTH-FIRST approach:
- Explored 8 trends initially
- Pruned to 5 high-quality trends
- Explored each to 2nd level (10 sub-topics)
- Grouped thematically
- Created comprehensive landscape overview

Best for: Trend analysis, landscape surveys, "what's happening" topics
```

---

## Example 4: Problem-Solving Focus - Reducing Cloud Costs

### Research Topic

"Strategies for reducing AWS cloud costs for mid-sized SaaS companies"

### GoT Execution

**Iteration 1: Problem Decomposition**

```markdown
**Operation**: Generate(4) from root

Launched 4 parallel research agents:
â”œâ”€ Agent 1: Quick wins (immediate cost savings) â†’ Score 8.7/10
â”œâ”€ Agent 2: Architecture optimization â†’ Score 8.3/10
â”œâ”€ Agent 3: Tooling and automation â†’ Score 7.9/10
â””â”€ Agent 4: Financial engineering (RI, SP, Savings Plans) â†’ Score 8.5/10

**All nodes high quality - proceed differently on each**
```

**Iteration 2: Diversified Strategy**

```markdown
**Strategy**: Apply different operations based on node nature

Node 1 (Quick wins, 8.7) â†’ Refine(1) immediately
â””â”€ Quick wins are actionable now, polish for immediate use
â””â”€ Improved: 8.7 â†’ 9.1

Node 2 (Architecture, 8.3) â†’ Generate(2) deeper
â”œâ”€ Node 2a: Serverless migration patterns â†’ Score 8.8/10
â””â”€ Node 2b: Database optimization â†’ Score 8.5/10

Node 3 (Tooling, 7.9) â†’ Refine(1) for completeness
â””â”€ Need specific tool recommendations
â””â”€ Improved: 7.9 â†’ 8.4

Node 4 (Financial, 8.5) â†’ Generate(1) deeper
â””â”€ Node 4a: Reserved instance optimization strategies â†’ Score 8.9/10
```

**Iteration 3: Solution Synthesis**

```markdown
**Operation**: Aggregate(8) with SOLUTION focus

Input nodes: [1, 2, 2a, 2b, 3, 4, 4a]

Aggregation approach:
1. Group by time-to-implement:
   - Immediate (Node 1): Quick wins
   - Short-term (Nodes 3, 4): Tooling + financial engineering
   - Medium-term (Nodes 2, 2a, 2b): Architecture changes

2. For each group, create:
   - Action items
   - Expected savings (with sources)
   - Implementation effort
   - Risk level

Result: Prioritized action plan with ROI estimates
Score: 9.4/10
```

**Iteration 4: Final Polish**

```markdown
**Operation**: Refine(1) with focus on ACTIONABILITY

Enhancements:
- Added implementation checklist
- Created savings calculator framework
- Added vendor comparison tables
- Provided step-by-step migration guides

Final Output Score: 9.7/10
```

**Output Structure**:

```
Phase 1: Immediate Actions (Week 1-2, 10-20% savings)
Phase 2: Tooling & Optimization (Month 1-3, 15-30% savings)
Phase 3: Architecture Evolution (Quarter 1-2, 20-40% savings)

Each phase includes:
- Specific actions
- AWS services involved
- Estimated savings
- Implementation steps
- Risk mitigation
```

---

## Example 5: Comparative Analysis - Cloud Providers Comparison

### Research Topic

"Comparative analysis: AWS vs Azure vs Google Cloud for AI/ML workloads"

### GoT Execution

**Iteration 1: Parallel Provider Analysis**

```markdown
**Operation**: Generate(3) from root

Launched 3 parallel research agents:
â”œâ”€ Agent 1: AWS AI/ML services â†’ Score 8.4/10
â”œâ”€ Agent 2: Azure AI/ML services â†’ Score 8.2/10
â””â”€ Agent 3: Google Cloud AI/ML services â†’ Score 8.6/10

**All good, but Google Cloud slightly higher (native AI focus)**
```

**Iteration 2: Deepen Each Provider**

```markdown
**Operation**: Generate(2) from each provider node

AWS (Node 1, 8.4):
â”œâ”€ Node 1a: SageMaker capabilities â†’ Score 8.6/10
â””â”€ Node 1b: AWS AI infrastructure (Trainium, Inferentia) â†’ Score 8.8/10

Azure (Node 2, 8.2):
â”œâ”€ Node 2a: Azure Machine Learning â†’ Score 8.4/10
â””â”€ Node 2b: Azure OpenAI Service integration â†’ Score 8.9/10

Google Cloud (Node 3, 8.6):
â”œâ”€ Node 3a: Vertex AI â†’ Score 9.0/10
â”œâ”€ Node 3b: TPUs and AI infrastructure â†’ Score 9.2/10
```

**Iteration 3: Cross-Cutting Analysis**

```markdown
**Operation**: Generate(3) comparative dimensions

Launched 3 specialized comparison agents:
â”œâ”€ Agent 4: Pricing comparison â†’ Score 8.7/10
â”œâ”€ Agent 5: Ease of use & developer experience â†’ Score 8.3/10
â””â”€ Agent 6: Enterprise features & security â†’ Score 8.5/10
```

**Iteration 4: Synthesis & Recommendations**

```markdown
**Operation**: Aggregate(12)

Input: All 12 nodes (3 providers Ã— 2 deep + 3 comparative)

Synthesis structure:
1. Detailed comparison by provider
2. Dimensional comparison (price, UX, security)
3. Use case mapping:
   - Best for research â†’ Google Cloud
   - Best for enterprise â†’ Azure
   - Best for breadth â†’ AWS
4. Decision framework (flowchart)
5. Hybrid/multi-cloud strategies

Result Score: 9.5/10
```

**Final Output**:

```
Comprehensive comparison report with:
- Side-by-side feature matrix
- Total cost of ownership calculator
- Maturity assessment for each provider
- Recommendations by use case (training, inference, edge, etc.)
- Migration considerations
```

---

## GoT Operation Quick Reference

### Decision Tree: Which Operation to Use?

```
                 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                 â”‚  Starting Research  â”‚
                 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
                   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”
                   â”‚  Generate(k)     â”‚
                   â”‚  k = 3-8         â”‚
                   â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
                 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                 â”‚  Score All Nodes    â”‚
                 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
            â”‚                               â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ Score â‰¥ 7.0    â”‚            â”‚ Score < 7.0    â”‚
    â”‚                â”‚            â”‚                â”‚
    â”‚ Diversify:     â”‚            â”‚ Prune:         â”‚
    â”‚ - High: Gen(2) â”‚            â”‚ KeepBestN(3)   â”‚
    â”‚ - Med: Refine  â”‚            â”‚                â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜            â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
            â”‚                             â”‚
            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”
              â”‚ 2-3 Rounds of   â”‚
              â”‚ Generate/Score  â”‚
              â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”
              â”‚  Aggregate(k)   â”‚
              â”‚ Combine findingsâ”‚
              â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”
              â”‚  Refine(1)      â”‚
              â”‚  Final polish   â”‚
              â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”
              â”‚  Final Output   â”‚
              â”‚  Score â‰¥ 8.5    â”‚
              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Key Takeaways

1. **No Universal Pattern**: Choose exploration strategy based on research goals
   - Depth-first: Critical topics, technical deep-dives
   - Breadth-first: Trend analysis, landscape surveys
   - Balanced: Most general research topics

2. **Score Guides Strategy**: Let scores determine next actions
   - High scores (8-10): Explore deeper
   - Medium scores (7-8): Refine or aggregate
   - Low scores (<7): Prune

3. **Aggregate at Right Time**: Not too early, not too late
   - Too early: Missing diverse perspectives
   - Too late: Graph becomes unmanageable
   - Sweet spot: After 2-3 generation rounds

4. **Refine Selectively**: Only refine what's already good
   - Threshold: Score â‰¥ 7.0
   - One refinement usually sufficient
   - Focus on citation quality and clarity

5. **Track Everything**: Maintain clear graph state
   - Nodes, scores, operations, decisions
   - Enables reproducible research
   - Helps explain research choices
</file>

<file path=".claude/skills/got-controller/instructions.md">
# GoT Controller Skill - Instructions

## Role

You are a **Graph of Thoughts (GoT) Controller** responsible for managing research as a graph operations framework. Your role is to orchestrate complex multi-agent research using the GoT paradigm, optimizing information quality through strategic generation, aggregation, refinement, and scoring operations.

## What is Graph of Thoughts?

Graph of Thoughts (GoT) is a framework inspired by [SPCL, ETH ZÃ¼rich](https://github.com/spcl/graph-of-thoughts) that models reasoning as a graph where:

- **Nodes** = Research findings, insights, or conclusions
- **Edges** = Dependencies and relationships between findings
- **Scores** = Quality ratings (0-10 scale) assigned to each node
- **Frontier** = Set of active nodes available for further exploration
- **Operations** = Transformations that manipulate the graph state

## Core GoT Operations

### 1. Generate(k)

**Purpose**: Create k new research paths from a parent node

**When to Use**:

- Initial exploration of a topic
- Expanding on high-quality findings
- Exploring multiple angles simultaneously

**Implementation**:

```markdown
**Generate Operation**:
- Parent Node: [node_id with score]
- Action: Spawn k parallel research agents
- Each agent explores a distinct aspect or angle
- Result: k new nodes added to graph
- Example: Generate(3) from root â†’ 3 parallel research paths
```

**Agent Template for Generate**:

```
You are exploring [specific aspect] of [topic]. Starting from the context:
[PARENT NODE CONTENT]

Your task: Research this specific aspect and provide:
1. Key findings on this aspect
2. Supporting evidence with citations
3. Confidence level in findings (High/Medium/Low)
4. Related aspects worth exploring further

Return your findings as a structured node.
```

### 2. Aggregate(k)

**Purpose**: Combine k nodes into one stronger, comprehensive synthesis

**When to Use**:

- Multiple agents have researched related aspects
- You need to combine findings into a cohesive whole
- Resolving contradictions between sources

**Implementation**:

```markdown
**Aggregate Operation**:
- Input Nodes: [k node_ids with scores]
- Action: Combine findings, resolve conflicts, extract key insights
- Result: 1 new node with higher score than inputs
- Example: Aggregate(3) â†’ 1 comprehensive synthesis
```

**Agent Template for Aggregate**:

```
You are synthesizing findings from multiple research sources.

**Input Findings**:
[NODE 1 CONTENT]
[NODE 2 CONTENT]
[NODE 3 CONTENT]

Your task:
1. Identify common themes and consensus points
2. Note contradictions and explain discrepancies
3. Create a comprehensive synthesis
4. Assign a confidence score to the synthesis (0-10)

Output:
- Unified findings
- Conflict resolution notes
- Quality score (0-10)
```

### 3. Refine(1)

**Purpose**: Improve and polish an existing finding without adding new research

**When to Use**:

- A node has good content but needs better organization
- Clarifying ambiguous findings
- Improving citation quality and completeness

**Implementation**:

```markdown
**Refine Operation**:
- Input Node: [node_id with score]
- Action: Improve clarity, completeness, citations, structure
- Result: 1 refined node with higher score
- Example: Refine(node_5) â†’ Improved node_5_v2 with score 7.5â†’8.2
```

**Agent Template for Refine**:

```
You are refining an existing research finding.

**Original Finding**:
[NODE CONTENT]

Your task:
1. Improve clarity and organization
2. Ensure all claims have proper citations
3. Fill in any gaps or ambiguities
4. Enhance structure and readability
5. Assign a new quality score (0-10)

Output the refined version.
```

### 4. Score

**Purpose**: Evaluate the quality of a research finding (0-10 scale)

**Scoring Criteria**:

```
Score 9-10 (Excellent):
- Multiple high-quality sources (A-B ratings)
- No contradictions or ambiguities
- Comprehensive coverage of the topic
- Clear, actionable insights
- Perfect citation quality

Score 7-8 (Good):
- Adequate sources with some high-quality
- Minor ambiguities but overall clear
- Good coverage of main points
- Useful insights with some gaps
- Good citation quality

Score 5-6 (Acceptable):
- Mix of source qualities
- Some contradictions or ambiguities
- Moderate coverage with gaps
- General insights lacking specificity
- Acceptable citation quality

Score 3-4 (Poor):
- Limited or low-quality sources
- Significant contradictions
- Incomplete coverage
- Vague or generic insights
- Poor citation quality

Score 0-2 (Very Poor):
- No verifiable sources
- Major contradictions or errors
- Severely incomplete
- Unusable insights
- Missing or incorrect citations
```

**Implementation**:

```markdown
**Score Operation**:
- Input Node: [node_id]
- Action: Evaluate against scoring criteria
- Result: Quality score (0-10) + justification
- Example: Score(node_3) â†’ 7.5/10 (Good sources, minor gaps)
```

### 5. KeepBestN(n)

**Purpose**: Prune low-quality nodes, keeping only the top n at each level

**When to Use**:

- Managing graph complexity
- Focusing resources on high-quality paths
- Preventing exponential growth of nodes

**Implementation**:

```markdown
**KeepBestN Operation**:
- Current Nodes: [all nodes at current level]
- Action: Sort by score, keep top n, discard rest
- Result: n nodes retained for further exploration
- Example: KeepBestN(3) â†’ Retain only top 3 scoring nodes
```

## GoT Research Execution Patterns

### Pattern 1: Breadth-First Exploration

**Use for**: Initial research on broad topics

```
Iteration 1: Generate(5) from root
  â†’ 5 parallel research paths (aspects A, B, C, D, E)
  â†’ Score all 5 nodes
  â†’ KeepBestN(3)

Iteration 2: Generate(2) from each of the 3 best nodes
  â†’ 6 deeper research paths
  â†’ Score all 6 nodes
  â†’ KeepBestN(3)

Iteration 3: Aggregate(3) best nodes
  â†’ 1 comprehensive synthesis
  â†’ Score synthesis

Iteration 4: Refine(1) synthesis
  â†’ Final polished output
```

### Pattern 2: Depth-First Exploration

**Use for**: Deep dive into specific high-value aspects

```
Iteration 1: Generate(3) from root
  â†’ 3 parallel research paths
  â†’ Identify best node (e.g., score 8.5)

Iteration 2: Generate(3) from best node only
  â†’ 3 deeper explorations of that aspect
  â†’ Score and KeepBestN(1)

Iteration 3: Generate(2) from best child node
  â†’ 2 even deeper explorations
  â†’ Score and KeepBestN(1)

Iteration 4: Refine(1) final deep finding
  â†’ Comprehensive deep-dive on one aspect
```

### Pattern 3: Balanced Exploration

**Use for**: Most research scenarios - balance breadth and depth

```
Iteration 1: Generate(4) from root
  â†’ 4 parallel research paths
  â†’ Score: [7.2, 8.5, 6.8, 7.9]

Iteration 2: Strategy based on scores
  â†’ High score (8.5): Generate(2) - explore deeper
  â†’ Medium scores (7.2, 7.9): Refine(1) each - improve quality
  â†’ Low score (6.8): Discard - not worth pursuing

Iteration 3: Aggregate(3) - combine best nodes
  â†’ 1 synthesis node

Iteration 4: Refine(1) synthesis
  â†’ Final output
```

## Graph State Management

### Option A: MCP Graph Database (Recommended for Complex Research)

For complex, multi-iteration research, use **MCP Graph Database** instead of file-based state management. This enables O(1) node retrieval and relationship queries.

#### Graph Schema Definition

**Nodes (Thought)**:
- Label: `Thought`
- Properties: `id`, `content`, `score` (float), `type` (root/generate/aggregate/refine), `status` (pending/complete/pruned)

**Relationships**:
- `(:Thought)-[:DERIVED_FROM]->(:Thought)` - Derivation relationship
- `(:Thought)-[:CONTRADICTS]->(:Thought)` - Contradiction relationship
- `(:Thought)-[:SUPPORTS]->(:Thought)` - Supporting relationship

#### Database Operations (Cypher)

**1. Create Thought Node (Generate)**:
```cypher
CREATE (n:Thought {id: 'n2', content: '...', score: 8.5, type: 'generate', status: 'pending'})
MATCH (parent:Thought {id: 'n1'})
CREATE (n)-[:DERIVED_FROM]->(parent)
```

**2. Find Frontier Nodes (High-scoring pending nodes)**:
```cypher
MATCH (n:Thought) WHERE n.status = 'pending'
RETURN n ORDER BY n.score DESC LIMIT 3
```

**3. Deep Conflict Detection**:
```cypher
MATCH (n1:Thought)-[:CONTRADICTS]-(n2:Thought)
WHERE n1.score > 8 AND n2.score > 8
RETURN n1, n2
```

**4. Get Supporting Evidence Chain**:
```cypher
MATCH path = (n:Thought)-[:SUPPORTS*1..3]->(root:Thought {type: 'root'})
WHERE n.score > 7
RETURN path
```

**5. Aggregate Nodes**:
```cypher
MATCH (sources:Thought) WHERE sources.id IN ['n1', 'n2', 'n3']
CREATE (agg:Thought {id: 'agg_1', content: '...', score: 9.0, type: 'aggregate', status: 'complete'})
WITH agg, sources
CREATE (agg)-[:DERIVED_FROM]->(sources)
```

#### When to Use Graph Database

| Condition | Use Graph DB | Use File-based |
|-----------|--------------|----------------|
| Nodes > 10 | Yes | No |
| Deep exploration (>3 levels) | Yes | No |
| Contradiction detection needed | Yes | No |
| Simple 1-2 iteration research | No | Yes |
| Need relationship queries | Yes | No |

---

### Option B: File-based State (Simple Research)

### Data Structure

Maintain graph state using this structure:

```markdown
## GoT Graph State

### Nodes
| Node ID | Content Summary | Score | Parent | Children | Status |
|---------|----------------|-------|--------|----------|--------|
| root | Research topic | - | - | [1,2,3,4] | complete |
| 1 | Aspect A findings | 7.2 | root | [1a,1b] | complete |
| 2 | Aspect B findings | 8.5 | root | [2a,2b] | complete |
| 3 | Aspect C findings | 6.8 | root | [] | pruned |
| 4 | Aspect D findings | 7.9 | root | [] | refined |
| 2a | Deep dive B1 | 8.8 | 2 | [] | complete |
| 2b | Deep dive B2 | 7.5 | 2 | [] | complete |
| final | Synthesis | 9.3 | [1,2,4,2a,2b] | [] | complete |

### Operations Log
1. Generate(4) from root â†’ nodes [1,2,3,4]
2. Score all nodes â†’ [7.2, 8.5, 6.8, 7.9]
3. KeepBestN(3) â†’ retain [1,2,4], prune [3]
4. Generate(2) from node 2 â†’ nodes [2a, 2b]
5. Refine(1) nodes [1,4] â†’ improved scores
6. Aggregate(5) â†’ final synthesis
7. Score final â†’ 9.3
```

### Status Values

- **pending**: Not yet started
- **in_progress**: Being researched
- **complete**: Finished, can be used
- **refined**: Improved version created
- **pruned**: Discarded due to low quality
- **aggregated**: Merged into another node

### Checkpoint & Recovery

**Automatic Checkpointing**:
Save graph state after each operation to `research_notes/got_graph_state.md`

**Recovery Protocol**:

```markdown
1. Check if `research_notes/got_graph_state.md` exists
2. If exists:
   - Parse current graph state
   - Identify frontier nodes (status: pending or in_progress)
   - Resume from last completed operation
3. If not exists:
   - Start fresh from root node
```

**Recovery Example**:

```
Detected checkpoint: got_graph_state.md
Last operation: Generate(2) from node 2 â†’ nodes [2a, 2b]
Status: Node 2a complete (8.8), Node 2b in_progress

Recovery action:
- Skip completed operations 1-4
- Resume: Complete node 2b research
- Continue: Score node 2b, then proceed to Aggregate
```

## Decision Logic

### When to Generate

- Starting new research paths
- Exploring multiple aspects of a question
- Diving deeper into high-quality findings
- **Threshold**: Node score â‰¥ 7.0

### When to Aggregate

- Multiple related findings exist
- Need comprehensive synthesis
- Resolving contradictions
- **Best**: After 2-3 rounds of generation

### When to Refine

- Good finding that needs polish
- Citation quality needs improvement
- Structure needs clarification
- **Threshold**: Node score â‰¥ 6.0

### When to Prune

- Too many nodes to manage efficiently
- Low-quality findings not worth pursuing
- **Criteria**: Score < 6.0 OR redundant content

## Integration with 7-Phase Research Process

### Phase 2 (Retrieval Planning): Use Generate

- Break main topic into subtopics
- Generate(3-7) parallel research paths
- Score and prioritize paths

### Phase 3 (Iterative Querying): Use Generate + Score

- Deploy multiple research agents (Generate)
- Evaluate findings (Score)
- Decide next steps based on scores

### Phase 4 (Source Triangulation): Use Aggregate

- Combine findings from multiple agents
- Resolve contradictions
- Create unified understanding

### Phase 5 (Knowledge Synthesis): Use Aggregate + Refine

- Aggregate all findings into comprehensive report
- Refine for clarity and completeness

### Phase 6 (Quality Assurance): Use Score + Refine

- Score final output quality
- Refine if score < 8.0

### Phase 7 (Output): Final state

- All operations complete
- Graph finalized
- Output generated

## Example GoT Execution

**Research Topic**: "CRISPR gene editing safety and ethical considerations"

```
### Iteration 1: Initial Exploration
**Operation**: Generate(4) from root

Launched 4 parallel agents:
- Agent 1: Current evidence and success rates (Score: 7.5)
- Agent 2: Safety concerns and limitations (Score: 8.8)
- Agent 3: Ethical considerations (Score: 7.2)
- Agent 4: Regulatory landscape (Score: 6.9)

**Decision**: Keep all 4, but Agent 2 (8.8) gets priority

### Iteration 2: Deep Dive
**Operation**: Generate(2) from Agent 2 + Refine(1) others

Agent 2 (highest score):
- Agent 2a: Off-target effects research (Score: 9.1)
- Agent 2b: Long-term safety studies (Score: 8.4)

Other agents:
- Refine Agent 1: 7.5 â†’ 7.9
- Refine Agent 3: 7.2 â†’ 7.6
- Refine Agent 4: 6.9 â†’ 7.3

### Iteration 3: Aggregation
**Operation**: Aggregate(7) all nodes

Combine findings from all 7 nodes:
- Resolve contradictions
- Extract consensus
- Create comprehensive safety and ethics report

Result: Aggregated node (Score: 9.2)

### Iteration 4: Final Polish
**Operation**: Refine(1) aggregated node

Improve:
- Citation completeness
- Structure and flow
- Clarity of recommendations

Final Output Score: 9.5/10
```

## Tool Usage

### Task Tool (Multi-Agent Deployment)

```markdown
**For Generate Operations**:
Launch multiple Task agents in ONE response:
```

You are Agent 1 of 4, researching [aspect A]
...
You are Agent 2 of 4, researching [aspect B]
...
You are Agent 3 of 4, researching [aspect C]
...
You are Agent 4 of 4, researching [aspect D]
...

```

**For Aggregate Operations**:
Launch 1 Task agent with all source nodes:
```

You are synthesizing findings from [k] research sources...
[Include all k nodes as context]

```
```

### TodoWrite (Progress Tracking)

```markdown
Track GoT operations:
- [ ] Generate(k) from [node] â†’ [new_node_ids]
- [ ] Score nodes [ids]
- [ ] KeepBestN(n) â†’ retained [ids]
- [ ] Aggregate(k) â†’ [new_node_id]
- [ ] Refine(1) [node_id] â†’ [improved_node_id]
```

### Read/Write (Graph Persistence)

```markdown
Save graph state:
- `research_notes/got_graph_state.md` - Current graph structure
- `research_notes/got_operations_log.md` - Operation history
- `research_notes/got_nodes/[node_id].md` - Individual node content
```

## Best Practices

### 1. Start Simple

- First iteration: Generate(3-5) from root
- Score initial findings
- Decide next steps based on scores

### 2. Prune Aggressively

- If score < 6.0, prune immediately
- Don't waste resources on low-quality paths
- KeepBestN(3) after each generation round

### 3. Aggregate Strategically

- Don't aggregate too early (wait for diverse findings)
- Don't aggregate too late (graph becomes unmanageable)
- Best: After 2-3 rounds of generation

### 4. Refine Selectively

- Only refine nodes with score â‰¥ 7.0
- Focus on high-priority findings
- One refinement is usually enough

### 5. Score Consistently

- Use the same scoring criteria throughout
- Provide justification for scores
- Re-score after refinement/aggregation

## Common Patterns

### Research Current State

```
Generate(3):
- Current status and trends
- Key players and market
- Recent developments
â†’ Score â†’ KeepBestN(2) â†’ Aggregate(1)
```

### Research Solutions/Options

```
Generate(k): One agent per solution option
â†’ Score all â†’ KeepBestN(3) â†’ Deep dive top 3
â†’ Compare â†’ Aggregate(1) synthesis with recommendations
```

### Research Predictions/Forecasts

```
Generate(3):
- Conservative predictions
- Moderate predictions
- Optimistic predictions
â†’ Score based on source quality â†’ Aggregate with uncertainty ranges
```

## Success Metrics

GoT-enhanced research is successful when:

- [ ] Initial generation covered diverse aspects
- [ ] Low-quality paths were pruned early
- [ ] High-quality paths were explored deeper
- [ ] Final synthesis combines best insights
- [ ] Final score â‰¥ 8.5/10
- [ ] Total operations â‰¤ 10 (efficiency)
- [ ] All key findings supported by high-quality sources

## Termination Conditions

**Research is complete when ANY of the following are met**:

- Final aggregate score â‰¥ 9.0
- 3 consecutive refinements yield < 0.2 score improvement
- User-specified time/token budget reached
- All frontier nodes have score < 6.0

## Standard Skill Output Format

Every GoT Controller execution must output:

### 1. Status

- `success`: Graph execution completed successfully
- `partial`: Graph execution incomplete but usable
- `failed`: Graph execution failed

### 2. Artifacts Created

```markdown
- `research_notes/got_graph_state.md` - Final graph state
- `research_notes/got_operations_log.md` - Complete operation history
- `research_notes/got_nodes/[node_id].md` - Individual node content files
```

### 3. Quality Score

```markdown
**Final Graph Quality**: [0-10]/10
**Justification**: [brief explanation]
**Nodes Created**: [total count]
**Operations Executed**: [total count]
**Efficiency**: [operations per quality point]
```

### 4. Next Steps

```markdown
**Recommended Next Action**: [synthesizer | research-executor | citation-validator | none]
**Reason**: [why this is the next step]
**Handoff Data**: [what the next skill needs]
```

## Operation Decision Matrix

| Score | Action | Rationale |
|-------|--------|-----------|
| â‰¥ 8.5 | Generate(2-3) | é«˜è´¨é‡è·¯å¾„å€¼å¾—æ·±å…¥æ¢ç´¢ |
| 7.0-8.4 | Refine(1) | å†…å®¹è‰¯å¥½ï¼Œéœ€æ‰“ç£¨ |
| 6.0-6.9 | Aggregate if multiple available | ä¸­ç­‰è´¨é‡ï¼Œåˆå¹¶æå‡ |
| < 6.0 | Prune | ä½è´¨é‡ï¼Œä¸¢å¼ƒ |

## Remember

You are the **GoT Controller** - you orchestrate the research as a graph, making strategic decisions about which paths to explore, which to prune, and how to combine findings. Your goal is to optimize research quality while managing complexity.

**Core Philosophy**: Better to explore 3 paths deeply than 10 paths shallowly.

**Your Superpower**: Parallel exploration + strategic pruning = higher quality than sequential research.
</file>

<file path=".claude/skills/got-controller/SKILL.md">
---
name: got-controller
description: Graph of Thoughts (GoT) Controller - ç®¡ç†ç ”ç©¶å›¾çŠ¶æ€ï¼Œæ‰§è¡Œå›¾æ“ä½œï¼ˆGenerate, Aggregate, Refine, Scoreï¼‰ï¼Œä¼˜åŒ–ç ”ç©¶è·¯å¾„è´¨é‡ã€‚å½“ç ”ç©¶ä¸»é¢˜å¤æ‚æˆ–å¤šæ–¹é¢ã€éœ€è¦ç­–ç•¥æ€§æ¢ç´¢ï¼ˆæ·±åº¦ vs å¹¿åº¦ï¼‰ã€é«˜è´¨é‡ç ”ç©¶æ—¶ä½¿ç”¨æ­¤æŠ€èƒ½ã€‚
---

# GoT Controller

## Overview

Orchestrate complex multi-agent research using Graph of Thoughts framework, optimizing information quality through strategic operations.

## When to Use

- Research topic is complex and multifaceted
- Need strategic exploration (depth vs breadth decisions)
- High-stakes research requiring quality optimization
- Exploratory research where optimal path is unclear

## Core Concepts

**Graph Elements**:

- **Nodes**: Research findings with quality scores (0-10)
- **Edges**: Dependencies between findings
- **Frontier**: Active nodes available for exploration
- **Operations**: Generate, Aggregate, Refine, Score, KeepBestN

## GoT Operations

1. **Generate(k)**: Create k parallel research paths
2. **Aggregate(k)**: Combine k findings into synthesis
3. **Refine(1)**: Improve existing finding
4. **Score**: Evaluate quality (0-10 scale)
5. **KeepBestN(n)**: Prune to top n nodes

## Execution Patterns

- **Breadth-First**: Wide exploration â†’ prune â†’ aggregate
- **Depth-First**: Find best path â†’ drill deep â†’ refine
- **Balanced**: Mix of breadth and depth based on scores

## Decision Logic

- **Generate**: Score â‰¥ 7.0, explore multiple angles
- **Aggregate**: After 2-3 generation rounds
- **Refine**: Score â‰¥ 6.0, needs polish
- **Prune**: Score < 6.0 or redundant

## State Management

Track graph state in `research_notes/got_graph_state.md`:

- Node IDs, scores, parents, children, status
- Operations log with timestamps
- Frontier nodes for next iteration

## Safety Limits

| Limit | Value | Action if Exceeded |
|-------|-------|-------------------|
| Max iterations | 10 | Force aggregation and terminate |
| Max nodes | 50 | Prune lowest-scoring nodes |
| Max time | 30 min | Checkpoint and notify user |
| Min score threshold | 3.0 | Auto-prune nodes below |

## Termination Conditions

Research completes when **any** of these are met:
- Final aggregate score â‰¥ 9.0
- 3 consecutive refinements yield < 0.2 improvement
- Max iterations (10) reached
- All frontier nodes have score < 6.0

## Examples

See [examples.md](./examples.md) for execution patterns.

## Detailed Instructions

See [instructions.md](./instructions.md) for complete GoT methodology.
</file>

<file path=".claude/skills/question-refiner/examples.md">
# Question Refiner Skill - Examples

## Example 1: Business Intelligence Research

### User Input

```
"I need research on AI in healthcare"
```

### Question Refiner Response

```
I'll help you research AI in healthcare. Let me ask some questions to ensure I deliver exactly what you need:

**1. Specific Focus**: What aspects of AI in healthcare interest you most?
   - Clinical diagnosis and treatment
   - Drug discovery and development
   - Hospital operations and management
   - Patient care and monitoring
   - Medical imaging and radiology
   - All of the above

**2. Output Format**: What type of deliverable would be most useful?
   - Comprehensive report (20-30 pages)
   - Executive summary (3-5 pages)
   - Presentation slides
   - Data analysis with visualizations

**3. Scope**: Any specific constraints?
   - Geographic focus (US, Europe, Global?)
   - Time period (current state, last 5 years, future projections?)
   - Healthcare sectors (hospitals, pharma, insurance, tech companies?)

**4. Audience**: Who will be reading this research?
   - Technical team
   - Business executives
   - Healthcare professionals
   - General audience
```

### User Provides Clarification

```
"Focus on clinical diagnosis, comprehensive report, global scope but emphasize US and Europe,
last 3 years plus future trends. Audience is healthcare executives considering AI adoption."
```

### Final Structured Prompt Generated

```markdown
### TASK

Analyze the current state and future trends of AI applications in clinical diagnosis and treatment,
with emphasis on market adoption, technology maturity, and implementation considerations for healthcare
executives making adoption decisions.

### CONTEXT/BACKGROUND

Healthcare organizations are evaluating AI tools for clinical diagnosis and treatment. This research
will inform C-suite executives (CEOs, CTOs, CMOs) at hospital systems and healthcare providers who are
considering AI adoption. The goal is to provide a strategic roadmap based on current market realities,
proven use cases, and realistic future projections.

### SPECIFIC QUESTIONS OR SUBTASKS

1. Current Market Landscape:
   - What AI clinical applications are currently in production use (not just pilot programs)?
   - What are the adoption rates among different healthcare provider segments?
   - Which clinical specialties have the most mature AI solutions?

2. Technology Assessment:
   - What is the current accuracy and reliability of AI diagnostic tools compared to human clinicians?
   - What are the technical limitations and failure modes?
   - How do different AI approaches (computer vision, NLP, predictive analytics) compare in clinical effectiveness?

3. Implementation Considerations:
   - What are the costs (implementation, training, maintenance) of AI clinical systems?
   - What integration challenges exist with existing healthcare IT systems (EHR, PACS, etc.)?
   - What regulatory approvals (FDA, CE marking) are required and what's the approval timeline?

4. Vendor Landscape:
   - Who are the leading vendors for AI clinical tools?
   - What are the key differentiators between vendors?
   - Which vendors have proven track records with large-scale deployments?

5. Future Outlook (2025-2028):
   - What technological breakthroughs are expected in the next 3 years?
   - How will regulatory landscapes evolve?
   - What emerging clinical applications show the most promise?

6. Case Studies:
   - What are 3-5 detailed case studies of successful AI clinical implementations?
   - What were the realized benefits (reduced diagnosis time, improved accuracy, cost savings)?
   - What lessons were learned from failures or challenges?

### KEYWORDS

AI clinical diagnosis, medical AI adoption, healthcare AI market, computer vision medical imaging,
AI diagnostic accuracy, FDA AI medical devices, clinical decision support systems, AI radiology,
AI pathology, healthcare AI ROI, telemedicine AI, AI treatment recommendations

### CONSTRAINTS

- **Timeframe**: Emphasis on 2022-present, with projections to 2028
- **Geography**: Global scope but prioritize US and European markets
- **Source Types**:
  - Peer-reviewed clinical trials and validation studies
  - Industry market reports (Gartner, Forrester, McKinsey healthcare)
  - FDA and EMA regulatory documents and approvals
  - Vendor case studies and technical documentation
  - Healthcare system press releases and implementation reports
- **Quality**: Prioritize sources with actual clinical deployment data over theoretical papers

### OUTPUT FORMAT

1. **Executive Summary** (2-3 pages)
   - Key findings and actionable recommendations
   - Market maturity assessment (Ready/Pilot/Experimental)
   - Top 3 strategic recommendations for healthcare executives

2. **Full Report** (25-30 pages)
   - Introduction and market overview
   - Current applications by clinical specialty (radiology, pathology, cardiology, etc.)
   - Technology deep-dive with accuracy benchmarks
   - Implementation guide with cost-benefit analysis
   - Vendor comparison matrix
   - Case studies (3-5 detailed examples)
   - Future roadmap with timelines
   - Risk assessment and mitigation strategies

3. **Decision Support Tools**:
   - Vendor evaluation checklist
   - ROI calculation framework
   - Implementation timeline template
   - Risk assessment matrix

4. **Visual Elements**:
   - Market size and growth charts (2022-2028)
   - Technology maturity timeline
   - Vendor comparison tables
   - Implementation flow diagram

5. **Citation Format**:
   - Inline numeric superscripts [1], [2], [3]
   - Full bibliography with URLs/DOIs
   - Include publication dates and source credibility ratings (A-E)

### FINAL INSTRUCTIONS

Remain concise yet comprehensive. Prioritize actionable insights over theoretical discussion.
Focus on what healthcare executives NEED TO KNOW to make adoption decisions.

**CRITICAL REQUIREMENTS**:
- Every claim must include: Author/Organization, Date, Source Title, URL/DOI
- Clinical accuracy claims must cite specific studies with sample sizes
- Cost/benefit claims must cite specific implementations or credible market research
- Distinguish between what's theoretically possible vs. what's production-ready
- Acknowledge limitations and uncertainties where they exist

If any aspect of this research scope is unclear, ask for clarification before proceeding.
```

---

## Example 2: Technical Deep-Dive

### User Input

```
"Research WebAssembly for me"
```

### Question Refiner Response

```
I'll help you research WebAssembly. To provide the most relevant information, I need to understand your specific needs:

**1. Research Angle**: What aspect of WebAssembly interests you most?
   - Technical architecture and how it works
   - Browser support and compatibility
   - Use cases and applications
   - Performance benchmarks
   - Comparison with alternatives (JavaScript, native code)
   - Future roadmap and upcoming features
   - All of the above

**2. Purpose**: Why are you researching WebAssembly?
   - Evaluating whether to use it in a project
   - Technical presentation or training material
   - Investment or product decision
   - General learning/curiosity
   - Other?

**3. Technical Depth**: What's your technical background?
   - Web developer (frontend/backend)
   - Systems programmer (C++, Rust, etc.)
   - Technical manager/architect
   - Non-technical

**4. Output Format**: What would be most useful?
   - Technical whitepaper
   - Implementation guide
   - Comparison report
   - Executive summary

**5. Specific Questions**: Is there anything specific you want to know?
   - Current browser support percentages?
   - Performance improvements over JavaScript?
   - How to compile code to WASM?
   - Leading companies using it in production?
```

---

## Example 3: Policy/Legal Research

### User Input

```
"I need to understand GDPR compliance for SaaS companies"
```

### Question Refiner Response

```
I'll help you research GDPR compliance for SaaS companies. Let me clarify a few things:

**1. Company Stage**: What describes your situation best?
   - Early-stage startup (pre-product)
   - Growth-stage (launching in Europe)
   - Established (already operating, reviewing compliance)
   - Preparing for GDPR audit

**2. Specific Concerns**: What aspects of GDPR are most relevant?
   - Data collection and consent management
   - Data storage and processing requirements
   - User rights (access, deletion, portability)
   - Data breach notification procedures
   - Cross-border data transfer
   - Vendor/supplier management
   - All of the above

**3. Geography**:
   - EU/EEA countries only?
   - Include UK (post-Brexit GDPR)?
   - European companies serving US customers?
   - US companies serving European customers?

**4. Output Format**:
   - Compliance checklist
   - Implementation guide
   - Legal analysis (for discussions with lawyers)
   - Technical recommendations for engineers

**5. Timeline**:
   - Focus on current (2024-2025) requirements?
   - Include upcoming regulatory changes?
```

---

## Example 4: Academic/Literature Review

### User Input

```
"Write a literature review on transformer models"
```

### Question Refiner Response

```
I'll help you create a literature review on transformer models. Let me clarify your requirements:

**1. Scope**: What aspect of transformers should I focus on?
   - Architecture and theoretical foundations
   - Applications (NLP, computer vision, multimodal)
   - Efficiency improvements (distillation, quantization, pruning)
   - Training techniques and optimizations
   - Interpretability and analysis
   - Specific domain applications (healthcare, finance, etc.)
   - Comprehensive overview of all aspects

**2. Academic Level**:
   - Undergraduate survey
   - Master's thesis literature review
   - PhD-level comprehensive review
   - Survey paper for publication

**3. Timeframe**:
   - From original "Attention Is All You Need" (2017) to present?
   - Focus on recent developments (last 2-3 years)?
   - Include historical context and earlier related work?

**4. Length**:
   - Brief overview (5-10 pages)
   - Standard literature review (15-25 pages)
   - Comprehensive survey (40+ pages)

**5. Key Papers**: Are there specific papers you want included?
   - Must-cite seminal papers?
   - Recent breakthroughs to highlight?
   - Your own work (if this is for a thesis)?

**6. Output Format**:
   - Academic paper with citations
   - Annotated bibliography
   - Visual timeline of developments
   - Categorization by application/domain
```

---

## Example 5: Competitive Analysis

### User Input

```
"Research the project management software market"
```

### Question Refiner Response

```
I'll help you research the project management software market. To provide the most valuable insights:

**1. Research Purpose**: What will this research inform?
   - Product development for a new PM tool
   - Investment decision (due diligence)
   - Marketing strategy (positioning against competitors)
   - Technology selection (which tool to adopt)

**2. Market Segment**:
   - Enterprise PM tools (Jira, Microsoft Project)
   - Collaborative/work management (Asana, Monday, Notion)
   - Technical/project tracking (Linear, Shortcut)
   - All-in-one suites (ClickUp, Smartsheet)
   - Open-source/self-hosted alternatives

**3. Key Metrics**: What data points matter most?
   - Market size and growth (CAGR)
   - User adoption and retention
   - Pricing models and revenue
   - Feature comparison matrix
   - User satisfaction scores
   - Technical capabilities and limitations

**4. Geography**:
   - Global market
   - North America focus
   - Europe focus
   - Specific countries

**5. Timeframe**:
   - Current state (2024-2025)
   - Historical trends (last 5 years)
   - Future projections (next 3-5 years)

**6. Output**:
   - Market landscape report
   - Competitive positioning map
   - Feature comparison tables
   - SWOT analysis for key players
```

---

## Pattern Recognition

Across these examples, notice the Question Refiner consistently:

1. **Starts with "I'll help you..."** - establishes collaborative tone
2. **Asks 4-6 focused questions** - not too many, not too few
3. **Provides multiple-choice options** - makes it easy for user to respond
4. **Probes for context/purpose** - understands WHY research is needed
5. **Clarifies technical depth** - matches output to user's expertise
6. **Confirms format preferences** - ensures deliverable matches needs
7. **Identifies constraints** - time, geography, source types

The structured prompts generated are:

- **Highly specific** - every field filled with concrete details
- **Action-oriented** - tell researchers exactly what to do
- **Well-structured** - clear hierarchy and organization
- **Citation-focused** - emphasize source requirements
- **User-centric** - reflect the user's stated needs
</file>

<file path=".claude/skills/question-refiner/instructions.md">
# Question Refiner Skill - Instructions

## Role

You are a **Deep Research Question Refiner** specializing in crafting, refining, and optimizing prompts for deep research. Your primary objectives are:

1. **Ask clarifying questions first** to ensure full understanding of the user's needs, scope, and context
2. **Generate structured research prompts** that follow best practices for deep research
3. **Eliminate the need for external tools** (like ChatGPT) - everything is done within Claude Code

## Core Directives

- **Do Not Answer the Research Query Directly**: Focus on prompt crafting, not solving the research request
- **Be Explicit & Skeptical**: If the user's instructions are vague or contradictory, request more detail
- **Enforce Structure**: Encourage the user to use headings, bullet points, or other organizational methods
- **Demand Constraints & Context**: Identify relevant timeframes, geographical scope, data sources, and desired output formats
- **Invite Clarification**: Prompt the user to clarify ambiguous instructions or incomplete details

## Interaction Flow

### Research Type Detection

Before asking questions, detect the research type to guide questioning:

| Research Type | Description | Question Pattern |
|---------------|-------------|------------------|
| **Exploratory** | "What is happening with X?" | Current state, trends, landscape |
| **Comparative** | "X vs Y comparison" | Criteria, trade-offs, recommendations |
| **Problem-Solving** | "How to solve X?" | Root causes, solutions, implementation |
| **Forecasting** | "What will happen with X?" | Projections, drivers, scenarios |
| **Deep Dive** | "Everything about X aspect of Y" | Technical details, comprehensive |

**Detection Keywords**:

- Exploratory: "what is", "overview", "landscape", "current state"
- Comparative: "vs", "versus", "compare", "difference", "better"
- Problem-Solving: "how to", "fix", "solve", "address", "overcome"
- Forecasting: "will", "future", "prediction", "forecast", "trend"
- Deep Dive: "comprehensive", "in-depth", "detailed", "everything about"

### Progressive Questioning Strategy

**DO NOT overwhelm users with 15+ questions at once.** Use a progressive, adaptive approach:

### Round 1: Core Questions (Always Ask)

Ask these 3 essential questions first:

1. **Research Focus**: What specific aspect of [topic] interests you most?
   - Provide 3-4 options based on the topic
   - Example: "For AI in healthcare, are you interested in: (a) Clinical diagnosis, (b) Drug discovery, (c) Hospital operations, or (d) All of the above?"

2. **Output Format**: What type of deliverable would be most useful?
   - Comprehensive report (20-30 pages)
   - Executive summary (3-5 pages)
   - Technical analysis
   - Market research

3. **Audience**: Who will be reading this research?
   - Technical team (engineers, researchers)
   - Business executives (CEO, board, investors)
   - General audience
   - Policymakers

### Round 2: Conditional Questions (Based on Round 1)

**Only ask relevant follow-up questions based on their answers:**

**If Academic/Technical Research**:

- Source requirements? (peer-reviewed only, preprints OK, industry reports)
- Technical depth? (highly technical, semi-technical, non-technical)

**If Business/Market Research**:

- Geographic scope? (global, US, Europe, specific regions)
- Time period? (current state, last 3 years, future projections)

**If High-Stakes Decision**:

- Specific data needed? (market size, growth rates, benchmarks)
- Regulatory considerations? (compliance, legal requirements)

### Round 3: Final Clarifications (If Needed)

Only ask if critical information is still missing:

- Exclusions: What should NOT be included?
- Special requirements: Any specific constraints or preferences?

### Step 2: Wait for User Response

**CRITICAL**: Do NOT generate the structured prompt until the user answers your questions. If they provide incomplete answers, ask targeted follow-up questions.

### Step 3: Generate Structured Prompt

Once you have sufficient clarity, generate a structured research prompt using this format:

```markdown
### TASK

[Clear, concise statement of what needs to be researched]

### CONTEXT/BACKGROUND

[Why this research matters, who will use it, what decisions it will inform]

### SPECIFIC QUESTIONS OR SUBTASKS

1. [First specific question]
2. [Second specific question]
3. [Third specific question]
...

### KEYWORDS

[keyword1, keyword2, keyword3, ...]

### CONSTRAINTS

- Timeframe: [specific date range]
- Geography: [specific regions]
- Source Types: [academic, industry, news, etc.]
- Length: [expected word count]
- Language: [if not English]

### OUTPUT FORMAT

- [Format 1: e.g., Executive Summary (1-2 pages)]
- [Format 2: e.g., Full Report (20-30 pages)]
- [Format 3: e.g., Data tables and visualizations]
- Citation style: [APA, MLA, Chicago, inline with URLs]
- Include: [checklists, roadmaps, blueprints if applicable]

### FINAL INSTRUCTIONS

Remain concise, reference sources accurately, and ask for clarification if any part of this prompt is unclear. Ensure every factual claim includes:
1. Author/Organization name
2. Publication date
3. Source title
4. Direct URL/DOI
5. Page numbers (if applicable)
```

## Structured Prompt Quality Checklist

Before delivering the structured prompt, verify:

- [ ] TASK is clear and specific (not vague like "research AI")
- [ ] CONTEXT explains why this research matters
- [ ] SPECIFIC QUESTIONS break down the topic into 3-7 concrete sub-questions
- [ ] KEYWORDS cover the main concepts and synonyms
- [ ] CONSTRAINTS specify timeframe, geography, and source types
- [ ] OUTPUT FORMAT is detailed with specific lengths and components
- [ ] FINAL INSTRUCTIONS emphasize citation requirements
- [ ] Research type is detected and reflected in prompt structure
- [ ] All required fields are complete (no placeholders or [TBD])

## Prompt Quality Validation Scoring

Calculate prompt quality before delivery (0-10 scale):

| Criteria | Points | Check |
|----------|--------|-------|
| TASK clarity | 0-2 | Specific, actionable, not vague |
| CONTEXT relevance | 0-1 | Explains why research matters |
| Questions quality | 0-2 | 3-7 concrete, specific sub-questions |
| Keywords coverage | 0-1 | Main concepts + synonyms |
| Constraints specificity | 0-2 | Time, geography, sources, length |
| Output detail | 0-2 | Format, components, citation style |

**Passing threshold**: â‰¥ 8/10. If below, revise the prompt.

## Standard Skill Output Format

Every Question Refiner execution must output:

### 1. Status

- `success`: Structured prompt generated successfully
- `partial`: User provided incomplete answers, prompt needs refinement
- `failed`: Unable to generate structured prompt

### 2. Artifacts Created

```markdown
- `research_notes/refined_prompt.md` - The structured research prompt
```

### 3. Quality Score

```markdown
**Prompt Quality**: [0-10]/10
**Justification**: [brief explanation of score]
**Research Type Detected**: [type]
**Questions Asked**: [number]
**User Responsiveness**: [high/medium/low]
```

### 4. Next Steps

```markdown
**Recommended Next Action**: [research-executor | got-controller | none]
**Reason**: [why this is the next step]
**Handoff Data**: [the structured prompt for the next skill]
```

## Common Patterns

### Academic Research

```
### TASK
Conduct a comprehensive literature review on [topic], focusing on [specific angle]

### CONTEXT/BACKGROUND
This research will support a [thesis/paper/grant proposal] on [topic]

### SPECIFIC QUESTIONS OR SUBTASKS
1. What are the main theoretical frameworks proposed since [year]?
2. What empirical evidence supports or challenges these frameworks?
3. What are the identified gaps in current research?
4. What methodological approaches are most commonly used?

### CONSTRAINTS
- Timeframe: [year]-present
- Sources: Peer-reviewed journals only
- Quality: Priority to journals with impact factor > [X]
```

### Business Intelligence

```
### TASK
Analyze the market landscape for [product/service/technology], focusing on [specific aspect]

### CONTEXT/BACKGROUND
This research will inform [strategic planning/investment decisions/product development]

### SPECIFIC QUESTIONS OR SUBTASKS
1. What is the current market size and growth trajectory?
2. Who are the key players and what are their market shares?
3. What are the key success factors and barriers to entry?
4. What are the emerging trends and disruptions expected in the next [X] years?

### CONSTRAINTS
- Timeframe: Last [X] years + [X]-year projections
- Sources: Industry reports, SEC filings, reputable market research
- Geography: [specific regions]
```

### Technology Assessment

```
### TASK
Evaluate [technology] for [specific use case], focusing on technical feasibility and implementation considerations

### CONTEXT/BACKGROUND
This research will guide [technology adoption decision/architecture choice/product development]

### SPECIFIC QUESTIONS OR SUBTASKS
1. What is the current maturity level of [technology]?
2. What are the key technical challenges and limitations?
3. What are the leading implementations and case studies?
4. What is the roadmap for future development?
5. What are the alternative technologies and how do they compare?

### CONSTRAINTS
- Timeframe: Emphasis on recent developments (last [X] years)
- Sources: Technical documentation, academic papers, developer communities
- Depth: Technical details suitable for [engineers/technical managers/CTOs]
```

## Handling Special Cases

### User Says "Just Research Everything"

Response: "I want to make sure I deliver exactly what you need. Let me ask a few quick questions to focus the research..."

### User Provides Extremely Vague Topic

Response: Ask specific questions to narrow down: "When you say [topic], are you more interested in [aspect A], [aspect B], or [aspect C]?"

### User Wants "Current State" Without Timeframe

Always clarify: "By 'current state', do you mean the last 6 months, last year, or last 3 years? This affects which sources I'll prioritize."

### User Doesn't Know What They Want

Provide a menu of options: "For [topic], I can research: (1) Technical overview, (2) Market analysis, (3) Implementation guide, (4) Future trends. Which of these would be most valuable, or would you like a combination?"

## Output Example

```
USER: "I need research on quantum computing"

YOU (Question Refiner):
I'll help you refine your quantum computing research question. Let me ask some clarifying questions to ensure I deliver exactly what you need:

**1. Specific Focus**: What aspects of quantum computing interest you most?
   - Hardware (qubits, quantum processors, error correction)
   - Software/algorithms (quantum algorithms, programming frameworks)
   - Applications (cryptography, optimization, machine learning, drug discovery)
   - Market/industry (investments, key players, adoption timelines)
   - All of the above

**2. Output Format**: What type of deliverable would be most useful?
   - Comprehensive technical report (20-30 pages)
   - Executive summary for business leaders (3-5 pages)
   - Market analysis with projections
   - Implementation guide for engineers

**3. Scope**: Any specific constraints?
   - Geographic focus (US, Europe, Global, specific countries?)
   - Time period (current state, last 3 years, future 5-year projections?)
   - Industry verticals (finance, healthcare, manufacturing, etc.?)

**4. Audience**: Who will be reading this?
   - Technical team (engineers, researchers)
   - Business executives (CEO, board, investors)
   - General audience
   - Policymakers

**5. Depth**: How technical should the report be?
   - Highly technical (assume quantum physics background)
   - Semi-technical (explain concepts, include some math)
   - Non-technical (focus on business implications, minimal math)

Please answer these questions so I can craft the perfect research prompt for your needs!
```

## Critical Success Factors

1. **Patience**: Never rush to generate the prompt. Better to ask one more question than deliver a vague prompt.
2. **Specificity**: Every field in the structured prompt should be filled with concrete, actionable details.
3. **User-Centric**: The prompt should reflect what the USER wants, not what YOU think they should want.
4. **Quality Over Speed**: A well-refined prompt saves hours of research time later.

## Remember

You are replacing ChatGPT's o3/o3-pro models for this task. The structured prompts you generate should be just as good or better than what ChatGPT would produce. This means:

- Ask MORE clarifying questions, not fewer
- Be MORE specific about constraints and output formats
- Provide BETTER structure and organization
- Ensure EVERY field is filled out completely

Your goal: The user should never feel the need to use ChatGPT for question refinement again.
</file>

<file path=".claude/skills/question-refiner/SKILL.md">
---
name: question-refiner
description: å°†åŸå§‹ç ”ç©¶é—®é¢˜ç»†åŒ–ä¸ºç»“æ„åŒ–çš„æ·±åº¦ç ”ç©¶ä»»åŠ¡ã€‚é€šè¿‡æé—®æ¾„æ¸…éœ€æ±‚ï¼Œç”Ÿæˆç¬¦åˆ OpenAI/Google Deep Research æ ‡å‡†çš„ç»“æ„åŒ–æç¤ºè¯ã€‚å½“ç”¨æˆ·æå‡ºç ”ç©¶é—®é¢˜ã€éœ€è¦å¸®åŠ©å®šä¹‰ç ”ç©¶èŒƒå›´ã€æˆ–æƒ³è¦ç”Ÿæˆç»“æ„åŒ–ç ”ç©¶æç¤ºè¯æ—¶ä½¿ç”¨æ­¤æŠ€èƒ½ã€‚
---

# Question Refiner

## Overview

Transform vague research questions into structured, actionable research prompts through strategic clarifying questions.

## When to Use

- User provides a raw, unstructured research question
- Research scope is unclear or too broad
- Need to generate structured prompt for research-executor
- User asks "research X" without specifics

## Core Approach

**Progressive Questioning Strategy**:

1. **Round 1** (3 core questions): Topic focus, output format, audience
2. **Round 2** (conditional): Scope, sources, special requirements based on Round 1 answers
3. **Generate**: Structured prompt with TASK, CONTEXT, QUESTIONS, KEYWORDS, CONSTRAINTS, OUTPUT_FORMAT

## Key Features

- **Adaptive Questioning**: Asks 3-6 questions, not overwhelming
- **Context-Aware**: Questions adapt based on research type (academic, business, technical)
- **Structured Output**: Generates complete research prompt ready for execution
- **Quality Checklist**: Validates prompt completeness before delivery

## Output Format

```markdown
### TASK
[Clear, specific research objective]

### CONTEXT/BACKGROUND
[Why this matters, who will use it]

### SPECIFIC QUESTIONS OR SUBTASKS
1-7 concrete sub-questions

### KEYWORDS
[Search terms and synonyms]

### CONSTRAINTS
- Timeframe, geography, source types, length

### OUTPUT FORMAT
- Deliverable specifications with citation style
```

## Examples

See [examples.md](./examples.md) for detailed interaction patterns.

## Detailed Instructions

See [instructions.md](./instructions.md) for complete questioning strategy.
</file>

<file path=".claude/skills/research-executor/examples.md">
# Research Executor Skill - Examples

## Example 1: AI in Healthcare Research

### Structured Input

```markdown
### TASK
Analyze the current state and future trends of AI applications in clinical diagnosis and treatment,
with emphasis on market adoption, technology maturity, and implementation considerations for healthcare
executives making adoption decisions.

### CONTEXT/BACKGROUND
Healthcare organizations are evaluating AI tools for clinical diagnosis and treatment. This research
will inform C-suite executives (CEOs, CTOs, CMOs) at hospital systems and healthcare providers who are
considering AI adoption.

### SPECIFIC QUESTIONS OR SUBTASKS
1. Current Market Landscape: What AI clinical applications are currently in production use?
2. Technology Assessment: What is the current accuracy and reliability of AI diagnostic tools?
3. Implementation Considerations: What are the costs and integration challenges?
4. Vendor Landscape: Who are the leading vendors and what are their differentiators?
5. Future Outlook (2025-2028): What technological breakthroughs are expected?
6. Case Studies: What are detailed examples of successful AI clinical implementations?

### KEYWORDS
AI clinical diagnosis, medical AI adoption, healthcare AI market, computer vision medical imaging,
AI diagnostic accuracy, FDA AI medical devices, clinical decision support systems

### CONSTRAINTS
- Timeframe: Emphasis on 2022-present, with projections to 2028
- Geography: Global scope but prioritize US and European markets
- Source Types: Peer-reviewed clinical trials, industry market reports, FDA regulatory documents

### OUTPUT FORMAT
1. Executive Summary (2-3 pages)
2. Full Report (25-30 pages)
3. Decision Support Tools (checklists, ROI framework)
4. Visual Elements (market charts, vendor comparison tables)
```

### Research Executor Response

**Phase 2: Retrieval Planning**

```markdown
## Research Plan

### Subtopics to Research:
1. **Current Market Landscape**
   - Search queries: ["AI clinical diagnosis market size 2024", "AI healthcare adoption rates statistics",
                      "FDA approved AI medical devices list", "clinical AI production deployments 2024"]
   - Target sources: Industry reports (Gartner, McKinsey Healthcare), FDA databases, market research

2. **Technology Assessment**
   - Search queries: ["AI diagnostic accuracy vs human clinicians studies", "computer vision medical imaging benchmarks",
                      "clinical decision support system effectiveness", "AI radiology accuracy meta-analysis"]
   - Target sources: Peer-reviewed journals, clinical validation studies, systematic reviews

3. **Implementation & ROI**
   - Search queries: ["AI clinical implementation costs case study", "healthcare AI ROI analysis",
                      "EHR AI integration challenges", "AI clinical workflow integration"]
   - Target sources: Case studies, vendor whitepapers, healthcare IT publications

4. **Vendor Landscape**
   - Search queries: ["leading AI clinical diagnosis vendors 2024", "medical AI companies comparison",
                      "healthcare AI vendor market share", "AI radiology vendors evaluation"]
   - Target sources: Industry analyst reports, vendor comparison studies, G2/Capterra reviews

5. **Case Studies**
   - Search queries: ["Mayo Clinic AI implementation case study", "Cleveland Clinic AI diagnostic tools",
                      "European hospital AI deployment success stories", "AI clinical implementation failures lessons"]
   - Target sources: Hospital press releases, academic case studies, healthcare conferences

6. **Future Trends**
   - Search queries: ["AI healthcare trends 2025-2028 predictions", "emerging medical AI technologies",
                      "FDA AI regulation roadmap", "AI clinical trials future applications"]
   - Target sources: Industry trend reports, expert forecasts, regulatory roadmaps

### Multi-Agent Deployment Strategy:
- **Phase 3**: Deploy 6 parallel research agents (1 per subtopic)
- **Phase 4**: Deploy 1 cross-reference agent for fact verification
- **Phase 5**: Synthesize findings into comprehensive report

### Output Structure:
```

RESEARCH/
â””â”€â”€ AI_Clinical_Diagnosis_2024/
    â”œâ”€â”€ README.md
    â”œâ”€â”€ executive_summary.md
    â”œâ”€â”€ full_report.md
    â”œâ”€â”€ data/
    â”‚   â”œâ”€â”€ market_statistics.md
    â”‚   â””â”€â”€ vendor_comparison.md
    â”œâ”€â”€ visuals/
    â”‚   â””â”€â”€ charts_descriptions.md
    â”œâ”€â”€ sources/
    â”‚   â”œâ”€â”€ bibliography.md
    â”‚   â””â”€â”€ source_quality_table.md
    â”œâ”€â”€ research_notes/
    â”‚   â””â”€â”€ agent_findings_summary.md
    â””â”€â”€ appendices/
        â””â”€â”€ methodology.md

```

Ready to proceed with research execution?
```

**Phase 3: Iterative Querying** (Launches 6 parallel Task agents)

**Phase 7: Final Output**

```markdown
# AI in Clinical Diagnosis - Research Report

## Executive Summary
[2-3 page summary with key findings and recommendations]

## Full Report
[25-30 page comprehensive analysis with full citations]

## Sources
[Complete bibliography with A-E quality ratings]

## Research Notes
[Detailed findings from each research agent]
```

---

## Example 2: Technology Assessment - WebAssembly

### Structured Input

```markdown
### TASK
Evaluate WebAssembly technology for production web applications, focusing on technical feasibility,
performance benefits, and implementation considerations.

### CONTEXT/BACKGROUND
Development team is considering adopting WebAssembly for performance-critical components.
This research will inform architecture decisions.

### SPECIFIC QUESTIONS OR SUBTASKS
1. What is the current maturity level of WebAssembly?
2. What are the performance improvements over JavaScript?
3. What are the key use cases and successful implementations?
4. What tooling and frameworks are available?
5. What are the limitations and browser compatibility issues?

### KEYWORDS
WebAssembly, WASM, browser performance, JavaScript alternatives, web performance optimization

### CONSTRAINTS
- Timeframe: Emphasis on 2023-2025
- Sources: Technical documentation, benchmark studies, developer community insights

### OUTPUT FORMAT
- Technical assessment report (10-15 pages)
- Performance comparison tables
- Implementation recommendations
- Code examples
```

### Research Executor Response

**Phase 2: Retrieval Planning**

```markdown
## Research Plan

### Subtopics to Research:
1. **WebAssembly Maturity & Adoption**
   - Search queries: ["WebAssembly browser support 2024", "WebAssembly production usage statistics",
                      "companies using WebAssembly", "WebAssembly roadmap features"]
   - Target sources: MDN documentation, browser vendor blogs, usage statistics

2. **Performance Benchmarks**
   - Search queries: ["WebAssembly vs JavaScript performance benchmark 2024",
                      "WebAssembly performance real world studies", "WASM computation speed tests"]
   - Target sources: Technical blogs, benchmark studies, academic papers

3. **Use Cases & Implementations**
   - Search queries: ["WebAssembly success stories case studies", "WebAssembly video encoding",
                      "WebAssembly image processing", "WebAssembly game engines"]
   - Target sources: Company blog posts, GitHub repositories, conference talks

4. **Tooling & Ecosystem**
   - Search queries: ["WebAssembly frameworks 2024", "wasm-bindgen tutorial", "WASM tooling comparison",
                      "WebAssembly development workflow"]
   - Target sources: Documentation sites, developer forums, GitHub projects

5. **Limitations & Challenges**
   - Search queries: ["WebAssembly limitations", "WebAssembly debugging challenges",
                      "WebAssembly garbage collection status", "WASM security concerns"]
   - Target sources: Technical discussions, issue trackers, expert analyses

### Output Structure:
```

RESEARCH/
â””â”€â”€ WebAssembly_Assessment_2024/
    â”œâ”€â”€ README.md
    â”œâ”€â”€ technical_assessment.md
    â”œâ”€â”€ performance_benchmarks.md
    â”œâ”€â”€ use_cases.md
    â”œâ”€â”€ tooling_guide.md
    â”œâ”€â”€ limitations.md
    â””â”€â”€ sources/
        â””â”€â”€ bibliography.md

```
```

---

## Example 3: Market Research - Project Management Software

### Structured Input

```markdown
### TASK
Analyze the project management software market for competitive intelligence and product positioning.

### CONTEXT/BACKGROUND
Our company is launching a new PM tool targeting software development teams.
This research will inform product differentiation and go-to-market strategy.

### SPECIFIC QUESTIONS OR SUBTASKS
1. What is the current market size and growth trajectory?
2. Who are the key competitors and what are their market positions?
3. What are the key features and pricing models?
4. What are the unmet needs and gaps in the market?
5. What are the emerging trends and disruptions?

### KEYWORDS
Project management software, SaaS PM tools, Jira alternatives, software development workflow,
agile project management, dev team productivity tools

### CONSTRAINTS
- Timeframe: 2022-2025 with projections to 2027
- Geography: Global market with emphasis on North America
- Sources: Industry reports, competitor analysis, user reviews

### OUTPUT FORMAT
- Market landscape report (15-20 pages)
- Competitive positioning matrix
- Feature comparison tables
- SWOT analysis for top 5 competitors
- Market opportunity assessment
```

### Research Executor Response

**Phase 2: Retrieval Planning**

```markdown
## Research Plan

### Subtopics to Research:
1. **Market Size & Growth**
   - Search queries: ["project management software market size 2024", "PM tools CAGR forecast",
                      "collaborative software market growth", "work management market trends"]
   - Target sources: Market research reports (Gartner, Forrester, IDC), industry analysis

2. **Competitive Landscape**
   - Search queries: ["project management software market share 2024", "Jira vs Asana vs Linear",
                      "PM tools for software teams comparison", "developer PM tool preferences"]
   - Target sources: Industry reports, user reviews, analyst rankings

3. **Feature Analysis**
   - Search queries: ["project management software features comparison", "agile PM tools must-have features",
                      "software development PM workflow features", "PM tool pricing models 2024"]
   - Target sources: Product documentation, comparison sites, user forums

4. **Market Gaps & Opportunities**
   - Search queries: ["project management software pain points", "developer PM tool frustrations",
                      "PM tool feature requests", "software team PM needs unmet"]
   - Target sources: User reviews, Reddit discussions, feedback forums

5. **Emerging Trends**
   - Search queries: ["project management software trends 2025", "AI in project management tools",
                      "future of work management software", "PM tool integration trends"]
   - Target sources: Industry predictions, trend reports, expert forecasts

### Output Structure:
```

RESEARCH/
â””â”€â”€ PM_Software_Market_2024/
    â”œâ”€â”€ README.md
    â”œâ”€â”€ market_overview.md
    â”œâ”€â”€ competitive_landscape.md
    â”œâ”€â”€ feature_analysis.md
    â”œâ”€â”€ market_opportunities.md
    â”œâ”€â”€ trends_2025.md
    â”œâ”€â”€ data/
    â”‚   â”œâ”€â”€ market_size_data.md
    â”‚   â””â”€â”€ competitor_features.md
    â””â”€â”€ sources/
        â””â”€â”€ bibliography.md

```
```

---

## Example 4: Academic Literature Review - Transformer Models

### Structured Input

```markdown
### TASK
Conduct a comprehensive literature review of transformer architectures in AI,
focusing on architectural innovations and efficiency improvements.

### CONTEXT/BACKGROUND
This literature review will support a PhD dissertation on efficient transformer architectures.
Target audience: AI researchers and graduate students.

### SPECIFIC QUESTIONS OR SUBTASKS
1. What are the foundational transformer architectures since "Attention Is All You Need" (2017)?
2. What efficiency improvements have been proposed (distillation, quantization, pruning)?
3. What are the key applications across different domains (NLP, vision, multimodal)?
4. What are the theoretical analyses and interpretability studies?
5. What are the current research gaps and future directions?

### KEYWORDS
Transformer architecture, attention mechanism, BERT, GPT, ViT, efficient transformers,
model distillation, quantization, pruning, scaling laws, transformer interpretability

### CONSTRAINTS
- Timeframe: 2017-present (from original transformer paper)
- Sources: Peer-reviewed papers, preprints (arXiv), top conferences (NeurIPS, ICML, ICLR, ACL)
- Quality: Prioritize papers with high citation counts and published in top-tier venues

### OUTPUT FORMAT
- Academic literature review (30-40 pages)
- Categorization by theme (architecture, efficiency, applications, theory)
- Timeline visualization of key developments
- Table comparing architectural innovations
- Bibliography with 50-100 key papers
```

### Research Executor Response

**Phase 2: Retrieval Planning**

```markdown
## Research Plan

### Subtopics to Research:
1. **Foundational Architectures**
   - Search queries: ["transformer architecture seminal papers 2017-2024",
                      "BERT GPT T5 architecture comparison", "vision transformer ViT review",
                      "transformer variants survey"]
   - Target sources: arXiv, Google Scholar, conference proceedings

2. **Efficiency Improvements**
   - Search queries: ["efficient transformer architectures survey", "model distillation transformer",
                      "transformer quantization pruning techniques", "sparse attention mechanisms"]
   - Target sources: Peer-reviewed papers, meta-analyses

3. **Applications by Domain**
   - Search queries: ["transformer NLP applications review", "vision transformer applications",
                      "multimodal transformer models", "transformer in biology chemistry"]
   - Target sources: Domain-specific conferences and journals

4. **Theoretical Analysis**
   - Search queries: ["transformer attention mechanism theoretical analysis",
                      "transformer scaling laws", "transformer interpretability techniques",
                      "why transformers work explanation"]
   - Target sources: Theoretical papers, analysis papers

5. **Research Gaps**
   - Search queries: ["transformer architecture limitations", "open problems transformer research",
                      "future directions transformer models", "unsolved transformer challenges"]
   - Target sources: Position papers, survey papers, workshop proceedings

### Output Structure:
```

RESEARCH/
â””â”€â”€ Transformer_Literature_Review/
    â”œâ”€â”€ README.md
    â”œâ”€â”€ executive_summary.md
    â”œâ”€â”€ full_review.md
    â”œâ”€â”€ architectural_innovations.md
    â”œâ”€â”€ efficiency_techniques.md
    â”œâ”€â”€ applications_by_domain.md
    â”œâ”€â”€ theoretical_foundations.md
    â”œâ”€â”€ research_gaps.md
    â”œâ”€â”€ timeline/
    â”‚   â””â”€â”€ key_developments.md
    â”œâ”€â”€ tables/
    â”‚   â””â”€â”€ architecture_comparison.md
    â””â”€â”€ bibliography.md

```
```

---

## Example 5: Policy Research - GDPR Compliance

### Structured Input

```markdown
### TASK
Provide a comprehensive analysis of GDPR compliance requirements for SaaS companies,
with focus on practical implementation guidance for US-based companies serving European customers.

### CONTEXT/BACKGROUND
US-based SaaS company is preparing to launch in European markets.
This research will guide compliance implementation and legal discussions.

### SPECIFIC QUESTIONS OR SUBTASKS
1. What are the core GDPR requirements for SaaS companies?
2. What data handling and consent management practices are required?
3. What technical controls and security measures must be implemented?
4. What are the documentation and compliance demonstration requirements?
5. What are the penalties for non-compliance and enforcement trends?
6. What are the recent regulatory updates and court interpretations (2022-2024)?

### KEYWORDS
GDPR compliance, SaaS data protection, EU data regulation, Article 25 privacy by design,
data processing agreement, DPIA, DSAR, GDPR fines, EDPB guidance

### CONSTRAINTS
- Timeframe: Current GDPR requirements (2024) with emphasis on 2022-2024 updates
- Geography: EU/EEA requirements, UK GDPR post-Brexit
- Sources: Official EU documents, EDPB guidance, legal analysis, case law

### OUTPUT FORMAT
- Compliance implementation guide (20-25 pages)
- Requirement checklist with priority levels
- Technical control recommendations
- Documentation templates overview
- Risk assessment framework
```

### Research Executor Response

**Phase 2: Retrieval Planning**

```markdown
## Research Plan

### Subtopics to Research:
1. **Core GDPR Requirements**
   - Search queries: ["GDPR requirements SaaS companies 2024", "GDPR data controller processor",
                      "GDPR compliance checklist software", "EDPB guidance SaaS"]
   - Target sources: Official EU GDPR text, EDPB guidelines, ICO documentation

2. **Data Handling & Consent**
   - Search queries: ["GDPR consent management requirements", "lawful basis data processing SaaS",
                      "data subject rights implementation", "cookie consent GDPR 2024"]
   - Target sources: Regulatory guidance, case law, compliance frameworks

3. **Technical Controls**
   - Search queries: ["GDPR technical security measures", "privacy by design implementation SaaS",
                      "data anonymization pseudonymization", "GDPR encryption requirements"]
   - Target sources: Technical guidelines, security frameworks, ENISA standards

4. **Documentation & Demonstration**
   - Search queries: ["GDPR documentation requirements", "records of processing activities template",
                      "DPIA requirement SaaS", "GDPR compliance evidence"]
   - Target sources: Regulatory templates, official guidance

5. **Enforcement & Penalties**
   - Search queries: ["GDPR fines 2022 2023 2024", "GDPR enforcement trends SaaS",
                      "Article 83 GDPR penalties", "EDPB sanction guidelines"]
   - Target sources: Enforcement decisions, penalty databases, legal analysis

6. **Recent Updates**
   - Search queries: ["GDPR updates 2024", "Schrems II decision impact", "EDPB guidelines 2022-2024",
                      "Data Act EU 2024", "UK GDPR changes post-Brexit"]
   - Target sources: Recent regulatory updates, court decisions, legal analysis

### Output Structure:
```

RESEARCH/
â””â”€â”€ GDPR_Compliance_Guide_2024/
    â”œâ”€â”€ README.md
    â”œâ”€â”€ executive_summary.md
    â”œâ”€â”€ compliance_guide.md
    â”œâ”€â”€ requirements_checklist.md
    â”œâ”€â”€ technical_implementation.md
    â”œâ”€â”€ documentation_templates/
    â”‚   â””â”€â”€ overview.md
    â”œâ”€â”€ risk_assessment.md
    â”œâ”€â”€ enforcement_overview.md
    â”œâ”€â”€ recent_updates.md
    â””â”€â”€ sources/
        â””â”€â”€ legal_references.md

```
```

---

## Pattern Recognition

Across these examples, the Research Executor consistently:

1. **Decomposes complex topics** into 3-7 actionable subtopics
2. **Generates targeted search queries** for each subtopic
3. **Identifies appropriate sources** based on constraints
4. **Creates clear output structure** before research begins
5. **Presents plan for approval** before launching agents
6. **Launches parallel agents** for efficient research
7. **Delivers structured outputs** matching user requirements

### Key Success Factors

- **Thorough Planning**: Research plans are detailed and specific
- **Source Awareness**: Matches sources to research domain (academic vs industry vs legal)
- **User Communication**: Presents plans clearly and seeks approval
- **Structured Outputs**: Organizes findings in accessible formats
- **Citation Discipline**: Always maintains source traceability
</file>

<file path=".claude/skills/research-executor/instructions.md">
# Research Executor Skill - Instructions

## Role

You are a **Deep Research Executor** responsible for conducting comprehensive, multi-phase research using the 7-stage deep research methodology and Graph of Thoughts (GoT) framework. Your role is to transform structured research prompts into well-cited, comprehensive research reports.

## Core Responsibilities

1. **Execute the 7-Phase Deep Research Process**
2. **Deploy Multi-Agent Research Strategy**
3. **Ensure Citation Accuracy and Quality**
4. **Generate Structured Research Outputs**

## The 7-Phase Deep Research Process

### Phase 1: Question Scoping âœ“ (Already Done)

The question has already been refined by the `question-refiner` skill. You will receive a structured research prompt with clear TASK, CONTEXT, SPECIFIC_QUESTIONS, KEYWORDS, CONSTRAINTS, and OUTPUT_FORMAT.

**Your job**: Verify the structured prompt is complete and ask for clarification if any critical information is missing.

### Phase 2: Retrieval Planning

Break down the main research question into actionable subtopics and create a research plan.

**Actions**:

1. Decompose the main question into 3-7 subtopics based on SPECIFIC_QUESTIONS
2. Generate specific search queries for each subtopic
3. Identify appropriate data sources based on CONSTRAINTS
4. Create a research execution plan
5. Present the plan to user for approval (if this is an interactive session)

**Output Structure**:

```markdown
## Research Plan

### Subtopics to Research:
1. **[Subtopic 1 Name]**
   - Research questions: [specific questions]
   - Search queries: [query 1, query 2, query 3]
   - Target sources: [source types]

2. **[Subtopic 2 Name]**
   - Research questions: [specific questions]
   - Search queries: [query 1, query 2, query 3]
   - Target sources: [source types]

...

### Multi-Agent Deployment Strategy:
- **Phase 2 Agents**: [number] parallel research agents
- **Phase 3 Strategy**: [web research, academic, verification agents]
- **Expected Timeline**: [estimate]

### Output Structure:
[Describe the folder and file structure that will be created]

Ready to proceed? (User: Yes/No/Modifications needed)
```

### Phase 3: Iterative Querying (Multi-Agent Execution)

Deploy multiple Task agents in parallel to gather information from different sources.

#### Token-Optimized Document Processing Pipeline

**CRITICAL**: To reduce token consumption by 60-90%, use the "Download â†’ Clean â†’ Read" workflow instead of reading raw HTML directly.

**Step 1: Fetch & Save Raw Data**
```markdown
- **DO NOT** directly use `WebFetch` content in the Context Window.
- After fetching content, immediately save to: `RESEARCH/[topic]/data/raw/[source_id].html`
- This keeps raw data out of the LLM context.
```

**Step 2: Pre-process Document**
```bash
python3 scripts/preprocess_document.py RESEARCH/[topic]/data/raw/[source_id].html
```

The script returns JSON with:
- `output_path`: Path to cleaned markdown file
- `saved_tokens`: Number of tokens saved
- `savings_percent`: Percentage reduction

**Step 3: Read Processed Data**
```markdown
- Use `Read` tool to access cleaned markdown from `data/processed/`
- If file is still large (>10k tokens), read first 50 lines for summary
- Then read specific sections by line number as needed
```

**Example Workflow**:
```markdown
1. Agent fetches: https://example.com/research-report
2. Agent saves to: RESEARCH/topic/data/raw/example_report.html (original: ~50k tokens)
3. Agent runs: python3 scripts/preprocess_document.py RESEARCH/topic/data/raw/example_report.html
4. Script outputs: data/processed/example_report_cleaned.md (~5k tokens, 90% saved)
5. Agent reads processed file with full context available
```

**Benefits**:
- Removes ads, navigation, scripts, styles
- Extracts only core content
- Preserves metadata in YAML frontmatter
- Typical savings: 60-90% token reduction

---

#### Task Complexity Assessment

Before deploying agents, assess the research complexity to optimize resource allocation:

| Research Type | Subtopics | Agent Count | Model Selection | Est. Time |
|---------------|-----------|-------------|-----------------|-----------|
| Quick Query | 1-2 | 2-3 | All haiku | 5-10 min |
| Standard Research | 3-5 | 4-5 | 2 sonnet + 3 haiku | 15-30 min |
| Deep Research | 5-7 | 6-8 | 3-4 sonnet + rest haiku | 45-90 min |
| Academic/Technical | 5+ | 7-8 | 4 sonnet + 4 haiku | 1-2 hours |
| Contradictory Sources | 3+ | +1 verification | All sonnet for verification | +30 min |

**Complexity Indicators**:

| Indicator | Low | Medium | High |
|-----------|-----|--------|------|
| Domain Scope | Single domain | Multi-domain | Cross-domain |
| Time Range | Recent only | 3-5 years | Historical + future |
| Ambiguity | Clear scope | Some ambiguity | High ambiguity |
| Stakes | Informational | Tactical | Strategic/High-stakes |
| Source Conflict | Expected consensus | Some disagreement | Contradictory sources expected |

**Agent Count Decision Matrix**:

| Conditions | Agent Count | Breakdown |
|------------|-------------|-----------|
| 1-2 subtopics, recent info | 3 | 2 web research + 1 cross-reference |
| 3-5 subtopics, technical | 5 | 3 web research + 1 academic + 1 cross-reference |
| 5+ subtopics, academic | 8 | 4 web research + 2 academic + 1 cross-reference + 1 verification |
| Contradictory sources expected | Base + 1 | Add dedicated verification agent |

**Model Selection Guide**:

| Agent Type | Use haiku when | Use sonnet when |
|------------|----------------|-----------------|
| Web Research | Recent news, straightforward topics | Complex analysis, cross-domain synthesis |
| Academic | Simple paper location | Deep technical analysis, methodology review |
| Cross-Reference | Basic fact-checking | Contradiction resolution, source triangulation |

#### Phase Transition Criteria

**Phase Gates** (must be met before proceeding):

| Phase | Exit Criteria | Decision |
|-------|---------------|----------|
| Phase 1 â†’ 2 | User approves refined question | Ask if unclear |
| Phase 2 â†’ 3 | Research plan approved | Present plan, wait for approval |
| Phase 3 â†’ 4 | All agents return findings | Wait for all agents |
| Phase 4 â†’ 5 | Source triangulation complete | Cross-check findings |
| Phase 5 â†’ 6 | Draft synthesis score â‰¥ 7.0 | Refine if below threshold |
| Phase 6 â†’ 7 | Validation score â‰¥ 8.0 | Fix issues before output |

**Phase 3: Parallel Agent Launch Pattern**:

```markdown
Launch ALL agents in a single response using multiple Task tool calls:

# Example: 4 parallel agents
Task(agent_1, "Research aspect A: [specific focus]...")
Task(agent_2, "Research aspect B: [specific focus]...")
Task(agent_3, "Research aspect C: [specific focus]...")
Task(agent_4, "Cross-reference verification of claims from agents 1-3...")
```

**Execution Protocol**:

1. **Launch ALL agents in a single response** using multiple Task tool calls
2. Use `run_in_background: true` for long-running agents
3. Collect results using TaskOutput when agents complete
4. Track agent progress with TodoWrite

**Agent Types and Deployment**:

#### Agent Type 1: Web Research Agents (3-5 agents)

**Focus**: Current information, trends, news, industry reports

**Agent Template**:

```
Research [specific aspect] of [main topic]. Use the following tools:
1. Start with WebSearch to find relevant sources
2. Use WebFetch to extract content from promising URLs
3. Use mcp__web-reader__webReader for better content extraction
4. Use mcp__zai-mcp-server__analyze_image if you encounter relevant charts/graphs

Focus on finding:
- Recent information (prioritize sources from [timeframe])
- Authoritative sources matching [source quality requirements]
- Specific data/statistics with verifiable sources
- Multiple perspectives on the topic

Provide a structured summary with:
- Key findings
- All source URLs with full citations
- Confidence ratings for claims (High/Medium/Low)
- Any contradictions or gaps found
```

#### Agent Type 2: Academic/Technical Agent (1-2 agents)

**Focus**: Research papers, technical specifications, methodologies

**Agent Template**:

```
Find technical/academic information about [topic aspect].

Tools to use:
1. WebSearch for academic papers and technical resources
2. WebFetch for PDF extraction and content analysis
3. Save important findings to files using Read/Write tools

Look for:
- Peer-reviewed papers
- Technical specifications
- Methodologies and frameworks
- Scientific evidence
- Expert consensus

Include proper academic citations:
- Author names, publication year
- Paper title, journal/conference name
- DOI or direct URL
- Key findings and sample sizes
```

#### Agent Type 3: Cross-Reference Agent (1 agent)

**Focus**: Fact-checking and verification

**Agent Template**:

```
Verify the following claims about [topic]:
[List key claims from other agents]

Use multiple search queries with WebSearch to find:
- Supporting evidence
- Contradicting information
- Original sources

For each claim, provide:
- Confidence rating: High/Medium/Low
- Supporting sources (minimum 2 for high confidence)
- Contradicting sources (if any)
- Explanation of any discrepancies
```

**Execution Protocol**:

1. **Launch ALL agents in a single response** using multiple Task tool calls
2. Use `run_in_background: true` for long-running agents
3. Collect results using TaskOutput when agents complete
4. Track agent progress with TodoWrite

### Phase 4: Source Triangulation

Compare findings across multiple sources and validate claims.

**Actions**:

1. Compile findings from all agents
2. Identify overlapping conclusions (high confidence)
3. Note contradictions between sources
4. Assess source credibility using A-E rating system
5. Resolve inconsistencies by finding authoritative sources

**Source Quality Ratings**:

- **A**: Peer-reviewed RCTs, systematic reviews, meta-analyses
- **B**: Cohort studies, case-control studies, clinical guidelines
- **C**: Expert opinion, case reports, mechanistic studies
- **D**: Preliminary research, preprints, conference abstracts
- **E**: Anecdotal, theoretical, or speculative

**Output for Each Claim**:

```markdown
**Claim**: [statement]

**Evidence**:
- Source 1: [Author, Year, Title, URL] - Rating: [A-E]
- Source 2: [Author, Year, Title, URL] - Rating: [A-E]
- Source 3: [Author, Year, Title, URL] - Rating: [A-E]

**Confidence**: High/Medium/Low

**Notes**: [any contradictions, limitations, or context]
```

### Phase 5: Knowledge Synthesis

Structure and write comprehensive research sections.

**Actions**:

1. Organize content logically according to SPECIFIC_QUESTIONS
2. Write comprehensive sections
3. Include inline citations for EVERY claim
4. Add data visualizations when relevant
5. Create clear narrative flow

**Citation Format Requirements**:
Every factual claim MUST include:

1. **Author/Organization** - Who made this claim
2. **Date** - When the information was published
3. **Source Title** - Name of paper, article, or report
4. **URL/DOI** - Direct link to verify the source
5. **Page Numbers** - For lengthy documents (when applicable)

**Inline Citation Examples**:

```
Good: "According to a study by Smith et al. (2023), metformin reduces diabetes incidence by 31% (Smith et al., 2023, NEJM, https://doi.org/10.xxxx/xxxxx)."

Poor: "Studies show that metformin reduces diabetes risk." (NO SOURCE)

Acceptable: "Multiple industry reports suggest the market will grow to $50B by 2025 (Gartner, 2024; McKinsey, 2024)."
```

**Section Structure**:

```markdown
## [Section Title]

[Opening paragraph providing context]

### Subsection 1
[Content with inline citations]

### Subsection 2
[Content with inline citations]

**Key Findings Summary**:
- Finding 1 [citation]
- Finding 2 [citation]
- Finding 3 [citation]
```

### Phase 6: Quality Assurance

Check for hallucinations, verify citations, ensure completeness.

**Chain-of-Verification Process**:

1. **Generate Initial Findings** â†’ (already done in Phase 5)

2. **Create Verification Questions**:
   For each key claim, ask: "Is this statement accurate? What is the source?"

3. **Search for Evidence**:
   Use WebSearch to verify critical claims from scratch

4. **Final Verification**:
   Cross-reference verification results with original findings

**Quality Checklist**:

- [ ] Every claim has a verifiable source
- [ ] Multiple sources corroborate key findings
- [ ] Contradictions are acknowledged and explained
- [ ] Sources are recent and authoritative
- [ ] No hallucinations or unsupported claims
- [ ] Clear logical flow from evidence to conclusions
- [ ] Proper citation format throughout
- [ ] All URLs are accessible
- [ ] No broken or suspicious links

**Hallucination Prevention**:

- If uncertain about a fact, state: "Source needed to verify this claim"
- Never invent statistics or quotes
- Always provide URLs for verification
- Distinguish between proven facts and expert opinions
- Explicitly state limitations

### Phase 7: Output & Packaging

Format and deliver the final research output.

**Required Output Structure**:

Create a folder in the output directory:

```
[output_directory]/
â””â”€â”€ [topic_name]/
    â”œâ”€â”€ README.md (Overview and navigation guide)
    â”œâ”€â”€ executive_summary.md (1-2 page summary)
    â”œâ”€â”€ full_report.md (Comprehensive findings)
    â”œâ”€â”€ data/
    â”‚   â”œâ”€â”€ statistics.md
    â”‚   â””â”€â”€ key_facts.md
    â”œâ”€â”€ visuals/
    â”‚   â””â”€â”€ descriptions.md (describe charts/graphs that could be created)
    â”œâ”€â”€ sources/
    â”‚   â”œâ”€â”€ bibliography.md (Full citations)
    â”‚   â””â”€â”€ source_quality_table.md (A-E ratings)
    â”œâ”€â”€ research_notes/
    â”‚   â””â”€â”€ agent_findings_summary.md
    â””â”€â”€ appendices/
        â”œâ”€â”€ methodology.md
        â””â”€â”€ limitations.md
```

**README.md Template**:

```markdown
# [Research Topic] - Deep Research Report

## Overview
This report contains comprehensive research on [topic], conducted on [date].

## Contents
1. **Executive Summary** (1-2 pages) - Key findings and recommendations
2. **Full Report** ([XX] pages) - Complete analysis with citations
3. **Data & Statistics** - Key numbers and facts
4. **Sources** - Complete bibliography with quality ratings
5. **Research Notes** - Detailed agent findings
6. **Appendices** - Methodology and limitations

## Quick Start
Read the [Executive Summary](executive_summary.md) for key findings.
Refer to the [Full Report](full_report.md) for detailed analysis.

## Research Quality
- **Total Sources**: [number]
- **High-Quality Sources (A-B)**: [number]
- **Recent Sources (last [X] years)**: [number]%
- **Citation Coverage**: 100% (all claims sourced)
```

**executive_summary.md Template**:

```markdown
# Executive Summary: [Research Topic]

## Key Findings

1. **[Finding 1]**
   [1-2 sentence summary with citation]

2. **[Finding 2]**
   [1-2 sentence summary with citation]

3. **[Finding 3]**
   [1-2 sentence summary with citation]

## Recommendations

[Based on the research, provide actionable recommendations]

## Methodology Summary

This research used:
- [Number] parallel research agents
- [Number] sources verified
- 7-phase deep research process
- Graph of Thoughts (GoT) framework

## Confidence Levels

| Claim Area | Confidence | Key Sources |
|------------|-----------|-------------|
| [Area 1] | High/Medium/Low | [source citations] |
| [Area 2] | High/Medium/Low | [source citations] |

## Limitations

[What could NOT be determined, gaps in research, uncertainties]

---

**Generated**: [Date]
**Research Method**: 7-Phase Deep Research with GoT
**Total Research Time**: [duration]
```

## Graph of Thoughts (GoT) Integration

While the basic 7-phase process above is sufficient for most research, you can enhance it with GoT operations for complex topics.

### GoT Operations Available

1. **Generate(k)**: Create k parallel research paths from a node
2. **Aggregate(k)**: Combine k findings into one stronger synthesis
3. **Refine(1)**: Improve and polish existing findings
4. **Score**: Evaluate information quality (0-10 scale)
5. **KeepBestN(n)**: Keep only the top n findings at each level

### When to Use GoT

Use GoT enhancement for:

- **Complex, multifaceted topics** (e.g., "AI safety across multiple domains")
- **High-stakes research** (medical, legal, financial decisions)
- **Exploratory research** where the optimal path is unclear

### GoT Execution Pattern

```markdown
**Iteration 1**: Initial Exploration
- Create 3 parallel research agents:
  * Agent A: Focus on [aspect 1]
  * Agent B: Focus on [aspect 2]
  * Agent C: Focus on [aspect 3]
- Score each agent's findings (0-10)
- Result: Finding A (7.5), Finding B (8.2), Finding C (6.8)

**Iteration 2**: Deepen Best Paths
- Finding B (8.2): Generate 2 agents to explore deeper
- Finding A (7.5): Refine with additional research
- Finding C (6.8): Discard or merge with others

**Iteration 3**: Aggregate
- Aggregate findings from best paths
- Create comprehensive synthesis

**Iteration 4**: Final Polish
- Refine synthesis for clarity and completeness
- Final score: 9.3
```

## Tool Usage Guidelines

### WebSearch

- Use for initial source discovery
- Try multiple query variations
- Use domain filtering for authoritative sources
- Include date-specific queries for recent information

### WebFetch / mcp__web-reader__webReader

- Use for extracting content from specific URLs
- Prefer mcp__web-reader__webReader for better content extraction
- Request specific information to avoid getting entire pages
- Archive important content to local files

### Image Analysis Tools (mcp__zai-mcp-server)

- Use `mcp__zai-mcp-server__analyze_image` for general image analysis
- Use `mcp__zai-mcp-server__analyze_data_visualization` for charts and graphs
- Extract data from visual sources
- Prompt: "Describe the data presented in this image, including labels, numbers, and trends"

### Task (Multi-Agent Deployment)

- **CRITICAL**: Launch multiple agents in ONE response
- Use `subagent_type="general-purpose"` for research agents
- Provide clear, detailed prompts to each agent
- Use `run_in_background: true` for long tasks
- Monitor progress with TodoWrite

### Read/Write

- Save research findings to files regularly
- Create organized folder structure
- Maintain source-to-claim mapping files
- Archive agent outputs for reference

### TodoWrite

- Track all research phases
- Mark items as in_progress/completed in real-time
- Create granular todos for multi-step processes

## Common Research Scenarios

### Scenario 1: Market Research

```markdown
**Focus**: Market size, growth, competition, trends

**Agent Deployment**:
- Agent 1: Current market size and growth data
- Agent 2: Key players and market shares
- Agent 3: Emerging trends and disruptions
- Agent 4: Consumer adoption and behavior

**Key Metrics to Find**:
- Total Addressable Market (TAM)
- Compound Annual Growth Rate (CAGR)
- Market share percentages
- Growth drivers and barriers
```

### Scenario 2: Technology Assessment

```markdown
**Focus**: Technical capabilities, limitations, use cases

**Agent Deployment**:
- Agent 1: Technical specifications and capabilities
- Agent 2: Current implementations and case studies
- Agent 3: Limitations and failure modes
- Agent 4: Competitive technologies

**Key Information to Find**:
- Performance benchmarks
- Technical maturity level
- Real-world adoption data
- Comparison with alternatives
```

### Scenario 3: Academic Literature Review

```markdown
**Focus**: Peer-reviewed research, methodologies, consensus

**Agent Deployment**:
- Agent 1: Seminal papers and theoretical foundations
- Agent 2: Recent empirical studies (last 3-5 years)
- Agent 3: Meta-analyses and systematic reviews
- Agent 4: Ongoing research and preprints

**Key Information to Find**:
- Sample sizes and statistical significance
- Replicated findings
- Gaps and contradictions in literature
- Emerging research directions
```

### Scenario 4: Policy/Legal Research

```markdown
**Focus**: Regulations, compliance, case law

**Agent Deployment**:
- Agent 1: Current regulations and guidelines
- Agent 2: Regulatory body positions and interpretations
- Agent 3: Case law and enforcement actions
- Agent 4: Upcoming regulatory changes

**Key Information to Find**:
- Specific regulatory citations
- Compliance requirements
- Penalties for non-compliance
- Timeline for regulatory changes
```

## Handling Issues

### When Sources Conflict

1. Check source quality ratings (A vs E)
2. Look for third-party arbiters
3. Examine publication dates (older may be outdated)
4. Present both perspectives with explanation
5. If still uncertain, state: "Sources disagree on this point"

### When Information is Scarce

1. Broaden search queries
2. Look for adjacent topics with relevant insights
3. Check if the question needs reframing
4. Explicitly state information gaps
5. Suggest areas where more research is needed

### When Research is Too Vast

1. Focus on highest-quality sources (A-B ratings)
2. Prioritize recent sources
3. Limit scope to most critical subtopics
4. Use aggregate/summarize sources when possible
5. Consult user on prioritization

## Success Metrics

Your research is successful when:

- [ ] 100% of claims have verifiable citations
- [ ] Multiple sources support key findings
- [ ] Contradictions are acknowledged and explained
- [ ] Output follows the specified format
- [ ] Research stays within defined constraints
- [ ] User's specific questions are answered
- [ ] Confidence levels are clearly stated
- [ ] Limitations and gaps are explicitly documented

## Critical Reminders

1. **Quality Over Speed**: A well-researched report beats a fast, inaccurate one
2. **Citation Discipline**: NEVER make claims without sources
3. **Parallel Execution**: Always launch multiple research agents simultaneously
4. **User Alignment**: When in doubt, ask the user for clarification
5. **Iterative Refinement**: First pass doesn't need to be perfect, but must be accurate
6. **Transparency**: Always admit when you don't know or can't verify something

## Your Value

You are replacing the need for manual deep research or expensive research services. Your outputs should be:

- **Comprehensive**: Cover all aspects of the research question
- **Accurate**: Every claim verified with sources
- **Actionable**: Provide insights that inform decisions
- **Professional**: Quality comparable to professional research analysts

## Standard Skill Output Format

Every Research Executor execution must output:

### 1. Status

- `success`: All 7 phases completed successfully
- `partial`: Research incomplete but usable
- `failed`: Research execution failed

### 2. Artifacts Created

```markdown
- `RESEARCH/[topic]/README.md` - Overview and navigation
- `RESEARCH/[topic]/executive_summary.md` - 1-2 page summary
- `RESEARCH/[topic]/full_report.md` - Comprehensive findings
- `RESEARCH/[topic]/data/statistics.md` - Key numbers and facts
- `RESEARCH/[topic]/sources/bibliography.md` - Complete citations
- `RESEARCH/[topic]/sources/source_quality_table.md` - A-E ratings
- `RESEARCH/[topic]/research_notes/agent_findings_summary.md` - Raw agent outputs
```

### 3. Quality Score

```markdown
**Research Quality**: [0-10]/10
**Citation Coverage**: [percentage]%
**High-Quality Sources (A-B)**: [count]
**Total Sources**: [count]
**Justification**: [brief explanation]
```

### 4. Next Steps

```markdown
**Recommended Next Action**: [synthesizer | citation-validator | got-controller | none]
**Reason**: [why this is the next step]
**Handoff Data**: [what the next skill needs]
```

You are the Research Executor. Execute with precision, integrity, and thoroughness.
</file>

<file path=".claude/skills/research-executor/SKILL.md">
---
name: research-executor
description: æ‰§è¡Œå®Œæ•´çš„ 7 é˜¶æ®µæ·±åº¦ç ”ç©¶æµç¨‹ã€‚æ¥æ”¶ç»“æ„åŒ–ç ”ç©¶ä»»åŠ¡ï¼Œè‡ªåŠ¨éƒ¨ç½²å¤šä¸ªå¹¶è¡Œç ”ç©¶æ™ºèƒ½ä½“ï¼Œç”Ÿæˆå¸¦å®Œæ•´å¼•ç”¨çš„ç»¼åˆç ”ç©¶æŠ¥å‘Šã€‚å½“ç”¨æˆ·æœ‰ç»“æ„åŒ–çš„ç ”ç©¶æç¤ºè¯æ—¶ä½¿ç”¨æ­¤æŠ€èƒ½ã€‚
---

# Research Executor

## Overview

The Research Executor conducts comprehensive, multi-phase research using the 7-stage deep research methodology and Graph of Thoughts (GoT) framework.

## When to Use

- User provides a structured research prompt (from question-refiner)
- Need to execute systematic research with multiple agents
- Require comprehensive report with verified citations
- Research involves 3+ subtopics requiring parallel investigation

## Core Capabilities

1. **7-Phase Research Process**: Question scoping â†’ Planning â†’ Querying â†’ Triangulation â†’ Synthesis â†’ QA â†’ Output
2. **Multi-Agent Deployment**: 3-8 parallel research agents based on complexity
3. **Citation Management**: A-E source quality ratings, 100% citation coverage
4. **GoT Integration**: Optional Graph of Thoughts for complex topics

## Quick Start

```markdown
Execute research on: [structured research prompt]

The executor will:
1. Verify prompt completeness
2. Create research plan with subtopics
3. Deploy parallel agents (web, academic, verification)
4. Triangulate sources and validate claims
5. Synthesize findings with inline citations
6. Generate structured output in RESEARCH/[topic]/
```

## Output Structure

```
RESEARCH/[topic]/
â”œâ”€â”€ README.md
â”œâ”€â”€ executive_summary.md
â”œâ”€â”€ full_report.md
â”œâ”€â”€ data/
â”œâ”€â”€ sources/
â””â”€â”€ appendices/
```

## Key Features

- **Task Complexity Assessment**: Automatic agent count and model selection
- **Parallel Execution**: All agents launch simultaneously
- **Source Quality Control**: A-E rating system
- **Hallucination Prevention**: Chain-of-Verification process

## âš ï¸ Critical: Token Optimization

**ALWAYS use the Download â†’ Clean â†’ Read pipeline:**

1. WebFetch â†’ Save to `data/raw/[source].html`
2. Preprocess â†’ Clean to `data/processed/[source].md`
3. Read from processed file (60-90% token savings)

> âŒ **FORBIDDEN**: Direct WebFetch content in context for pages > 5KB

## Shared Resources

> ğŸ“‹ **Source Ratings**: `.claude/shared/constants/source_quality_ratings.md`
> ğŸ“‹ **Report Templates**: `.claude/shared/templates/report_structure.md`
> ğŸ“‹ **Citation Format**: `.claude/shared/templates/citation_format.md`

## Safety Limits

| Limit | Value |
|-------|-------|
| Max parallel agents | 8 |
| Max research time | 90 minutes |
| Min quality score | 8.0 to pass |
| Max token per source | 10,000 |

## Examples

See [examples.md](./examples.md) for detailed usage scenarios.

## Detailed Instructions

See [instructions.md](./instructions.md) for complete implementation guide.
</file>

<file path=".claude/skills/synthesizer/examples.md">
# Synthesizer Skill - Examples

## Example 1: Synthesizing AI Healthcare Market Research

### Input Findings from Multiple Agents

**Agent 1: Market Analysis**

```markdown
Global AI in healthcare market: $22.4B in 2023, growing at 37.5% CAGR to 2030.
Key segments: diagnostic imaging (32%), drug discovery (28%), clinical decision
support (22%), others (18%).
Source: Grand View Research, 2024 (Quality: B)
Score: 8.5/10
```

**Agent 2: Technology Assessment**

```markdown
AI diagnostic accuracy vs human clinicians:
- Radiology: AI 94% vs Human 88% (Smith et al., 2023, Nature Medicine)
- Pathology: AI 96% vs Human 90% (Jones et al., 2024, Lancet Digital Health)
- Cardiology: AI 91% vs Human 87% (Lee et al., 2023, JACC)

Limitations: Real-world deployment shows 15-20% lower accuracy due to data
quality issues (Davis, 2024, Healthcare IT News).
Sources: Multiple peer-reviewed studies (Quality: A)
Score: 9.2/10
```

**Agent 3: Vendor Landscape**

```markdown
Leading vendors by market share:
1. IBM Watson Health: 18% (focusing on oncology)
2. Google Health: 15% (focusing on imaging)
3. Microsoft Healthcare Bot: 12% (focusing on chatbots)
4. Amazon Comprehend Medical: 10% (focusing on NLP)
5. Others: 45%

Key differentiator: Specialized vs. general AI solutions.
Source: Gartner Magic Quadrant 2024 (Quality: B)
Score: 7.8/10
```

**Agent 4: Implementation Challenges**

```markdown
Primary barriers to adoption:
- Cost: $500K-$5M initial investment (KPMG, 2024)
- Integration: 6-18 months implementation time (Deloitte, 2024)
- Regulatory: FDA approval required (avg. 18 months) (FDA data, 2024)
- Staff training: 40-60 hours per clinician (Mayo Clinic case study, 2024)

Success factors: Executive sponsorship, data governance, phased rollout.
Sources: Industry reports, case studies (Quality: B-C mix)
Score: 8.0/10
```

### Synthesis Output

```markdown
# AI in Clinical Diagnosis: Market Analysis and Strategic Assessment

## Executive Summary

The global AI in healthcare market is experiencing explosive growth, reaching
$22.4 billion in 2023 and projected to grow at 37.5% CAGR through 2030
(Grand View Research, 2024). Diagnostic imaging represents the largest segment
(32% of market), driven by proven superior accuracyâ€”AI systems achieve 91-96%
accuracy compared to 87-90% for human clinicians across radiology, pathology,
and cardiology (Smith et al., 2023; Jones et al., 2024; Lee et al., 2023).

However, real-world deployment reveals a significant accuracy gap of 15-20%
compared to lab conditions due to data quality challenges (Davis, 2024). The
market is consolidating around specialized vendors, with IBM Watson Health (18%)
and Google Health (15%) leading through domain-specific solutions (Gartner, 2024).

**Strategic Implication**: Organizations should prioritize diagnostic imaging
and drug discovery applications (proven ROI), implement robust data governance
before AI deployment, and budget 18-36 months for implementation including
regulatory approval cycles.

## 1. Market Overview

### 1.1 Market Size and Growth

The AI in healthcare market has reached $22.4 billion in 2023, with projections
of 37.5% compound annual growth rate through 2030 (Grand View Research, 2024).
This growth rate significantly exceeds overall healthcare technology market growth
(typically 12-15% CAGR), indicating strong demand and accelerating adoption.

**Market Segmentation**:
- Diagnostic Imaging: 32% ($7.2B) - Largest segment, mature technology
- Drug Discovery: 28% ($6.3B) - High-growth, pharma investment
- Clinical Decision Support: 22% ($4.9B) - Rapid adoption in hospitals
- Other Applications: 18% ($4.0B) - Administrative, operational AI

### 1.2 Growth Drivers

Research across multiple sources identifies three primary growth drivers:

1. **Proven Clinical Value**: AI diagnostic systems consistently demonstrate
   superior accuracy in controlled studies (91-96% vs. 87-90% for clinicians)
   (Smith et al., 2023; Jones et al., 2024; Lee et al., 2023)

2. **Labor Shortages**: Healthcare provider shortage creates demand for
   productivity-enhancing tools (Deloitte, 2024)

3. **Falling Costs**: Cloud-based AI solutions reduce upfront investment from
   $2-5M to $500K-2M, broadening accessibility (KPMG, 2024)

## 2. Technology Assessment

### 2.1 Accuracy by Medical Specialty

Multiple peer-reviewed studies establish AI's superior diagnostic performance:

| Specialty | AI Accuracy | Human Accuracy | Improvement | Source |
|-----------|-------------|----------------|-------------|---------|
| Radiology  | 94%         | 88%            | +6%         | Smith et al., 2023 |
| Pathology  | 96%         | 90%            | +6%         | Jones et al., 2024 |
| Cardiology | 91%         | 87%            | +4%         | Lee et al., 2023 |

**Consensus Level**: STRONG - All findings from peer-reviewed sources (A-rated),
consistent across specialties.

### 2.2 The Lab-to-Reality Gap

A critical synthesis finding: Real-world AI accuracy is 15-20 percentage points
lower than lab-validated results (Davis, 2024). This discrepancy stems from:

- **Data Quality Issues**: Real patient data is noisier than curated datasets
- **Integration Challenges**: EHR integration introduces data loss
- **Workflow Factors**: Time pressure, interruptions affect AI utilization

**Implication**: ROI calculations based on lab accuracy will be overstated.
Organizations should use 70-80% of lab-published accuracy in planning models.

### 2.3 Technology Maturity

**Maturity Assessment by Application**:

- **High Maturity** (Production-ready): Medical imaging, radiology AI
- **Medium Maturity** (Pilot to early production): Pathology, cardiology
- **Low Maturity** (Experimental): Drug discovery, predictive analytics

## 3. Vendor Landscape

### 3.1 Market Share and Specialization

| Vendor        | Market Share | Focus Area      | Differentiation           |
|---------------|--------------|-----------------|---------------------------|
| IBM Watson    | 18%          | Oncology        | Deep cancer specialization |
| Google Health | 15%          | Medical Imaging | Computer vision expertise  |
| Microsoft     | 12%          | Chatbots/NLP    | Azure ecosystem integration|
| Amazon        | 10%          | NLP/Patient Data | AWS cloud integration     |
| Others        | 45%          | Varied          | Niche specialization       |

Source: Gartner Magic Quadrant 2024 (Quality: B)

**Synthesis Insight**: Market leaders succeed through **specialization, not
generalization**. IBM's dominance in oncology (specific use case) exceeds
generalist AI platforms.

### 3.2 Vendor Selection Framework

Based on implementation data from Agent 4, vendor selection criteria should
prioritize:

1. **Domain Expertise**: Specialized vs. general AI
2. **Integration Capability**: EHR compatibility, API availability
3. **Regulatory Status**: FDA/EMA approval status
4. **Total Cost of Ownership**: Include implementation, training, maintenance

## 4. Implementation Considerations

### 4.1 Cost Structure

**Initial Investment Range**: $500K - $5M (KPMG, 2024)

Cost breakdown:
- Software licensing: $100K-500K/year
- Implementation services: $200K-1.5M (one-time)
- Hardware/cloud infrastructure: $100K-1M
- Staff training: $50K-200K
- Ongoing maintenance: 20-30% of license fees annually

**Implementation Timeline**: 6-18 months (Deloitte, 2024)
- Planning and vendor selection: 2-4 months
- Technical integration: 3-8 months
- Staff training and rollout: 2-6 months
- Regulatory approval (if required): +12-18 months

### 4.2 Critical Success Factors

Synthesis of multiple sources identifies consistent success factors:

1. **Executive Sponsorship**: C-level support required for cross-functional
   coordination (Mayo Clinic case study, 2024)

2. **Data Governance Foundation**: AI fails without clean, structured data
   (all sources agree)

3. **Phased Rollout**: Start with single department, expand based on learnings
   (Deloitte, 2024; Mayo Clinic, 2024)

4. **Change Management**: 40-60 hours training per clinician required (Mayo Clinic, 2024)

## 5. Gaps and Limitations

### 5.1 Research Gaps Identified

1. **Long-term Outcomes**: No studies track AI diagnostic impact over 5+ years
2. **Cost-Benefit Analysis**: Limited ROI data beyond initial implementation
3. **Liability Frameworks**: Legal precedents for AI diagnostic errors unclear

### 5.2 Uncertainties

- **Regulatory Evolution**: FDA/EMA regulations evolving rapidly; current
  approvals may not reflect future requirements
- **Competitive Dynamics**: Market may consolidate further; vendor viability
  risk for smaller players

## 6. Recommendations

### 6.1 For Healthcare Executives

**Recommended Approach**:

1. **Phase 1 (Months 1-6)**: Assessment and Planning
   - Identify high-impact use cases (start with diagnostic imaging)
   - Assess data readiness
   - Select specialized vendors (not generalists)

2. **Phase 2 (Months 7-18)**: Pilot Implementation
   - Single department pilot
   - Use conservative accuracy assumptions (70-80% of lab figures)
   - Budget $500K-2M for initial deployment

3. **Phase 3 (Months 19-36)**: Expansion
   - Scale successful pilots
   - Add new use cases based on learnings
   - Total investment typically $2-5M over 3 years

### 6.2 Vendor Selection Priorities

Prioritize vendors with:
- Proven deployments in your specialty
- Regulatory approvals (FDA/EMA)
- Strong integration capabilities
- Clear ROI case studies from similar organizations

### 6.3 Risk Mitigation

- **Accuracy Risk**: Use conservative assumptions in planning
- **Vendor Risk**: Choose established players with strong funding
- **Regulatory Risk**: Engage legal early, allocate 12-18 months for approvals
- **Adoption Risk**: Invest heavily in training and change management

## 7. Conclusion

AI in clinical diagnosis represents a transformative technology with proven
superior accuracy in controlled studies. However, the lab-to-reality gap
(15-20% accuracy reduction in real-world settings) requires conservative
planning assumptions.

Organizations should approach AI adoption as a 3-year journey requiring
$2-5M investment, strong executive sponsorship, and robust data governance.
Specialized vendors with domain expertise outperform generalist platforms.

The market will grow at 37.5% CAGR through 2030, making early adopters
competitive in recruiting and patient outcomes. The question is not whether
to adopt AI clinical tools, but how to do so effectively.

## References

[Complete bibliography with all sources cited]

---

**Synthesis Quality Score**: 9.2/10

**Assessment**:
- âœ“ Coverage: All 4 agent findings incorporated
- âœ“ Coherence: Logical flow from market â†’ tech â†’ vendors â†’ implementation â†’ recommendations
- âœ“ Accuracy: All claims cited, no new assertions
- âœ“ Insight: Lab-to-reality gap identified as key synthesis insight
- âœ“ Clarity: Structured, accessible to executive audience

**Key Synthesis Contributions**:
1. Identified consistent accuracy advantage across specialties
2. Highlighted critical lab-to-reality accuracy gap (not in individual agent reports)
3. Mapped vendor success to specialization strategy
4. Created phased implementation framework based on cost/timeline data
5. Provided actionable recommendations with specific timelines and budgets
```

---

## Example 2: Resolving Contradictions in Cloud Provider Comparison

### Input Findings

**Agent A: AWS Perspective**

```markdown
AWS has the largest market share at 32% and most comprehensive service offerings
(2,000+ services). Best for enterprises requiring breadth of capabilities.
Pricing: Moderate to high, but with significant volume discounts available.
Synergy Research, 2024.
```

**Agent B: Azure Perspective**

```markdown
Azure has 23% market share and strongest enterprise integration (Office 365,
Active Directory). Best for Microsoft-centric organizations.
Pricing: Competitive, with hybrid cloud discounts.
Gartner, 2024.
```

**Agent C: Google Cloud Perspective**

```markdown
Google Cloud has 11% market share but leading AI/ML capabilities (Vertex AI,
TPUs). Best for AI/ML workloads and data analytics.
Pricing: Most aggressive pricing for sustained workloads.
Forrester, 2024.
```

**Agent D: Developer Experience Comparison**

```markdown
Developer satisfaction surveys:
- AWS: 7.2/10 (comprehensive but complex)
- Azure: 7.8/10 (good documentation, integrated tools)
- Google Cloud: 8.5/10 (cleanest UI, best developer experience)
Stack Overflow Survey 2024.
```

### Synthesis Output

```markdown
# Cloud Provider Comparison: AWS vs Azure vs Google Cloud

## Contradictions and Resolutions

### Contradiction 1: Market Share vs. Developer Satisfaction

**Observation**: AWS has largest market share (32%) but lowest developer
satisfaction (7.2/10). Google Cloud has smallest share (11%) but highest
satisfaction (8.5/10).

**Resolution**: This is NOT a contradictionâ€”rather, it reflects different
market strategies:

- **AWS (32% share, 7.2 satisfaction)**: Enterprise-focused, comprehensive
  platform. High share due to first-mover advantage and breadth of services.
  Lower satisfaction reflects complexity and learning curve.

- **Azure (23% share, 7.8 satisfaction)**: Enterprise Microsoft ecosystem.
  Share driven by Office 365/Active Directory integration.

- **GCP (11% share, 8.5 satisfaction)**: Developer-focused, specialized in
  AI/ML. Lower share due to narrower focus, but high satisfaction from cleaner
  UX and developer-centric design.

**Synthesis Insight**: Market share and satisfaction measure different things.
Share reflects enterprise adoption; satisfaction reflects developer experience.
Choose based on priorities: enterprise requirements (AWS/Azure) vs. developer
productivity (GCP).

### Contradiction 2: Pricing Claims

**Observation**:
- Agent A: "AWS pricing moderate to high"
- Agent B: "Azure pricing competitive"
- Agent C: "GCP most aggressive pricing"

**Resolution**: All statements are accurate for different use cases:

**AWS Pricing Truth**:
- Moderate to high for on-demand, small workloads
- Highly competitive for large, sustained workloads (volume discounts)
- Complex pricing structure (multiple instance types, pricing options)

**Azure Pricing Truth**:
- Competitive for Microsoft stack (Windows, SQL Server)
- Hybrid discounts for on-prem + cloud combinations
- Enterprise Agreements (EA) provide significant discounts

**GCP Pricing Truth**:
- Most aggressive for sustained-use workloads (sustained-use discounts)
- Simplified pricing (fewer instance types)
- Per-second billing (vs. AWS/Azure hourly)

**Synthesis Insight**: "Cheapest" depends entirely on workload pattern:

| Workload Pattern | Most Cost-Effective |
|------------------|---------------------|
| Small, sporadic  | AWS/Azure (tiered pricing) |
| Large, sustained | GCP (sustained-use discounts) |
| Microsoft stack  | Azure (licensing integration) |
| Enterprise EA    | AWS/Azure (EA discounts) |

### Contradiction 3: AI/ML Capabilities

**Observation**:
- Agent A claims "AWS comprehensive AI/ML" (SageMaker, 200+ AI features)
- Agent C claims "GCP leading AI/ML" (Vertex AI, TPUs)

**Resolution**: Different dimensions of "AI/ML leadership":

**AWS Strength**: Breadth of AI/ML services
- SageMaker: Most comprehensive ML platform
- 200+ AI-specific features
- Best for: General ML workloads, enterprise ML platforms

**GCP Strength**: Depth in AI/ML infrastructure
- Vertex AI: Unified ML platform (simpler than SageMaker)
- TPUs: Custom hardware for ML training (faster than CPUs/GPUs for some workloads)
- TensorFlow: Google-created, first-class integration
- Best for: Deep learning, large-scale training, TensorFlow users

**Synthesis Insight**: Neither is universally "best" for AI/ML:

- Choose **AWS SageMaker** for: Enterprise ML platforms, breadth of tools,
  multi-framework requirements

- Choose **GCP Vertex AI** for: Deep learning, TensorFlow workloads,
  TPUs for large training jobs

- **Azure** is third for AI/ML: Adequate for basic needs, but lags in
  advanced ML capabilities

## Integrated Recommendations

### By Use Case

| Use Case | Recommended Provider | Rationale |
|----------|---------------------|-----------|
| General enterprise workloads | AWS | Breadth of services, maturity |
| Microsoft-centric organizations | Azure | O365/AD integration |
| AI/ML and analytics | GCP (with AWS fallback) | TPUs, Vertex AI, TensorFlow |
| Cost-sensitive sustained workloads | GCP | Sustained-use discounts |
| Enterprise EA requirements | AWS or Azure | Enterprise agreement capabilities |
| Developer productivity priority | GCP | Best developer experience (8.5/10) |

### Decision Framework

**Step 1**: Identify primary constraint
- Integration requirements â†’ Azure (if Microsoft) or AWS (if neutral)
- AI/ML focus â†’ GCP (with AWS backup)
- Cost â†’ GCP (sustained), AWS (enterprise discounts)

**Step 2**: Assess secondary needs
- If multiple providers viable, choose based on team familiarity
- Consider multi-cloud strategy (e.g., GCP for AI/ML, AWS for general workloads)

**Step 3**: Validate with proof-of-concept
- All providers offer free tiers for testing
- Prototype critical workloads on 2 providers before committing

## Conclusion

No single cloud provider is universally "best." The contradictions in vendor
comparisons reflect legitimate differences in focus areas:

- **AWS**: Broadest capabilities, enterprise maturity
- **Azure**: Microsoft ecosystem integration
- **GCP**: Developer experience, AI/ML specialization

The right choice depends on your specific priorities: integration requirements,
workload patterns, team expertise, and strategic goals.
```

---

## Example 3: Progressive Synthesis (Refining Iteratively)

### Iteration 1: Initial Synthesis

```markdown
Research finds that remote work increases productivity by 15-20% on average,
but decreases employee satisfaction by 10-15% due to isolation.
```

**Critique**: Too simplistic, doesn't explore nuances or contradictions.

### Iteration 2: Refined Synthesis

```markdown
Remote work productivity gains of 15-20% (Stanford Study, 2023) are
concentrated among knowledge workers with dedicated home offices.
However, these gains come at a cost: 10-15% decrease in employee satisfaction
(Gallup, 2024) primarily due to social isolation and blurred work-life boundaries.
```

**Critique**: Better, but still doesn't explain WHY or address contradictions.

### Iteration 3: Final Synthesis with Nuance

```markdown
Remote work creates an **experience paradox**: productivity increases while
satisfaction decreases for many workers.

**Productivity Gains** (+15-20%):
- Eliminated commuting (avg. 1.5 hours/day reclaimed)
- Fewer workplace interruptions
- Flexible scheduling around peak productivity hours
*Concentrated among: Knowledge workers, home owners, parents*

**Satisfaction Declines** (-10-15%):
- Social isolation and loneliness
- Blurred work-life boundaries (work bleeds into personal time)
- Reduced career visibility (out of sight, out of mind for promotions)
*Concentrated among: Early-career professionals, extroverts, urban dwellers in small apartments*

**Resolution**: The "hybrid" model (2-3 days remote) emerges as optimal,
capturing 80% of productivity gains with minimal satisfaction impact
(Microsoft, 2024).

**Synthesis Insight**: Remote work is not uniformly good or badâ€”it creates
winners (parents, senior staff, home owners) and losers (early-career,
extroverts, small-space dwellers). Organizations must design policies
addressing both groups' needs.
```

---

## Synthesis Quality Checklist

Use this checklist to evaluate synthesis quality:

```
Input Coverage
[ ] All major findings from all sources are included
[ ] No important findings are omitted
[ ] Minority viewpoints are acknowledged

Contradiction Handling
[ ] Contradictions are identified
[ ] Contradictions are explained or resolved
[ ] Multiple perspectives are presented fairly

Citation Accuracy
[ ] All claims have citations
[ ] Citations are preserved accurately
[ ] No new claims are introduced without sources

Coherence and Flow
[ ] Logical organization
[ ] Clear transitions between topics
[ ] Builds understanding progressively

Insight Value
[ ] Goes beyond summary to synthesis
[ ] Identifies patterns not in individual reports
[ ] Provides actionable recommendations

Clarity
[ ] Accessible to target audience
[ ] Well-structured with headings/formatting
[ ] Clear language, minimal jargon
```

## Key Synthesis Principles

1. **Integration, Not Aggregation**: Don't just concatenate findingsâ€”integrate
   them into coherent understanding

2. **Context Over Content**: Explain WHY findings matter, not just WHAT they are

3. **Patterns Over Individual Findings**: Identify cross-source patterns

4. **Nuance Over Simplification**: Acknowledge complexity and contradictions

5. **Insight Over Information**: Move from "what was found" to "what it means"
</file>

<file path=".claude/skills/synthesizer/instructions.md">
# Synthesizer Skill - Instructions

## Role

You are a **Research Synthesizer** responsible for combining findings from multiple research agents into a coherent, well-structured, and insightful research report. Your role is to transform raw research data into actionable knowledge.

## Core Responsibilities

1. **Integrate Findings**: Combine multiple research sources into unified content
2. **Resolve Contradictions**: Identify and explain conflicting information
3. **Extract Consensus**: Identify themes and conclusions supported by multiple sources
4. **Create Narrative**: Build a logical flow from introduction to conclusions
5. **Maintain Citations**: Preserve source attribution throughout synthesis
6. **Identify Gaps**: Note what is still unknown or needs further research

## Synthesis Process

### Phase 1: Review and Organize

**Input Analysis**:

- Review all research findings from agents
- Identify common themes and topics
- Note contradictions and discrepancies
- Assess source quality and credibility
- Group related findings together

**Organization Strategy**:

```markdown
Create thematic clusters:
1. Theme A: [related findings]
   - Finding 1.1 (Source: Agent X, Score: 8.5)
   - Finding 1.2 (Source: Agent Y, Score: 7.8)
   - Finding 1.3 (Source: Agent Z, Score: 8.2)

2. Theme B: [related findings]
   - Finding 2.1 (Source: Agent X, Score: 7.5)
   - Finding 2.2 (Source: Agent W, Score: 8.9)

3. Theme C: [contradictory findings]
   - Finding 3.1 (Source: Agent Y, Score: 8.0)
   - Finding 3.2 (Source: Agent Z, Score: 7.2) [CONTRADICTS 3.1]
```

### Phase 2: Consensus Building

**For each theme, identify**:

1. **Strong Consensus**: Findings supported by 3+ high-quality sources
2. **Moderate Consensus**: Findings supported by 2 sources or 1 high-quality + 1 medium-quality
3. **Weak Consensus**: Findings from only 1 source
4. **No Consensus**: Contradictory findings with no resolution

```markdown
**Example Consensus Assessment**:

Theme: AI in Healthcare Market Size

Finding 1: "$22.4B in 2023" (Grand View Research, 2024) [Quality: B]
Finding 2: "$21.8B in 2023" (MarketsandMarkets, 2024) [Quality: B]
Finding 3: "$23.1B in 2023" (Fortune Business Insights, 2024) [Quality: B]

**Consensus Level**: STRONG
**Synthesis**: "Multiple industry reports estimate the 2023 AI in healthcare market
at approximately $22-23 billion, with Grand View Research reporting $22.4B,
MarketsandMarkets reporting $21.8B, and Fortune Business Insights reporting $23.1B
(Grand View Research, 2024; MarketsandMarkets, 2024; Fortune Business Insights, 2024)."
```

### Phase 3: Contradiction Resolution

**Contradiction Resolution Strategy Table**:

| Contradiction Type | Resolution Approach | Example Synthesis |
|-------------------|---------------------|-------------------|
| Numerical | Report range, explain sources/methods | "Growth projections vary from 37.5% to 42.1% CAGR depending on market definition" |
| Temporal | "As of X date..." vs "Recent data shows..." | "Adoption grew from 25% in 2022 to 45% in 2024" |
| Scope | Different for [segment A] vs [segment B] | "In healthcare: 90% accuracy; in retail: 65% accuracy" |
| Source Quality | Prefer A-rated sources, note D-rated | "While Source B suggests..., Source A (peer-reviewed) found..." |
| Geographic | "[Region A]" vs "Globally" | "US adoption: 45%; Global adoption: 28%" |
| Methodological | Different measurement methods | "Lab conditions: 90%; Real-world: 65%" |

**Types of Contradictions**:

#### Type A: Numerical Discrepancies

```
Finding A: "Market will grow 37.5% CAGR" (Source X)
Finding B: "Market will grow 42.1% CAGR" (Source Y)

Resolution Strategy:
1. Check publication dates (older vs newer)
2. Check methodology (different definitions?)
3. Check scope (different geographic markets?)
4. Present range or explain discrepancy

Synthesis: "Growth projections vary from 37.5% to 42.1% CAGR depending on
market definition and geographic scope (Source X, 2024; Source Y, 2024)."
```

#### Type B: Causal Claims

```
Finding A: "X causes Y" (Source X, observational study)
Finding B: "X does not cause Y" (Source Y, RCT)

Resolution Strategy:
- Prioritize RCT over observational (higher quality)
- Present as "evidence suggests" not "proven"
- Note level of certainty

Synthesis: "While Source X suggests X may influence Y (observational data),
Source Y found no causal relationship in randomized controlled trials (Source Y, 2024).
Current evidence does not support a definitive causal claim."
```

#### Type C: Temporal Changes

```
Finding A: "Technology adoption is 25%" (Source X, 2022)
Finding B: "Technology adoption is 45%" (Source Y, 2024)

Resolution Strategy:
- Present as trend/growth
- Use newer data for current state
- Note temporal change

Synthesis: "Adoption has grown from 25% in 2022 (Source X) to 45% in 2024 (Source Y),
indicating accelerating adoption."
```

#### Type D: Scope Differences

```
Finding A: "90% accuracy" (Source X, lab conditions)
Finding B: "65% accuracy" (Source Y, real-world deployment)

Resolution Strategy:
- Contextualize both findings
- Explain conditions matter
- Present both with appropriate caveats

Synthesis: "While lab tests demonstrate up to 90% accuracy (Source X, 2024),
real-world deployments typically achieve 60-70% accuracy due to challenging
conditions (Source Y, 2024)."
```

### Phase 4: Structured Synthesis

**Select the appropriate template based on research type**:

## Synthesis Templates

### Template 1: Consensus Report

**Use for**: Multiple sources on the same topic

```markdown
# [Research Topic]: Consensus Report

## Executive Summary
[1-2 page synthesis of key findings]

## 1. Strong Consensus
[Findings supported by 3+ high-quality sources]
### Consensus Point 1
- Evidence from multiple sources
- Confidence level: HIGH
- Key citations

## 2. Moderate Consensus
[Findings supported by 2 sources or 1 high-quality + 1 medium-quality]
### Consensus Point 1
- Evidence from limited sources
- Confidence level: MEDIUM
- Key citations

## 3. Weak Consensus
[Findings from only 1 source]
### Point 1
- Single-source finding
- Confidence level: LOW
- Requires verification

## 4. Areas of Disagreement
[Where sources conflict]
### Disagreement 1
- Source A says X
- Source B says Y
- Possible explanations

## 5. Research Gaps
[What could NOT be determined]

## 6. Conclusions
[Based on consensus strength]
```

### Template 2: Comparative Analysis

**Use for**: X vs Y comparisons

```markdown
# [Topic A] vs [Topic B]: Comparative Analysis

## Executive Summary
[Key comparison insights]

## 1. Criteria Overview
[Define comparison dimensions]

## 2. Side-by-Side Comparison Matrix
| Criterion | Topic A | Topic B | Winner |
|-----------|---------|---------|--------|
| Criterion 1 | [details] | [details] | [A/B/Tie] |
| Criterion 2 | [details] | [details] | [A/B/Tie] |
| ... | ... | ... | ... |

## 3. Detailed Analysis by Criterion
### Criterion 1
- Topic A: [analysis with citations]
- Topic B: [analysis with citations]
- Comparison: [which is better and why]

## 4. Recommendations by Use Case
### Use Case 1: Best choice is [Topic A]
- Reasoning with citations

### Use Case 2: Best choice is [Topic B]
- Reasoning with citations

## 5. Decision Framework
[Flowchart or decision tree]

## 6. Conclusion
[Overall recommendation]
```

### Template 3: Problem-Solution

**Use for**: "How to solve X" research

```markdown
# [Problem]: Analysis and Solutions

## Executive Summary
[Problem definition and recommended solution]

## 1. Problem Statement
- What is the problem?
- Why does it matter?
- Impact assessment

## 2. Root Causes
### Root Cause 1
- Evidence with citations
- Contribution to problem

## 3. Current Approaches
### Approach 1
- Description with citations
- Effectiveness data
- Limitations

## 4. Emerging Solutions
### Solution 1
- Description with citations
- Evidence of effectiveness
- Implementation requirements

## 5. Comparative Analysis of Solutions
| Solution | Pros | Cons | Evidence | Maturity |
|----------|------|------|----------|----------|
| Solution 1 | [pros] | [cons] | [citations] | [level] |
| Solution 2 | [pros] | [cons] | [citations] | [level] |

## 6. Implementation Considerations
- Cost with citations
- Timeline with citations
- Risk factors with citations

## 7. Recommended Approach
### Primary Recommendation
- What to do
- Why (with evidence)
- How to implement

### Alternative (if primary not feasible)
- Backup option
- When to use it

## 8. Risk Mitigation
[How to address potential issues]
```

## Default Report Structure

If no template applies, use this general structure:

```markdown
# [Research Topic]: Comprehensive Report

## Executive Summary
[1-2 page synthesis of key findings]

## 1. Introduction
[Context, scope, methodology]

## 2. [Theme 1]
### 2.1 Consensus Findings
[Findings supported by multiple sources]

### 2.2 Key Insights
[Synthesized insights from findings]

### 2.3 Evidence Base
[Summary of sources and quality]

## 3. [Theme 2]
[Same structure as Theme 1]

## 4. [Theme with Contradictions]
### 4.1 Differing Perspectives
[Present conflicting findings fairly]

### 4.2 Resolution
[Explain contradictions, present balanced view]

## 5. Integrated Analysis
### 5.1 Cross-Theme Insights
[Connections between themes]

### 5.2 Patterns and Trends
[Identified patterns across findings]

### 5.3 Cause-Effect Relationships
[Supported causal claims]

## 6. Gaps and Limitations
[What is unknown, needs further research]

## 7. Conclusions and Recommendations
[Actionable insights]

## References
[Complete bibliography]
```

### Phase 5: Quality Enhancement

**Synthesis Quality Checklist**:

- [ ] All major findings are included
- [ ] Contradictions are acknowledged and addressed
- [ ] Consensus is clearly distinguished from minority views
- [ ] Citations are preserved and accurate
- [ ] Narrative flow is logical and coherent
- [ ] Insights are actionable, not just summary
- [ ] Uncertainties and limitations are explicit
- [ ] No new claims are introduced without sources

## Synthesis Techniques

### Technique 1: Thematic Grouping

**Best for**: Diverse findings on related topics

```markdown
Instead of:
"Agent 1 found X. Agent 2 found Y. Agent 3 found Z."

Use:
"Three key patterns emerge from the research: First, X... Second, Y... Third, Z..."
```

### Technique 2: Source Triangulation

**Best for**: Validating claims across sources

```markdown
"When multiple high-quality sources converge on the same finding, confidence
in the result increases. For example, [Claim] is supported by Source A (2024),
Source B (2024), and Source C (2023), all using different methodologies but
arriving at similar conclusions."
```

### Technique 3: Progressive Disclosure

**Best for**: Building understanding gradually

```markdown
"Before examining [complex topic], it is important to understand [foundational concept]...
With this foundation in place, we can now explore [complex topic]..."
```

### Technique 4: Comparative Synthesis

**Best for**: Options, alternatives, or comparisons

```markdown
| Dimension | Option A | Option B | Option C |
|-----------|----------|----------|----------|
| Cost      | $$$      | $$       | $        |
| Maturity  | High     | Medium   | Low      |
| Adoption  | 45%      | 30%      | 15%      |

**Recommendation**: Choose [Option] because..."
```

### Technique 5: Narrative Arc

**Best for**: Historical or evolutionary topics

```markdown
"The evolution of [topic] can be traced through three distinct phases:

**Phase 1 (2017-2019)**: Early experimentation...
**Phase 2 (2020-2022)**: Rapid adoption and scaling...
**Phase 3 (2023-present)**: Maturity and optimization...

Understanding this trajectory helps explain current state and suggests future directions..."
```

## Handling Specific Synthesis Challenges

### Challenge 1: Overwhelming Amount of Data

**Solution**: Create hierarchy

1. Executive Summary (high-level only)
2. Main Report (key details)
3. Appendices (comprehensive data)

### Challenge 2: Conflicting High-Quality Sources

**Solution**:

1. Acknowledge both perspectives
2. Explain why they might differ (methodology, scope, timing)
3. If no resolution, present both with appropriate context
4. Avoid choosing sides arbitrarily

### Challenge 3: Weak Sources on Important Topics

**Solution**:

1. Clearly flag as "needs verification"
2. Present as "preliminary" or "suggestive"
3. Recommend additional research
4. Don't overstate certainty

### Challenge 4: Gaps in Research

**Solution**:

1. Explicitly state what is unknown
2. Explain why it might be hard to research
3. Suggest approaches for filling gaps
4. Don't speculate beyond evidence

## Synthesis Output Formats

### Format 1: Comprehensive Report

```markdown
[Full detailed report with all findings, citations, and analysis]
```

### Format 2: Executive Summary

```markdown
[Condensed 1-2 page summary focusing on key insights and recommendations]
```

### Format 3: Thematic Analysis

```markdown
[Organized by themes with findings grouped under each theme]
```

### Format 4: Comparative Matrix

```markdown
[Side-by-side comparison of options, sources, or approaches]
```

### Format 5: Decision Framework

```markdown
[Structured decision-making guide with criteria and recommendations]
```

## Integration with GoT Operations

The Synthesizer is often called after GoT **Aggregate** operations:

```markdown
**GoT Aggregate(7)**: Combines 7 nodes into 1 synthesis
  â†“
**Synthesizer**: Takes those 7 findings and creates coherent report
  â†“
**Output**: Structured, cited, actionable research report
```

The Synthesizer can also be used for:

- **GoT Refine(1)**: Improve existing synthesis
- **Final output generation**: After all GoT operations complete

## Quality Metrics

**Synthesis Quality Score** (0-10):

- **Coverage** (0-2): All important findings included?
- **Coherence** (0-2): Logical flow and structure?
- **Accuracy** (0-2): Citations preserved, no new claims?
- **Insight** (0-2): Actionable insights, not just summary?
- **Clarity** (0-2): Clear, well-organized, accessible?

**Score Interpretation**:

- 9-10: Excellent - Professional publication quality
- 7-8: Good - Solid, actionable research
- 5-6: Fair - Adequate but needs improvement
- 3-4: Poor - Significant issues
- 0-2: Very Poor - Not usable

## Tool Usage

### Read/Write

```markdown
# Save synthesis outputs
Write synthesized report to:
- `full_report.md` (comprehensive)
- `executive_summary.md` (condensed)
- `synthesis_notes.md` (process documentation)
```

### Task (for additional research)

```markdown
# If synthesis reveals gaps
Launch new research agents:
"Research has identified gap in [topic]. Investigate this specific aspect."
```

## Best Practices

1. **Stay True to Sources**: Don't introduce claims not supported by research
2. **Acknowledge Uncertainty**: Clearly state what is unknown
3. **Fair Presentation**: Present all credible perspectives
4. **Logical Organization**: Group related findings, build understanding progressively
5. **Actionable Insights**: Move beyond summary to implications and recommendations
6. **Source Diversity**: Synthesize from multiple source types when possible
7. **Citation Discipline**: Maintain attribution throughout

## Common Synthesis Patterns

### Pattern 1: Problem-Solution

```
1. Define the problem
2. Current approaches (synthesized from research)
3. Limitations of current approaches
4. Emerging solutions
5. Recommendations
```

### Pattern 2: Past-Present-Future

```
1. Historical context
2. Current state (synthesized from multiple sources)
3. Emerging trends
4. Future projections
5. Strategic implications
```

### Pattern 3: Comparative Evaluation

```
1. Options/approaches overview
2. Comparison by criteria
3. Pros/cons (synthesized from research)
4. Use case mapping
5. Recommendation framework
```

### Pattern 4: Causal Analysis

```
1. Phenomenon description
2. Identified causes (synthesized, with certainty levels)
3. Mechanisms (how causes lead to effects)
4. Evidence strength assessment
5. Intervention points
```

## Success Criteria

Synthesis is successful when:

- [ ] All relevant findings are incorporated
- [ ] Contradictions are resolved or explained
- [ ] Consensus is clearly identified
- [ ] Citations are preserved and accurate
- [ ] Narrative is coherent and logical
- [ ] Insights are actionable
- [ ] Gaps are acknowledged
- [ ] Quality score â‰¥ 8/10

## Remember

You are the **Synthesizer** - you transform raw research data into knowledge. Your value is not in summarizing, but in **integrating, contextualizing, and illuminating**.

**Good synthesis** = "Here's what the research says, what it means, and what you should do about it."

**Bad synthesis** = "Here's a list of things the research found."

**Be the former, not the latter.**

## Standard Skill Output Format

Every Synthesizer execution must output:

### 1. Status

- `success`: Synthesis completed successfully
- `partial`: Synthesis incomplete but usable
- `failed`: Synthesis failed

### 2. Artifacts Created

```markdown
- `RESEARCH/[topic]/full_report.md` - Comprehensive synthesis
- `RESEARCH/[topic]/executive_summary.md` - Condensed summary
- `RESEARCH/[topic]/synthesis_notes.md` - Process documentation
```

### 3. Quality Score

```markdown
**Synthesis Quality**: [0-10]/10
**Coverage**: [0-2]/2
**Coherence**: [0-2]/2
**Accuracy**: [0-2]/2
**Insight**: [0-2]/2
**Clarity**: [0-2]/2
**Justification**: [brief explanation]
```

### 4. Next Steps

```markdown
**Recommended Next Action**: [citation-validator | got-controller | none]
**Reason**: [why this is the next step]
**Handoff Data**: [what the next skill needs]
```
</file>

<file path=".claude/skills/synthesizer/SKILL.md">
---
name: synthesizer
description: å°†å¤šä¸ªç ”ç©¶æ™ºèƒ½ä½“çš„å‘ç°ç»¼åˆæˆè¿è´¯ã€ç»“æ„åŒ–çš„ç ”ç©¶æŠ¥å‘Šã€‚è§£å†³çŸ›ç›¾ã€æå–å…±è¯†ã€åˆ›å»ºç»Ÿä¸€å™è¿°ã€‚å½“å¤šä¸ªç ”ç©¶æ™ºèƒ½ä½“å®Œæˆç ”ç©¶ã€éœ€è¦å°†å‘ç°ç»„åˆæˆç»Ÿä¸€æŠ¥å‘Šã€å‘ç°ä¹‹é—´å­˜åœ¨çŸ›ç›¾æ—¶ä½¿ç”¨æ­¤æŠ€èƒ½ã€‚
---

# Synthesizer

## Overview

Transform raw research findings from multiple agents into coherent, insightful, and actionable reports.

## When to Use

- Multiple research agents have completed their work
- Need to combine findings into unified narrative
- Contradictions exist between sources
- Ready to create final comprehensive report

## Core Responsibilities

1. **Integrate Findings**: Combine multiple sources into unified content
2. **Resolve Contradictions**: Identify and explain conflicting information
3. **Extract Consensus**: Find themes supported by multiple sources
4. **Create Narrative**: Build logical flow from introduction to conclusions
5. **Maintain Citations**: Preserve source attribution throughout

## Synthesis Process

1. **Review & Organize**: Group findings by theme, assess quality
2. **Consensus Building**: Strong/Moderate/Weak/No consensus
3. **Contradiction Resolution**: Numerical, causal, temporal, scope differences
4. **Structured Synthesis**: Create report with clear sections
5. **Quality Enhancement**: Verify completeness and coherence

> ğŸ“‹ **Templates**: See `.claude/shared/templates/report_structure.md` for standard templates.
> ğŸ“‹ **Citations**: See `.claude/shared/templates/citation_format.md` for citation standards.

## Key Techniques

- **Thematic Grouping**: Organize by themes, not by agent
- **Source Triangulation**: Multiple sources = higher confidence
- **Progressive Disclosure**: Build understanding gradually
- **Comparative Synthesis**: Side-by-side comparisons
- **Narrative Arc**: Trace evolution through phases

## Quality Metrics

**Synthesis Score (0-10)**:

- Coverage (0-2): All findings included?
- Coherence (0-2): Logical flow?
- Accuracy (0-2): Citations preserved?
- Insight (0-2): Actionable, not just summary?
- Clarity (0-2): Well-organized?

Target: â‰¥ 8/10

## Examples

See [examples.md](./examples.md) for synthesis patterns.

## Detailed Instructions

See [instructions.md](./instructions.md) for complete synthesis methodology.
</file>

<file path=".claude/settings.local.json">
{
  "permissions": {
    "allow": [
      "Bash(mkdir:*)",
      "Bash(mv:*)",
      "Bash(ls:*)",
      "Bash(cat:*)",
      "Bash(head:*)",
      "Bash(rm:*)",
      "Bash(cp:*)",
      "Bash(python3:*)"
    ]
  },
  "research": {
    "default_output_dir": "RESEARCH",
    "max_agents": 8,
    "default_quality_threshold": 8.0,
    "citation_style": "inline-with-url"
  }
}
</file>

<file path="docs/reference/skills-guide.md">
# Agent Skills

> åœ¨ Claude Code ä¸­åˆ›å»ºã€ç®¡ç†å’Œå…±äº« Skills ä»¥æ‰©å±• Claude çš„åŠŸèƒ½ã€‚

æœ¬æŒ‡å—å±•ç¤ºäº†å¦‚ä½•åœ¨ Claude Code ä¸­åˆ›å»ºã€ä½¿ç”¨å’Œç®¡ç† Agent Skillsã€‚Skills æ˜¯æ¨¡å—åŒ–åŠŸèƒ½ï¼Œé€šè¿‡åŒ…å«è¯´æ˜ã€è„šæœ¬å’Œèµ„æºçš„æœ‰ç»„ç»‡çš„æ–‡ä»¶å¤¹æ¥æ‰©å±• Claude çš„åŠŸèƒ½ã€‚

## å‰ç½®æ¡ä»¶

* Claude Code ç‰ˆæœ¬ 1.0 æˆ–æ›´é«˜ç‰ˆæœ¬
* å¯¹ [Claude Code](/zh-CN/quickstart) çš„åŸºæœ¬ç†Ÿæ‚‰

## ä»€ä¹ˆæ˜¯ Agent Skillsï¼Ÿ

Agent Skills å°†ä¸“ä¸šçŸ¥è¯†æ‰“åŒ…æˆå¯å‘ç°çš„åŠŸèƒ½ã€‚æ¯ä¸ª Skill åŒ…å«ä¸€ä¸ª `SKILL.md` æ–‡ä»¶ï¼Œå…¶ä¸­åŒ…å« Claude åœ¨ç›¸å…³æ—¶è¯»å–çš„è¯´æ˜ï¼Œä»¥åŠå¯é€‰çš„æ”¯æŒæ–‡ä»¶ï¼Œå¦‚è„šæœ¬å’Œæ¨¡æ¿ã€‚

**Skills å¦‚ä½•è¢«è°ƒç”¨**ï¼šSkills æ˜¯**æ¨¡å‹è°ƒç”¨çš„**â€”â€”Claude æ ¹æ®æ‚¨çš„è¯·æ±‚å’Œ Skill çš„æè¿°è‡ªä¸»å†³å®šä½•æ—¶ä½¿ç”¨å®ƒä»¬ã€‚è¿™ä¸æ–œæ å‘½ä»¤ä¸åŒï¼Œæ–œæ å‘½ä»¤æ˜¯**ç”¨æˆ·è°ƒç”¨çš„**ï¼ˆæ‚¨æ˜¾å¼è¾“å…¥ `/command` æ¥è§¦å‘å®ƒä»¬ï¼‰ã€‚

**ä¼˜åŠ¿**ï¼š

* ä¸ºæ‚¨çš„ç‰¹å®šå·¥ä½œæµæ‰©å±• Claude çš„åŠŸèƒ½
* é€šè¿‡ git åœ¨æ‚¨çš„å›¢é˜Ÿä¸­å…±äº«ä¸“ä¸šçŸ¥è¯†
* å‡å°‘é‡å¤æç¤º
* ä¸ºå¤æ‚ä»»åŠ¡ç»„åˆå¤šä¸ª Skills

åœ¨ [Agent Skills æ¦‚è¿°](https://docs.claude.com/en/docs/agents-and-tools/agent-skills/overview) ä¸­äº†è§£æ›´å¤šä¿¡æ¯ã€‚

<Note>
  æœ‰å…³ Agent Skills çš„æ¶æ„å’Œå®é™…åº”ç”¨çš„æ·±å…¥æ¢è®¨ï¼Œè¯·é˜…è¯»æˆ‘ä»¬çš„å·¥ç¨‹åšå®¢ï¼š[ä½¿ç”¨ Agent Skills ä¸ºçœŸå®ä¸–ç•Œè£…å¤‡ä»£ç†](https://www.anthropic.com/engineering/equipping-agents-for-the-real-world-with-agent-skills)ã€‚
</Note>

## åˆ›å»º Skill

Skills å­˜å‚¨ä¸ºåŒ…å« `SKILL.md` æ–‡ä»¶çš„ç›®å½•ã€‚

### ä¸ªäºº Skills

ä¸ªäºº Skills åœ¨æ‚¨çš„æ‰€æœ‰é¡¹ç›®ä¸­éƒ½å¯ç”¨ã€‚å°†å®ƒä»¬å­˜å‚¨åœ¨ `~/.claude/skills/` ä¸­ï¼š

```bash  theme={null}
mkdir -p ~/.claude/skills/my-skill-name
```

**ä½¿ç”¨ä¸ªäºº Skills çš„åœºæ™¯**ï¼š

* æ‚¨çš„ä¸ªäººå·¥ä½œæµå’Œåå¥½
* æ‚¨æ­£åœ¨å¼€å‘çš„å®éªŒæ€§ Skills
* ä¸ªäººç”Ÿäº§åŠ›å·¥å…·

### é¡¹ç›® Skills

é¡¹ç›® Skills ä¸æ‚¨çš„å›¢é˜Ÿå…±äº«ã€‚å°†å®ƒä»¬å­˜å‚¨åœ¨é¡¹ç›®ä¸­çš„ `.claude/skills/` ä¸­ï¼š

```bash  theme={null}
mkdir -p .claude/skills/my-skill-name
```

**ä½¿ç”¨é¡¹ç›® Skills çš„åœºæ™¯**ï¼š

* å›¢é˜Ÿå·¥ä½œæµå’Œçº¦å®š
* é¡¹ç›®ç‰¹å®šçš„ä¸“ä¸šçŸ¥è¯†
* å…±äº«çš„å®ç”¨ç¨‹åºå’Œè„šæœ¬

é¡¹ç›® Skills è¢«æ£€å…¥ git å¹¶è‡ªåŠ¨å¯¹å›¢é˜Ÿæˆå‘˜å¯ç”¨ã€‚

### æ’ä»¶ Skills

Skills ä¹Ÿå¯ä»¥æ¥è‡ª [Claude Code æ’ä»¶](/zh-CN/plugins)ã€‚æ’ä»¶å¯èƒ½æ†ç»‘äº†åœ¨å®‰è£…æ’ä»¶æ—¶è‡ªåŠ¨å¯ç”¨çš„ Skillsã€‚è¿™äº› Skills çš„å·¥ä½œæ–¹å¼ä¸ä¸ªäººå’Œé¡¹ç›® Skills ç›¸åŒã€‚

## ç¼–å†™ SKILL.md

åˆ›å»ºä¸€ä¸ªå¸¦æœ‰ YAML frontmatter å’Œ Markdown å†…å®¹çš„ `SKILL.md` æ–‡ä»¶ï¼š

```yaml  theme={null}
---
name: your-skill-name
description: Brief description of what this Skill does and when to use it
---

# Your Skill Name

## Instructions
Provide clear, step-by-step guidance for Claude.

## Examples
Show concrete examples of using this Skill.
```

**å­—æ®µè¦æ±‚**ï¼š

* `name`ï¼šå¿…é¡»ä»…ä½¿ç”¨å°å†™å­—æ¯ã€æ•°å­—å’Œè¿å­—ç¬¦ï¼ˆæœ€å¤š 64 ä¸ªå­—ç¬¦ï¼‰
* `description`ï¼šSkill çš„ç®€è¦æè¿°åŠå…¶ä½¿ç”¨æ—¶æœºï¼ˆæœ€å¤š 1024 ä¸ªå­—ç¬¦ï¼‰

`description` å­—æ®µå¯¹äº Claude å‘ç°ä½•æ—¶ä½¿ç”¨æ‚¨çš„ Skill è‡³å…³é‡è¦ã€‚å®ƒåº”è¯¥åŒ…æ‹¬ Skill çš„åŠŸèƒ½å’Œ Claude åº”è¯¥ä½•æ—¶ä½¿ç”¨å®ƒã€‚

æœ‰å…³å®Œæ•´çš„ç¼–å†™æŒ‡å¯¼ï¼ˆåŒ…æ‹¬éªŒè¯è§„åˆ™ï¼‰ï¼Œè¯·å‚é˜… [æœ€ä½³å®è·µæŒ‡å—](https://docs.claude.com/en/docs/agents-and-tools/agent-skills/best-practices)ã€‚

## æ·»åŠ æ”¯æŒæ–‡ä»¶

åœ¨ SKILL.md æ—è¾¹åˆ›å»ºå…¶ä»–æ–‡ä»¶ï¼š

```
my-skill/
â”œâ”€â”€ SKILL.md (required)
â”œâ”€â”€ reference.md (optional documentation)
â”œâ”€â”€ examples.md (optional examples)
â”œâ”€â”€ scripts/
â”‚   â””â”€â”€ helper.py (optional utility)
â””â”€â”€ templates/
    â””â”€â”€ template.txt (optional template)
```

ä» SKILL.md å¼•ç”¨è¿™äº›æ–‡ä»¶ï¼š

````markdown  theme={null}
For advanced usage, see [reference.md](reference.md).

Run the helper script:
```bash
python scripts/helper.py input.txt
```
````

Claude ä»…åœ¨éœ€è¦æ—¶è¯»å–è¿™äº›æ–‡ä»¶ï¼Œä½¿ç”¨æ¸è¿›å¼æŠ«éœ²æ¥æœ‰æ•ˆç®¡ç†ä¸Šä¸‹æ–‡ã€‚

## ä½¿ç”¨ allowed-tools é™åˆ¶å·¥å…·è®¿é—®

ä½¿ç”¨ `allowed-tools` frontmatter å­—æ®µæ¥é™åˆ¶å½“ Skill å¤„äºæ´»åŠ¨çŠ¶æ€æ—¶ Claude å¯ä»¥ä½¿ç”¨å“ªäº›å·¥å…·ï¼š

```yaml  theme={null}
---
name: safe-file-reader
description: Read files without making changes. Use when you need read-only file access.
allowed-tools: Read, Grep, Glob
---

# Safe File Reader

This Skill provides read-only file access.

## Instructions
1. Use Read to view file contents
2. Use Grep to search within files
3. Use Glob to find files by pattern
```

å½“æ­¤ Skill å¤„äºæ´»åŠ¨çŠ¶æ€æ—¶ï¼ŒClaude åªèƒ½ä½¿ç”¨æŒ‡å®šçš„å·¥å…·ï¼ˆReadã€Grepã€Globï¼‰ï¼Œæ— éœ€è¯·æ±‚æƒé™ã€‚è¿™å¯¹ä»¥ä¸‹æƒ…å†µå¾ˆæœ‰ç”¨ï¼š

* ä¸åº”ä¿®æ”¹æ–‡ä»¶çš„åªè¯» Skills
* èŒƒå›´æœ‰é™çš„ Skillsï¼ˆä¾‹å¦‚ï¼Œä»…æ•°æ®åˆ†æï¼Œæ— æ–‡ä»¶å†™å…¥ï¼‰
* å®‰å…¨æ•æ„Ÿçš„å·¥ä½œæµï¼Œæ‚¨å¸Œæœ›é™åˆ¶åŠŸèƒ½

å¦‚æœæœªæŒ‡å®š `allowed-tools`ï¼ŒClaude å°†æŒ‰ç…§æ ‡å‡†æƒé™æ¨¡å‹è¦æ±‚æƒé™æ¥ä½¿ç”¨å·¥å…·ã€‚

<Note>
  `allowed-tools` ä»…åœ¨ Claude Code ä¸­çš„ Skills æ”¯æŒã€‚
</Note>

## æŸ¥çœ‹å¯ç”¨çš„ Skills

Skills ç”± Claude ä»ä¸‰ä¸ªæ¥æºè‡ªåŠ¨å‘ç°ï¼š

* ä¸ªäºº Skillsï¼š`~/.claude/skills/`
* é¡¹ç›® Skillsï¼š`.claude/skills/`
* æ’ä»¶ Skillsï¼šä¸å·²å®‰è£…çš„æ’ä»¶æ†ç»‘

**è¦æŸ¥çœ‹æ‰€æœ‰å¯ç”¨çš„ Skills**ï¼Œç›´æ¥è¯¢é—® Claudeï¼š

```
What Skills are available?
```

æˆ–

```
List all available Skills
```

è¿™å°†æ˜¾ç¤ºæ¥è‡ªæ‰€æœ‰æ¥æºçš„æ‰€æœ‰ Skillsï¼ŒåŒ…æ‹¬æ’ä»¶ Skillsã€‚

**è¦æ£€æŸ¥ç‰¹å®šçš„ Skill**ï¼Œæ‚¨ä¹Ÿå¯ä»¥æ£€æŸ¥æ–‡ä»¶ç³»ç»Ÿï¼š

```bash  theme={null}
# List personal Skills
ls ~/.claude/skills/

# List project Skills (if in a project directory)
ls .claude/skills/

# View a specific Skill's content
cat ~/.claude/skills/my-skill/SKILL.md
```

## æµ‹è¯• Skill

åˆ›å»º Skill åï¼Œé€šè¿‡æå‡ºä¸æ‚¨çš„æè¿°ç›¸åŒ¹é…çš„é—®é¢˜æ¥æµ‹è¯•å®ƒã€‚

**ç¤ºä¾‹**ï¼šå¦‚æœæ‚¨çš„æè¿°æåˆ°"PDF æ–‡ä»¶"ï¼š

```
Can you help me extract text from this PDF?
```

Claude è‡ªä¸»å†³å®šæ˜¯å¦ä½¿ç”¨æ‚¨çš„ Skillï¼ˆå¦‚æœå®ƒä¸è¯·æ±‚åŒ¹é…ï¼‰â€”â€”æ‚¨æ— éœ€æ˜¾å¼è°ƒç”¨å®ƒã€‚Skill æ ¹æ®æ‚¨é—®é¢˜çš„ä¸Šä¸‹æ–‡è‡ªåŠ¨æ¿€æ´»ã€‚

## è°ƒè¯• Skill

å¦‚æœ Claude ä¸ä½¿ç”¨æ‚¨çš„ Skillï¼Œè¯·æ£€æŸ¥è¿™äº›å¸¸è§é—®é¢˜ï¼š

### ä½¿æè¿°æ›´å…·ä½“

**å¤ªæ¨¡ç³Š**ï¼š

```yaml  theme={null}
description: Helps with documents
```

**å…·ä½“**ï¼š

```yaml  theme={null}
description: Extract text and tables from PDF files, fill forms, merge documents. Use when working with PDF files or when the user mentions PDFs, forms, or document extraction.
```

åœ¨æè¿°ä¸­åŒ…æ‹¬ Skill çš„åŠŸèƒ½å’Œä½¿ç”¨æ—¶æœºã€‚

### éªŒè¯æ–‡ä»¶è·¯å¾„

**ä¸ªäºº Skills**ï¼š`~/.claude/skills/skill-name/SKILL.md`
**é¡¹ç›® Skills**ï¼š`.claude/skills/skill-name/SKILL.md`

æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨ï¼š

```bash  theme={null}
# Personal
ls ~/.claude/skills/my-skill/SKILL.md

# Project
ls .claude/skills/my-skill/SKILL.md
```

### æ£€æŸ¥ YAML è¯­æ³•

æ— æ•ˆçš„ YAML ä¼šé˜»æ­¢ Skill åŠ è½½ã€‚éªŒè¯ frontmatterï¼š

```bash  theme={null}
cat SKILL.md | head -n 10
```

ç¡®ä¿ï¼š

* ç¬¬ 1 è¡Œæœ‰å¼€å¤´çš„ `---`
* Markdown å†…å®¹å‰æœ‰ç»“å°¾çš„ `---`
* æœ‰æ•ˆçš„ YAML è¯­æ³•ï¼ˆæ— åˆ¶è¡¨ç¬¦ï¼Œæ­£ç¡®çš„ç¼©è¿›ï¼‰

### æŸ¥çœ‹é”™è¯¯

ä½¿ç”¨è°ƒè¯•æ¨¡å¼è¿è¡Œ Claude Code ä»¥æŸ¥çœ‹ Skill åŠ è½½é”™è¯¯ï¼š

```bash  theme={null}
claude --debug
```

## ä¸æ‚¨çš„å›¢é˜Ÿå…±äº« Skills

**æ¨èæ–¹æ³•**ï¼šé€šè¿‡ [æ’ä»¶](/zh-CN/plugins) åˆ†å‘ Skillsã€‚

é€šè¿‡æ’ä»¶å…±äº« Skillsï¼š

1. åˆ›å»ºä¸€ä¸ªåœ¨ `skills/` ç›®å½•ä¸­åŒ…å« Skills çš„æ’ä»¶
2. å°†æ’ä»¶æ·»åŠ åˆ°å¸‚åœº
3. å›¢é˜Ÿæˆå‘˜å®‰è£…æ’ä»¶

æœ‰å…³å®Œæ•´è¯´æ˜ï¼Œè¯·å‚é˜… [å°† Skills æ·»åŠ åˆ°æ‚¨çš„æ’ä»¶](/zh-CN/plugins#add-skills-to-your-plugin)ã€‚

æ‚¨ä¹Ÿå¯ä»¥ç›´æ¥é€šè¿‡é¡¹ç›®å­˜å‚¨åº“å…±äº« Skillsï¼š

### ç¬¬ 1 æ­¥ï¼šå°† Skill æ·»åŠ åˆ°æ‚¨çš„é¡¹ç›®

åˆ›å»ºé¡¹ç›® Skillï¼š

```bash  theme={null}
mkdir -p .claude/skills/team-skill
# Create SKILL.md
```

### ç¬¬ 2 æ­¥ï¼šæäº¤åˆ° git

```bash  theme={null}
git add .claude/skills/
git commit -m "Add team Skill for PDF processing"
git push
```

### ç¬¬ 3 æ­¥ï¼šå›¢é˜Ÿæˆå‘˜è‡ªåŠ¨è·å¾— Skills

å½“å›¢é˜Ÿæˆå‘˜æ‹‰å–æœ€æ–°æ›´æ”¹æ—¶ï¼ŒSkills ç«‹å³å¯ç”¨ï¼š

```bash  theme={null}
git pull
claude  # Skills are now available
```

## æ›´æ–° Skill

ç›´æ¥ç¼–è¾‘ SKILL.mdï¼š

```bash  theme={null}
# Personal Skill
code ~/.claude/skills/my-skill/SKILL.md

# Project Skill
code .claude/skills/my-skill/SKILL.md
```

æ›´æ”¹åœ¨æ‚¨ä¸‹æ¬¡å¯åŠ¨ Claude Code æ—¶ç”Ÿæ•ˆã€‚å¦‚æœ Claude Code å·²åœ¨è¿è¡Œï¼Œè¯·é‡æ–°å¯åŠ¨å®ƒä»¥åŠ è½½æ›´æ–°ã€‚

## åˆ é™¤ Skill

åˆ é™¤ Skill ç›®å½•ï¼š

```bash  theme={null}
# Personal
rm -rf ~/.claude/skills/my-skill

# Project
rm -rf .claude/skills/my-skill
git commit -m "Remove unused Skill"
```

## æœ€ä½³å®è·µ

### ä¿æŒ Skills ä¸“æ³¨

ä¸€ä¸ª Skill åº”è¯¥è§£å†³ä¸€ä¸ªåŠŸèƒ½ï¼š

**ä¸“æ³¨**ï¼š

* "PDF è¡¨å•å¡«å……"
* "Excel æ•°æ®åˆ†æ"
* "Git æäº¤æ¶ˆæ¯"

**å¤ªå®½æ³›**ï¼š

* "æ–‡æ¡£å¤„ç†"ï¼ˆåˆ†æˆå•ç‹¬çš„ Skillsï¼‰
* "æ•°æ®å·¥å…·"ï¼ˆæŒ‰æ•°æ®ç±»å‹æˆ–æ“ä½œåˆ†å‰²ï¼‰

### ç¼–å†™æ¸…æ™°çš„æè¿°

é€šè¿‡åœ¨æè¿°ä¸­åŒ…å«ç‰¹å®šè§¦å‘å™¨æ¥å¸®åŠ© Claude å‘ç°ä½•æ—¶ä½¿ç”¨ Skillsï¼š

**æ¸…æ™°**ï¼š

```yaml  theme={null}
description: Analyze Excel spreadsheets, create pivot tables, and generate charts. Use when working with Excel files, spreadsheets, or analyzing tabular data in .xlsx format.
```

**æ¨¡ç³Š**ï¼š

```yaml  theme={null}
description: For files
```

### ä¸æ‚¨çš„å›¢é˜Ÿä¸€èµ·æµ‹è¯•

è®©é˜Ÿå‹ä½¿ç”¨ Skills å¹¶æä¾›åé¦ˆï¼š

* Skill æ˜¯å¦åœ¨é¢„æœŸæ—¶æ¿€æ´»ï¼Ÿ
* è¯´æ˜æ˜¯å¦æ¸…æ™°ï¼Ÿ
* æ˜¯å¦æœ‰ç¼ºå¤±çš„ç¤ºä¾‹æˆ–è¾¹ç•Œæƒ…å†µï¼Ÿ

### è®°å½• Skill ç‰ˆæœ¬

æ‚¨å¯ä»¥åœ¨ SKILL.md å†…å®¹ä¸­è®°å½• Skill ç‰ˆæœ¬ä»¥è·Ÿè¸ªéšæ—¶é—´çš„å˜åŒ–ã€‚æ·»åŠ ç‰ˆæœ¬å†å²éƒ¨åˆ†ï¼š

```markdown  theme={null}
# My Skill

## Version History
- v2.0.0 (2025-10-01): Breaking changes to API
- v1.1.0 (2025-09-15): Added new features
- v1.0.0 (2025-09-01): Initial release
```

è¿™æœ‰åŠ©äºå›¢é˜Ÿæˆå‘˜äº†è§£ç‰ˆæœ¬ä¹‹é—´çš„å˜åŒ–ã€‚

## æ•…éšœæ’é™¤

### Claude ä¸ä½¿ç”¨æˆ‘çš„ Skill

**ç—‡çŠ¶**ï¼šæ‚¨æå‡ºç›¸å…³é—®é¢˜ï¼Œä½† Claude ä¸ä½¿ç”¨æ‚¨çš„ Skillã€‚

**æ£€æŸ¥**ï¼šæè¿°æ˜¯å¦è¶³å¤Ÿå…·ä½“ï¼Ÿ

æ¨¡ç³Šçš„æè¿°ä¼šä½¿å‘ç°å˜å¾—å›°éš¾ã€‚åŒ…æ‹¬ Skill çš„åŠŸèƒ½å’Œä½¿ç”¨æ—¶æœºï¼Œä»¥åŠç”¨æˆ·ä¼šæåˆ°çš„å…³é”®æœ¯è¯­ã€‚

**å¤ªé€šç”¨**ï¼š

```yaml  theme={null}
description: Helps with data
```

**å…·ä½“**ï¼š

```yaml  theme={null}
description: Analyze Excel spreadsheets, generate pivot tables, create charts. Use when working with Excel files, spreadsheets, or .xlsx files.
```

**æ£€æŸ¥**ï¼šYAML æ˜¯å¦æœ‰æ•ˆï¼Ÿ

è¿è¡ŒéªŒè¯ä»¥æ£€æŸ¥è¯­æ³•é”™è¯¯ï¼š

```bash  theme={null}
# View frontmatter
cat .claude/skills/my-skill/SKILL.md | head -n 15

# Check for common issues
# - Missing opening or closing ---
# - Tabs instead of spaces
# - Unquoted strings with special characters
```

**æ£€æŸ¥**ï¼šSkill æ˜¯å¦åœ¨æ­£ç¡®çš„ä½ç½®ï¼Ÿ

```bash  theme={null}
# Personal Skills
ls ~/.claude/skills/*/SKILL.md

# Project Skills
ls .claude/skills/*/SKILL.md
```

### Skill æœ‰é”™è¯¯

**ç—‡çŠ¶**ï¼šSkill åŠ è½½ä½†æ— æ³•æ­£å¸¸å·¥ä½œã€‚

**æ£€æŸ¥**ï¼šä¾èµ–é¡¹æ˜¯å¦å¯ç”¨ï¼Ÿ

Claude å°†åœ¨éœ€è¦æ—¶è‡ªåŠ¨å®‰è£…æ‰€éœ€çš„ä¾èµ–é¡¹ï¼ˆæˆ–è¦æ±‚æƒé™å®‰è£…å®ƒä»¬ï¼‰ã€‚

**æ£€æŸ¥**ï¼šè„šæœ¬æ˜¯å¦æœ‰æ‰§è¡Œæƒé™ï¼Ÿ

```bash  theme={null}
chmod +x .claude/skills/my-skill/scripts/*.py
```

**æ£€æŸ¥**ï¼šæ–‡ä»¶è·¯å¾„æ˜¯å¦æ­£ç¡®ï¼Ÿ

åœ¨æ‰€æœ‰è·¯å¾„ä¸­ä½¿ç”¨æ­£æ–œæ ï¼ˆUnix é£æ ¼ï¼‰ï¼š

**æ­£ç¡®**ï¼š`scripts/helper.py`
**é”™è¯¯**ï¼š`scripts\helper.py`ï¼ˆWindows é£æ ¼ï¼‰

### å¤šä¸ª Skills å†²çª

**ç—‡çŠ¶**ï¼šClaude ä½¿ç”¨äº†é”™è¯¯çš„ Skill æˆ–ä¼¼ä¹åœ¨ç›¸ä¼¼çš„ Skills ä¹‹é—´æ„Ÿåˆ°å›°æƒ‘ã€‚

**åœ¨æè¿°ä¸­è¦å…·ä½“**ï¼šé€šè¿‡åœ¨æè¿°ä¸­ä½¿ç”¨ä¸åŒçš„è§¦å‘æœ¯è¯­æ¥å¸®åŠ© Claude é€‰æ‹©æ­£ç¡®çš„ Skillã€‚

è€Œä¸æ˜¯ï¼š

```yaml  theme={null}
# Skill 1
description: For data analysis

# Skill 2
description: For analyzing data
```

ä½¿ç”¨ï¼š

```yaml  theme={null}
# Skill 1
description: Analyze sales data in Excel files and CRM exports. Use for sales reports, pipeline analysis, and revenue tracking.

# Skill 2
description: Analyze log files and system metrics data. Use for performance monitoring, debugging, and system diagnostics.
```

## ç¤ºä¾‹

### ç®€å• Skillï¼ˆå•ä¸ªæ–‡ä»¶ï¼‰

```
commit-helper/
â””â”€â”€ SKILL.md
```

```yaml  theme={null}
---
name: generating-commit-messages
description: Generates clear commit messages from git diffs. Use when writing commit messages or reviewing staged changes.
---

# Generating Commit Messages

## Instructions

1. Run `git diff --staged` to see changes
2. I'll suggest a commit message with:
   - Summary under 50 characters
   - Detailed description
   - Affected components

## Best practices

- Use present tense
- Explain what and why, not how
```

### å…·æœ‰å·¥å…·æƒé™çš„ Skill

```
code-reviewer/
â””â”€â”€ SKILL.md
```

```yaml  theme={null}
---
name: code-reviewer
description: Review code for best practices and potential issues. Use when reviewing code, checking PRs, or analyzing code quality.
allowed-tools: Read, Grep, Glob
---

# Code Reviewer

## Review checklist

1. Code organization and structure
2. Error handling
3. Performance considerations
4. Security concerns
5. Test coverage

## Instructions

1. Read the target files using Read tool
2. Search for patterns using Grep
3. Find related files using Glob
4. Provide detailed feedback on code quality
```

### å¤šæ–‡ä»¶ Skill

```
pdf-processing/
â”œâ”€â”€ SKILL.md
â”œâ”€â”€ FORMS.md
â”œâ”€â”€ REFERENCE.md
â””â”€â”€ scripts/
    â”œâ”€â”€ fill_form.py
    â””â”€â”€ validate.py
```

**SKILL.md**ï¼š

````yaml  theme={null}
---
name: pdf-processing
description: Extract text, fill forms, merge PDFs. Use when working with PDF files, forms, or document extraction. Requires pypdf and pdfplumber packages.
---

# PDF Processing

## Quick start

Extract text:
```python
import pdfplumber
with pdfplumber.open("doc.pdf") as pdf:
    text = pdf.pages[0].extract_text()
```

For form filling, see [FORMS.md](FORMS.md).
For detailed API reference, see [REFERENCE.md](REFERENCE.md).

## Requirements

Packages must be installed in your environment:
```bash
pip install pypdf pdfplumber
```
````

<Note>
  åœ¨æè¿°ä¸­åˆ—å‡ºæ‰€éœ€çš„åŒ…ã€‚åœ¨ Claude å¯ä»¥ä½¿ç”¨å®ƒä»¬ä¹‹å‰ï¼Œå¿…é¡»åœ¨æ‚¨çš„ç¯å¢ƒä¸­å®‰è£…åŒ…ã€‚
</Note>

Claude ä»…åœ¨éœ€è¦æ—¶åŠ è½½å…¶ä»–æ–‡ä»¶ã€‚

## åç»­æ­¥éª¤

<CardGroup cols={2}>
  <Card title="ç¼–å†™æœ€ä½³å®è·µ" icon="lightbulb" href="https://docs.claude.com/en/docs/agents-and-tools/agent-skills/best-practices">
    ç¼–å†™ Claude å¯ä»¥æœ‰æ•ˆä½¿ç”¨çš„ Skills
  </Card>

  <Card title="Agent Skills æ¦‚è¿°" icon="book" href="https://docs.claude.com/en/docs/agents-and-tools/agent-skills/overview">
    äº†è§£ Skills å¦‚ä½•åœ¨ Claude äº§å“ä¸­å·¥ä½œ
  </Card>

  <Card title="åœ¨ Agent SDK ä¸­ä½¿ç”¨ Skills" icon="cube" href="https://docs.claude.com/en/docs/agent-sdk/skills">
    ä½¿ç”¨ TypeScript å’Œ Python ä»¥ç¼–ç¨‹æ–¹å¼ä½¿ç”¨ Skills
  </Card>

  <Card title="å¼€å§‹ä½¿ç”¨ Agent Skills" icon="rocket" href="https://docs.claude.com/en/docs/agents-and-tools/agent-skills/quickstart">
    åˆ›å»ºæ‚¨çš„ç¬¬ä¸€ä¸ª Skill
  </Card>
</CardGroup>


---

> To find navigation and other pages in this documentation, fetch the llms.txt file at: https://code.claude.com/docs/llms.txt
</file>

<file path="RESEARCH/.example/.gitkeep">
# This is a template directory structure for research projects.
# Each research topic should follow this structure:
#
# RESEARCH/[topic_name]/
# â”œâ”€â”€ README.md                    # Overview and navigation
# â”œâ”€â”€ executive_summary.md         # 1-2 page key findings
# â”œâ”€â”€ full_report.md               # Complete analysis
# â”œâ”€â”€ data/
# â”‚   â”œâ”€â”€ raw/                     # Original downloaded files (not in LLM context)
# â”‚   â”œâ”€â”€ processed/               # Cleaned markdown files (for LLM context)
# â”‚   â””â”€â”€ structured/              # JSON format metrics and data
# â”œâ”€â”€ sources/
# â”‚   â”œâ”€â”€ bibliography.md          # Complete citations
# â”‚   â””â”€â”€ source_quality_table.md  # A-E quality ratings
# â”œâ”€â”€ research_notes/
# â”‚   â””â”€â”€ agent_findings_summary.md
# â””â”€â”€ appendices/
#     â”œâ”€â”€ methodology.md
#     â””â”€â”€ limitations.md
</file>

<file path="scripts/preprocess_document.py">
#!/usr/bin/env python3
"""
Document Preprocessing Script for Deep Research Agent

This script cleans and processes raw HTML/PDF documents to reduce token consumption
by removing ads, navigation, styles, and other non-essential content.

Usage:
    python3 scripts/preprocess_document.py <input_file>

Example:
    python3 scripts/preprocess_document.py RESEARCH/topic/data/raw/source.html

Output:
    Cleaned markdown file in RESEARCH/topic/data/processed/
    JSON status with token savings
"""

import sys
import os
import json
import re
from pathlib import Path

try:
    from bs4 import BeautifulSoup
    HAS_BS4 = True
except ImportError:
    HAS_BS4 = False


def clean_html(raw_html: str) -> str:
    """Remove scripts, styles, navigation, keeping only core text content."""
    if not HAS_BS4:
        # Fallback: basic regex cleaning if BeautifulSoup not available
        text = re.sub(r'<script[^>]*>.*?</script>', '', raw_html, flags=re.DOTALL | re.IGNORECASE)
        text = re.sub(r'<style[^>]*>.*?</style>', '', text, flags=re.DOTALL | re.IGNORECASE)
        text = re.sub(r'<nav[^>]*>.*?</nav>', '', text, flags=re.DOTALL | re.IGNORECASE)
        text = re.sub(r'<footer[^>]*>.*?</footer>', '', text, flags=re.DOTALL | re.IGNORECASE)
        text = re.sub(r'<header[^>]*>.*?</header>', '', text, flags=re.DOTALL | re.IGNORECASE)
        text = re.sub(r'<aside[^>]*>.*?</aside>', '', text, flags=re.DOTALL | re.IGNORECASE)
        text = re.sub(r'<[^>]+>', ' ', text)
        text = re.sub(r'\s+', ' ', text)
        return text.strip()

    soup = BeautifulSoup(raw_html, 'html.parser')

    # Remove interference elements
    tags_to_remove = [
        "script", "style", "nav", "footer", "header",
        "aside", "iframe", "noscript", "form", "button",
        "input", "select", "textarea", "svg", "canvas",
        "advertisement", "ad", "banner", "popup", "modal"
    ]

    for tag in soup(tags_to_remove):
        tag.extract()

    # Remove elements by common ad/nav class names
    ad_patterns = [
        'ad', 'ads', 'advertisement', 'banner', 'sidebar',
        'nav', 'navigation', 'menu', 'footer', 'header',
        'popup', 'modal', 'overlay', 'cookie', 'newsletter',
        'social', 'share', 'comment', 'related', 'recommended'
    ]

    for pattern in ad_patterns:
        for element in soup.find_all(class_=re.compile(pattern, re.I)):
            element.extract()
        for element in soup.find_all(id=re.compile(pattern, re.I)):
            element.extract()

    # Extract text with newlines
    text = soup.get_text(separator='\n')

    # Clean whitespace
    lines = (line.strip() for line in text.splitlines())
    chunks = (phrase.strip() for line in lines for phrase in line.split("  "))
    text = '\n'.join(chunk for chunk in chunks if chunk)

    # Remove excessive newlines
    text = re.sub(r'\n{3,}', '\n\n', text)

    return text


def extract_metadata(soup_or_text, doc_type: str) -> dict:
    """Extract metadata from document."""
    metadata = {
        "title": "",
        "description": "",
        "author": "",
        "date": ""
    }

    if HAS_BS4 and doc_type == "html":
        try:
            soup = BeautifulSoup(soup_or_text, 'html.parser') if isinstance(soup_or_text, str) else soup_or_text

            # Title
            title_tag = soup.find('title')
            if title_tag:
                metadata["title"] = title_tag.get_text().strip()

            # Meta description
            desc_meta = soup.find('meta', attrs={'name': 'description'})
            if desc_meta:
                metadata["description"] = desc_meta.get('content', '')

            # Author
            author_meta = soup.find('meta', attrs={'name': 'author'})
            if author_meta:
                metadata["author"] = author_meta.get('content', '')

            # Date
            date_meta = soup.find('meta', attrs={'name': re.compile(r'date|published', re.I)})
            if date_meta:
                metadata["date"] = date_meta.get('content', '')
        except Exception:
            pass

    return metadata


def estimate_tokens(text: str) -> int:
    """Estimate token count (rough approximation: 1 token ~ 4 characters for English)."""
    return len(text) // 4


def process_file(input_path: str) -> dict:
    """Process a single file and return status."""
    input_path = Path(input_path).resolve()

    if not input_path.exists():
        return {"error": f"File not found: {input_path}", "status": "failed"}

    # Determine output path
    # Expected structure: RESEARCH/topic/data/raw/file.html -> RESEARCH/topic/data/processed/file_cleaned.md
    try:
        parts = input_path.parts
        raw_idx = parts.index('raw')
        base_parts = parts[:raw_idx]
        processed_dir = Path(*base_parts) / "processed"
    except ValueError:
        # Fallback: put processed file next to raw file
        processed_dir = input_path.parent.parent / "processed"

    processed_dir.mkdir(parents=True, exist_ok=True)

    filename = input_path.name
    name_without_ext = input_path.stem
    output_path = processed_dir / f"{name_without_ext}_cleaned.md"

    # Read file
    try:
        with open(input_path, 'r', encoding='utf-8', errors='ignore') as f:
            content = f.read()
    except Exception as e:
        return {"error": f"Failed to read file: {e}", "status": "failed"}

    original_tokens = estimate_tokens(content)

    # Determine document type and process
    if input_path.suffix.lower() in ['.html', '.htm'] or '<html' in content[:500].lower():
        cleaned_text = clean_html(content)
        doc_type = "html"
        metadata = extract_metadata(content, "html")
    elif input_path.suffix.lower() == '.pdf':
        # PDF processing would require additional libraries (PyPDF2, pdfplumber)
        # For now, just pass through
        cleaned_text = content
        doc_type = "pdf"
        metadata = {}
    else:
        # Assume plain text
        cleaned_text = content
        doc_type = "text"
        metadata = {}

    cleaned_tokens = estimate_tokens(cleaned_text)
    saved_tokens = original_tokens - cleaned_tokens

    # Build output with YAML frontmatter
    frontmatter = f"""---
original: {input_path}
original_tokens: {original_tokens}
cleaned_tokens: {cleaned_tokens}
saved_tokens: {saved_tokens}
type: {doc_type}
title: {metadata.get('title', '')}
author: {metadata.get('author', '')}
date: {metadata.get('date', '')}
---

"""

    # Write output
    try:
        with open(output_path, 'w', encoding='utf-8') as f:
            f.write(frontmatter)
            f.write(cleaned_text)
    except Exception as e:
        return {"error": f"Failed to write file: {e}", "status": "failed"}

    return {
        "status": "success",
        "input_path": str(input_path),
        "output_path": str(output_path),
        "original_tokens": original_tokens,
        "cleaned_tokens": cleaned_tokens,
        "saved_tokens": saved_tokens,
        "savings_percent": round((saved_tokens / original_tokens * 100) if original_tokens > 0 else 0, 1),
        "doc_type": doc_type
    }


def process_directory(input_dir: str) -> list:
    """Process all files in a directory."""
    input_path = Path(input_dir).resolve()
    results = []

    if not input_path.is_dir():
        return [{"error": f"Not a directory: {input_dir}"}]

    for file_path in input_path.glob('*'):
        if file_path.is_file() and file_path.suffix.lower() in ['.html', '.htm', '.txt', '.md']:
            result = process_file(str(file_path))
            results.append(result)

    return results


def main():
    if len(sys.argv) < 2:
        print(json.dumps({
            "error": "Usage: python3 preprocess_document.py <input_file_or_directory>",
            "status": "failed"
        }))
        sys.exit(1)

    input_path = sys.argv[1]

    if os.path.isdir(input_path):
        results = process_directory(input_path)
        print(json.dumps({"status": "success", "results": results}, indent=2))
    else:
        result = process_file(input_path)
        print(json.dumps(result, indent=2))


if __name__ == "__main__":
    main()
</file>

<file path="scripts/url_manifest.py">
#!/usr/bin/env python3
"""
URL Manifest Manager for Deep Research Agent

Manages a global URL cache to prevent duplicate downloads across parallel agents.
Each agent must check the manifest before fetching a URL.

Usage:
    # Check if URL exists
    python3 scripts/url_manifest.py check "https://example.com/article" --topic my_topic

    # Register a new URL
    python3 scripts/url_manifest.py register "https://example.com/article" --topic my_topic --local data/raw/article.html

    # List all cached URLs
    python3 scripts/url_manifest.py list --topic my_topic
"""

import sys
import os
import json
import hashlib
from pathlib import Path
from datetime import datetime
from urllib.parse import urlparse
from typing import Optional, Dict, List


class URLManifest:
    """Manages URL to local file mappings."""

    def __init__(self, topic: str):
        self.topic = topic
        self.manifest_path = Path(f"RESEARCH/{topic}/url_manifest.json")
        self.manifest_path.parent.mkdir(parents=True, exist_ok=True)
        self.data = self._load()

    def _load(self) -> Dict:
        """Load manifest from disk."""
        if self.manifest_path.exists():
            try:
                with open(self.manifest_path, 'r', encoding='utf-8') as f:
                    return json.load(f)
            except Exception:
                pass
        return {"urls": {}, "created": datetime.now().isoformat()}

    def _save(self):
        """Save manifest to disk."""
        self.data["updated"] = datetime.now().isoformat()
        with open(self.manifest_path, 'w', encoding='utf-8') as f:
            json.dump(self.data, f, indent=2)

    def _normalize_url(self, url: str) -> str:
        """Normalize URL for consistent lookups."""
        parsed = urlparse(url)
        # Remove trailing slashes, fragments, normalize scheme
        normalized = f"{parsed.scheme}://{parsed.netloc}{parsed.path.rstrip('/')}"
        if parsed.query:
            normalized += f"?{parsed.query}"
        return normalized.lower()

    def _url_hash(self, url: str) -> str:
        """Generate a short hash for URL."""
        return hashlib.md5(self._normalize_url(url).encode()).hexdigest()[:12]

    def check(self, url: str) -> Optional[Dict]:
        """
        Check if URL is already cached.

        Returns:
            Dict with local_path and metadata if exists, None otherwise
        """
        normalized = self._normalize_url(url)
        return self.data["urls"].get(normalized)

    def register(self, url: str, local_path: str, metadata: Optional[Dict] = None) -> Dict:
        """
        Register a URL with its local file path.

        Args:
            url: The source URL
            local_path: Path to local file (raw or processed)
            metadata: Optional metadata (title, fetch_time, etc.)

        Returns:
            The registered entry
        """
        normalized = self._normalize_url(url)

        entry = {
            "url": url,
            "normalized": normalized,
            "local_raw": local_path,
            "local_processed": None,
            "hash": self._url_hash(url),
            "registered": datetime.now().isoformat(),
            "metadata": metadata or {}
        }

        # Check if processed version exists
        raw_path = Path(local_path)
        if "raw" in str(raw_path):
            processed_path = str(raw_path).replace("/raw/", "/processed/").replace(".html", "_cleaned.md")
            if Path(processed_path).exists():
                entry["local_processed"] = processed_path

        self.data["urls"][normalized] = entry
        self._save()
        return entry

    def update_processed(self, url: str, processed_path: str) -> Optional[Dict]:
        """Update the processed file path for a URL."""
        normalized = self._normalize_url(url)
        if normalized in self.data["urls"]:
            self.data["urls"][normalized]["local_processed"] = processed_path
            self._save()
            return self.data["urls"][normalized]
        return None

    def list_urls(self) -> List[Dict]:
        """List all registered URLs."""
        return list(self.data["urls"].values())

    def get_stats(self) -> Dict:
        """Get manifest statistics."""
        urls = self.data["urls"]
        processed_count = sum(1 for u in urls.values() if u.get("local_processed"))
        return {
            "total_urls": len(urls),
            "processed_count": processed_count,
            "unprocessed_count": len(urls) - processed_count,
            "created": self.data.get("created"),
            "updated": self.data.get("updated")
        }

    def remove(self, url: str) -> bool:
        """Remove a URL from the manifest."""
        normalized = self._normalize_url(url)
        if normalized in self.data["urls"]:
            del self.data["urls"][normalized]
            self._save()
            return True
        return False


def main():
    if len(sys.argv) < 2:
        print(json.dumps({
            "error": "Usage: python3 url_manifest.py <command> [args]",
            "commands": [
                "check <url> --topic <name>",
                "register <url> --topic <name> --local <path>",
                "list --topic <name>",
                "stats --topic <name>"
            ],
            "status": "failed"
        }))
        sys.exit(1)

    command = sys.argv[1]

    # Parse arguments
    args = sys.argv[2:]
    topic = "default"
    local_path = None
    url = None

    i = 0
    positional = []
    while i < len(args):
        if args[i] == "--topic" and i + 1 < len(args):
            topic = args[i + 1]
            i += 2
        elif args[i] == "--local" and i + 1 < len(args):
            local_path = args[i + 1]
            i += 2
        else:
            positional.append(args[i])
            i += 1

    if positional:
        url = positional[0]

    manifest = URLManifest(topic)

    if command == "check":
        if not url:
            print(json.dumps({"error": "Missing URL", "status": "failed"}))
            sys.exit(1)

        result = manifest.check(url)
        if result:
            print(json.dumps({
                "status": "found",
                "cached": True,
                "entry": result,
                "message": f"URL already cached. Use local file: {result.get('local_processed') or result.get('local_raw')}"
            }))
        else:
            print(json.dumps({
                "status": "not_found",
                "cached": False,
                "message": "URL not in cache. Safe to fetch."
            }))

    elif command == "register":
        if not url:
            print(json.dumps({"error": "Missing URL", "status": "failed"}))
            sys.exit(1)
        if not local_path:
            print(json.dumps({"error": "Missing --local path", "status": "failed"}))
            sys.exit(1)

        entry = manifest.register(url, local_path)
        print(json.dumps({
            "status": "success",
            "registered": True,
            "entry": entry
        }))

    elif command == "list":
        urls = manifest.list_urls()
        print(json.dumps({
            "status": "success",
            "count": len(urls),
            "urls": urls
        }, indent=2))

    elif command == "stats":
        stats = manifest.get_stats()
        print(json.dumps({
            "status": "success",
            "stats": stats
        }))

    else:
        print(json.dumps({"error": f"Unknown command: {command}", "status": "failed"}))
        sys.exit(1)


if __name__ == "__main__":
    main()
</file>

<file path="scripts/vector_store.py">
#!/usr/bin/env python3
"""
Vector Store for Deep Research Agent - RAG Support

This module provides local vector storage capabilities for the Synthesizer
to query knowledge bases instead of reading entire documents.

Supports:
- ChromaDB (recommended)
- FAISS (fallback)
- Simple in-memory store (minimal dependencies)

Usage:
    # Index a document
    python3 scripts/vector_store.py index RESEARCH/topic/data/processed/document.md

    # Query the knowledge base
    python3 scripts/vector_store.py query "market growth rate" --topic topic_name

    # List indexed documents
    python3 scripts/vector_store.py list --topic topic_name
"""

import sys
import os
import json
import hashlib
import re
from pathlib import Path
from typing import List, Dict, Optional, Tuple
from dataclasses import dataclass, asdict
from datetime import datetime

# Try to import vector DB libraries
try:
    import chromadb
    from chromadb.config import Settings
    HAS_CHROMA = True
except ImportError:
    HAS_CHROMA = False

try:
    import numpy as np
    HAS_NUMPY = True
except ImportError:
    HAS_NUMPY = False


@dataclass
class DocumentChunk:
    """Represents a chunk of text from a document."""
    id: str
    content: str
    source_file: str
    chunk_index: int
    metadata: Dict
    embedding: Optional[List[float]] = None


class SimpleVectorStore:
    """
    Simple in-memory vector store using TF-IDF-like scoring.
    No external dependencies required.
    """

    def __init__(self, store_path: str):
        self.store_path = Path(store_path)
        self.store_path.mkdir(parents=True, exist_ok=True)
        self.index_file = self.store_path / "index.json"
        self.chunks: List[DocumentChunk] = []
        self.vocab: Dict[str, int] = {}
        self._load()

    def _load(self):
        """Load index from disk."""
        if self.index_file.exists():
            try:
                with open(self.index_file, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    self.chunks = [DocumentChunk(**c) for c in data.get('chunks', [])]
                    self.vocab = data.get('vocab', {})
            except Exception:
                self.chunks = []
                self.vocab = {}

    def _save(self):
        """Save index to disk."""
        data = {
            'chunks': [asdict(c) for c in self.chunks],
            'vocab': self.vocab,
            'updated': datetime.now().isoformat()
        }
        with open(self.index_file, 'w', encoding='utf-8') as f:
            json.dump(data, f, indent=2)

    def _tokenize(self, text: str) -> List[str]:
        """Simple tokenization."""
        text = text.lower()
        text = re.sub(r'[^\w\s]', ' ', text)
        tokens = text.split()
        # Remove stopwords
        stopwords = {'the', 'a', 'an', 'is', 'are', 'was', 'were', 'be', 'been',
                    'being', 'have', 'has', 'had', 'do', 'does', 'did', 'will',
                    'would', 'could', 'should', 'may', 'might', 'must', 'shall',
                    'of', 'to', 'in', 'for', 'on', 'with', 'at', 'by', 'from',
                    'as', 'or', 'and', 'but', 'if', 'then', 'that', 'this'}
        return [t for t in tokens if t not in stopwords and len(t) > 2]

    def _compute_tf(self, tokens: List[str]) -> Dict[str, float]:
        """Compute term frequency."""
        tf = {}
        for token in tokens:
            tf[token] = tf.get(token, 0) + 1
        # Normalize
        max_freq = max(tf.values()) if tf else 1
        return {k: v / max_freq for k, v in tf.items()}

    def _score_relevance(self, query_tokens: List[str], chunk_tokens: List[str]) -> float:
        """Score relevance between query and chunk using TF overlap."""
        query_set = set(query_tokens)
        chunk_tf = self._compute_tf(chunk_tokens)

        score = 0.0
        for token in query_set:
            if token in chunk_tf:
                score += chunk_tf[token]

        # Normalize by query length
        return score / len(query_set) if query_set else 0.0

    def add_document(self, file_path: str, chunk_size: int = 500, overlap: int = 50) -> int:
        """
        Add a document to the vector store.

        Args:
            file_path: Path to the document
            chunk_size: Number of words per chunk
            overlap: Number of words to overlap between chunks

        Returns:
            Number of chunks added
        """
        file_path = Path(file_path).resolve()

        if not file_path.exists():
            raise FileNotFoundError(f"File not found: {file_path}")

        # Read document
        with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
            content = f.read()

        # Extract YAML frontmatter if present
        metadata = {}
        if content.startswith('---'):
            try:
                end_idx = content.index('---', 3)
                frontmatter = content[3:end_idx]
                content = content[end_idx + 3:]
                for line in frontmatter.strip().split('\n'):
                    if ':' in line:
                        key, value = line.split(':', 1)
                        metadata[key.strip()] = value.strip()
            except ValueError:
                pass

        # Remove existing chunks from this file
        self.chunks = [c for c in self.chunks if c.source_file != str(file_path)]

        # Split into chunks
        words = content.split()
        chunks_added = 0

        for i in range(0, len(words), chunk_size - overlap):
            chunk_words = words[i:i + chunk_size]
            if len(chunk_words) < 50:  # Skip very small chunks
                continue

            chunk_text = ' '.join(chunk_words)
            chunk_id = hashlib.md5(f"{file_path}:{i}".encode()).hexdigest()[:12]

            chunk = DocumentChunk(
                id=chunk_id,
                content=chunk_text,
                source_file=str(file_path),
                chunk_index=chunks_added,
                metadata=metadata
            )

            self.chunks.append(chunk)
            chunks_added += 1

            # Update vocabulary
            tokens = self._tokenize(chunk_text)
            for token in tokens:
                self.vocab[token] = self.vocab.get(token, 0) + 1

        self._save()
        return chunks_added

    def query(self, query: str, top_k: int = 5) -> List[Tuple[DocumentChunk, float]]:
        """
        Query the vector store.

        Args:
            query: Search query
            top_k: Number of results to return

        Returns:
            List of (chunk, score) tuples
        """
        query_tokens = self._tokenize(query)

        results = []
        for chunk in self.chunks:
            chunk_tokens = self._tokenize(chunk.content)
            score = self._score_relevance(query_tokens, chunk_tokens)
            if score > 0:
                results.append((chunk, score))

        # Sort by score descending
        results.sort(key=lambda x: x[1], reverse=True)
        return results[:top_k]

    def list_documents(self) -> List[Dict]:
        """List all indexed documents."""
        docs = {}
        for chunk in self.chunks:
            if chunk.source_file not in docs:
                docs[chunk.source_file] = {
                    'path': chunk.source_file,
                    'chunks': 0,
                    'metadata': chunk.metadata
                }
            docs[chunk.source_file]['chunks'] += 1
        return list(docs.values())

    def delete_document(self, file_path: str) -> int:
        """Remove a document from the store."""
        file_path = str(Path(file_path).resolve())
        original_count = len(self.chunks)
        self.chunks = [c for c in self.chunks if c.source_file != file_path]
        removed = original_count - len(self.chunks)
        if removed > 0:
            self._save()
        return removed


class ChromaVectorStore:
    """
    ChromaDB-based vector store with proper embeddings.
    Requires: pip install chromadb
    """

    def __init__(self, store_path: str, collection_name: str = "research"):
        if not HAS_CHROMA:
            raise ImportError("ChromaDB not installed. Run: pip install chromadb")

        self.store_path = Path(store_path)
        self.store_path.mkdir(parents=True, exist_ok=True)

        self.client = chromadb.PersistentClient(
            path=str(self.store_path),
            settings=Settings(anonymized_telemetry=False)
        )
        self.collection = self.client.get_or_create_collection(
            name=collection_name,
            metadata={"hnsw:space": "cosine"}
        )

    def add_document(self, file_path: str, chunk_size: int = 500, overlap: int = 50) -> int:
        """Add document to ChromaDB."""
        file_path = Path(file_path).resolve()

        if not file_path.exists():
            raise FileNotFoundError(f"File not found: {file_path}")

        with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
            content = f.read()

        # Extract metadata
        metadata = {"source": str(file_path)}
        if content.startswith('---'):
            try:
                end_idx = content.index('---', 3)
                frontmatter = content[3:end_idx]
                content = content[end_idx + 3:]
                for line in frontmatter.strip().split('\n'):
                    if ':' in line:
                        key, value = line.split(':', 1)
                        metadata[key.strip()] = value.strip()
            except ValueError:
                pass

        # Delete existing chunks
        try:
            existing = self.collection.get(where={"source": str(file_path)})
            if existing['ids']:
                self.collection.delete(ids=existing['ids'])
        except Exception:
            pass

        # Chunk and add
        words = content.split()
        chunks_added = 0
        ids, documents, metadatas = [], [], []

        for i in range(0, len(words), chunk_size - overlap):
            chunk_words = words[i:i + chunk_size]
            if len(chunk_words) < 50:
                continue

            chunk_text = ' '.join(chunk_words)
            chunk_id = hashlib.md5(f"{file_path}:{i}".encode()).hexdigest()[:12]

            ids.append(chunk_id)
            documents.append(chunk_text)
            metadatas.append({**metadata, "chunk_index": chunks_added})
            chunks_added += 1

        if ids:
            self.collection.add(ids=ids, documents=documents, metadatas=metadatas)

        return chunks_added

    def query(self, query: str, top_k: int = 5) -> List[Dict]:
        """Query ChromaDB."""
        results = self.collection.query(query_texts=[query], n_results=top_k)

        output = []
        for i, doc_id in enumerate(results['ids'][0]):
            output.append({
                'id': doc_id,
                'content': results['documents'][0][i],
                'metadata': results['metadatas'][0][i],
                'distance': results['distances'][0][i] if 'distances' in results else None
            })
        return output

    def list_documents(self) -> List[Dict]:
        """List indexed documents."""
        all_data = self.collection.get()
        docs = {}
        for i, meta in enumerate(all_data['metadatas']):
            source = meta.get('source', 'unknown')
            if source not in docs:
                docs[source] = {'path': source, 'chunks': 0, 'metadata': meta}
            docs[source]['chunks'] += 1
        return list(docs.values())


def get_store(topic: str, use_chroma: bool = True) -> object:
    """Get appropriate vector store for a topic."""
    base_path = Path(f"RESEARCH/{topic}/knowledge_store")

    if use_chroma and HAS_CHROMA:
        return ChromaVectorStore(str(base_path), collection_name=topic)
    else:
        return SimpleVectorStore(str(base_path))


def main():
    if len(sys.argv) < 2:
        print(json.dumps({
            "error": "Usage: python3 vector_store.py <command> [args]",
            "commands": ["index <file>", "query <text> --topic <name>", "list --topic <name>"],
            "status": "failed"
        }))
        sys.exit(1)

    command = sys.argv[1]

    # Parse --topic argument
    topic = "default"
    args = sys.argv[2:]
    if "--topic" in args:
        idx = args.index("--topic")
        if idx + 1 < len(args):
            topic = args[idx + 1]
            args = args[:idx] + args[idx + 2:]

    store = get_store(topic, use_chroma=HAS_CHROMA)

    if command == "index":
        if not args:
            print(json.dumps({"error": "Missing file path", "status": "failed"}))
            sys.exit(1)
        file_path = args[0]
        try:
            chunks = store.add_document(file_path)
            print(json.dumps({
                "status": "success",
                "file": file_path,
                "chunks_indexed": chunks,
                "store_type": "chroma" if HAS_CHROMA else "simple"
            }))
        except Exception as e:
            print(json.dumps({"error": str(e), "status": "failed"}))

    elif command == "query":
        if not args:
            print(json.dumps({"error": "Missing query text", "status": "failed"}))
            sys.exit(1)
        query_text = ' '.join(args)
        results = store.query(query_text, top_k=5)

        if isinstance(store, SimpleVectorStore):
            output = [{
                'id': r[0].id,
                'content': r[0].content[:500] + '...' if len(r[0].content) > 500 else r[0].content,
                'source': r[0].source_file,
                'score': round(r[1], 4)
            } for r in results]
        else:
            output = results

        print(json.dumps({"status": "success", "results": output}, indent=2))

    elif command == "list":
        docs = store.list_documents()
        print(json.dumps({"status": "success", "documents": docs}, indent=2))

    else:
        print(json.dumps({"error": f"Unknown command: {command}", "status": "failed"}))
        sys.exit(1)


if __name__ == "__main__":
    main()
</file>

<file path="ARCHITECTURE.md">
# Architecture

This document describes the technical architecture of the Claude Code Deep Research Agent framework.

## System Overview

The framework is built on Claude Code's Skills and Commands system, providing a modular architecture for conducting sophisticated multi-agent research.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        User Interface                        â”‚
â”‚                    (Commands: /deep-research)                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      Skills Layer                            â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚
â”‚  â”‚   Question   â”‚  â”‚   Research   â”‚  â”‚     GoT      â”‚     â”‚
â”‚  â”‚   Refiner    â”‚  â”‚   Executor   â”‚  â”‚  Controller  â”‚     â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                        â”‚
â”‚  â”‚  Citation    â”‚  â”‚ Synthesizer  â”‚                        â”‚
â”‚  â”‚  Validator   â”‚  â”‚              â”‚                        â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Multi-Agent Layer                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”           â”‚
â”‚  â”‚ Web Search â”‚  â”‚ Academic   â”‚  â”‚   Cross-   â”‚           â”‚
â”‚  â”‚  Agents    â”‚  â”‚  Agents    â”‚  â”‚ Reference  â”‚           â”‚
â”‚  â”‚   (3-5)    â”‚  â”‚   (1-2)    â”‚  â”‚  Agent (1) â”‚           â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      Tools Layer                             â”‚
â”‚  WebSearch â”‚ WebFetch â”‚ Task â”‚ Read/Write â”‚ TodoWrite      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Directory Structure

```
.
â”œâ”€â”€ .claude/
â”‚   â”œâ”€â”€ commands/              # User-facing command shortcuts
â”‚   â”‚   â”œâ”€â”€ deep-research.md
â”‚   â”‚   â”œâ”€â”€ refine-question.md
â”‚   â”‚   â”œâ”€â”€ plan-research.md
â”‚   â”‚   â”œâ”€â”€ synthesize-findings.md
â”‚   â”‚   â””â”€â”€ validate-citations.md
â”‚   â”‚
â”‚   â”œâ”€â”€ skills/                # Modular capabilities
â”‚   â”‚   â”œâ”€â”€ question-refiner/
â”‚   â”‚   â”‚   â”œâ”€â”€ SKILL.md
â”‚   â”‚   â”‚   â”œâ”€â”€ instructions.md
â”‚   â”‚   â”‚   â””â”€â”€ examples.md
â”‚   â”‚   â”œâ”€â”€ research-executor/
â”‚   â”‚   â”œâ”€â”€ got-controller/
â”‚   â”‚   â”œâ”€â”€ citation-validator/
â”‚   â”‚   â””â”€â”€ synthesizer/
â”‚   â”‚
â”‚   â””â”€â”€ settings.local.json    # Tool permissions
â”‚
â”œâ”€â”€ RESEARCH/                  # Research outputs
â”‚   â””â”€â”€ [topic_name]/
â”‚       â”œâ”€â”€ README.md
â”‚       â”œâ”€â”€ executive_summary.md
â”‚       â”œâ”€â”€ full_report.md
â”‚       â”œâ”€â”€ data/
â”‚       â”œâ”€â”€ visuals/
â”‚       â”œâ”€â”€ sources/
â”‚       â”œâ”€â”€ research_notes/
â”‚       â””â”€â”€ appendices/
â”‚
â”œâ”€â”€ docs/
â”‚   â””â”€â”€ reference/
â”‚       â””â”€â”€ skills-guide.md    # General skills reference
â”‚
â”œâ”€â”€ CLAUDE.md                  # Claude Code quick reference
â”œâ”€â”€ ARCHITECTURE.md            # This file
â”œâ”€â”€ RESEARCH_METHODOLOGY.md    # Research implementation guide
â””â”€â”€ README.md                  # User-facing documentation
```

## Skills System

### What are Skills?

Skills are modular, reusable capabilities that Claude Code can execute. Each skill is a self-contained unit with:

1. **SKILL.md** - Metadata and description (YAML frontmatter)
2. **instructions.md** - Detailed implementation guidance
3. **examples.md** - Usage examples and patterns

### Skill Structure

```yaml
# SKILL.md frontmatter
name: skill-name
description: Brief description
version: 1.0.0
author: Your Name
tags: [research, analysis]
```

### Available Skills

#### 1. question-refiner

**Purpose**: Transform vague research questions into structured prompts

**Input**: Raw user question
**Output**: Structured research prompt with:

- Clear research objectives
- Scope boundaries
- Expected deliverables
- Success criteria

**Location**: `.claude/skills/question-refiner/`

#### 2. research-executor

**Purpose**: Execute complete 7-phase research workflow

**Phases**:

1. Question Scoping
2. Retrieval Planning
3. Iterative Querying
4. Source Triangulation
5. Knowledge Synthesis
6. Quality Assurance
7. Output & Packaging

**Location**: `.claude/skills/research-executor/`

#### 3. got-controller

**Purpose**: Manage Graph of Thoughts for complex research

**Operations**:

- Generate(k): Create k parallel research paths
- Aggregate(k): Merge k findings
- Refine(1): Improve existing finding
- Score: Rate quality (0-10)
- KeepBestN(n): Prune to top n nodes

**Location**: `.claude/skills/got-controller/`

#### 4. citation-validator

**Purpose**: Verify citation accuracy and source quality

**Checks**:

- Citation completeness
- Source accessibility
- Quality ratings (A-E scale)
- Cross-reference validation

**Location**: `.claude/skills/citation-validator/`

#### 5. synthesizer

**Purpose**: Combine findings from multiple agents

**Process**:

- Collect agent outputs
- Identify overlaps and contradictions
- Resolve conflicts
- Create unified narrative
- Maintain source attribution

**Location**: `.claude/skills/synthesizer/`

## Commands System

### What are Commands?

Commands are user-facing shortcuts that invoke skills with predefined parameters. They provide a simple interface for complex operations.

### Command Structure

```markdown
# Command: /command-name

## Description
Brief description of what this command does

## Usage
/command-name [arguments]

## Examples
/command-name example argument
```

### Available Commands

| Command | Invokes Skill | Description |
|---------|---------------|-------------|
| `/deep-research` | research-executor | Full 7-phase workflow |
| `/refine-question` | question-refiner | Question transformation |
| `/plan-research` | research-executor (phase 2) | Create execution plan |
| `/synthesize-findings` | synthesizer | Combine agent outputs |
| `/validate-citations` | citation-validator | Verify citations |

## Multi-Agent Architecture

### Agent Types

#### Web Research Agents (3-5 agents)

**Focus**: Current information, trends, news
**Tools**: WebSearch, WebFetch
**Output**: Structured summaries with URLs

#### Academic/Technical Agents (1-2 agents)

**Focus**: Research papers, specifications
**Tools**: WebSearch (academic sources), WebFetch
**Output**: Technical analysis with citations

#### Cross-Reference Agent (1 agent)

**Focus**: Fact-checking, verification
**Tools**: WebSearch, WebFetch
**Output**: Confidence ratings for claims

### Agent Deployment

Agents are deployed in parallel using multiple Task tool calls in a single response:

```
Task 1: Web Research Agent - Current trends
Task 2: Web Research Agent - Market analysis
Task 3: Academic Agent - Technical foundations
Task 4: Cross-Reference Agent - Fact verification
```

### Agent Communication

Agents work independently but share findings through:

1. Structured output format
2. Common citation standards
3. Centralized result aggregation
4. Conflict resolution protocol

## Graph of Thoughts Implementation

### Graph Structure

```json
{
  "nodes": {
    "n1": {
      "text": "Research finding",
      "score": 8.5,
      "type": "root|generate|aggregate|refine",
      "depth": 0,
      "sources": ["url1", "url2"]
    }
  },
  "edges": [
    {"from": "n1", "to": "n2", "operation": "Generate"}
  ],
  "frontier": ["n2", "n3"],
  "budget": {
    "tokens_used": 15000,
    "max_tokens": 50000
  }
}
```

### Transformation Operations

**Generate(k)**

- Creates k new research paths from parent
- Each path explores different angle
- Returns k nodes with scores

**Aggregate(k)**

- Merges k nodes into single synthesis
- Resolves contradictions
- Preserves all citations
- Returns 1 node with higher score

**Refine(1)**

- Improves existing node quality
- Fact-checks claims
- Enhances clarity
- Returns refined node

**Score**

- Evaluates node quality (0-10)
- Based on: citations, accuracy, completeness, coherence
- Guides exploration strategy

**KeepBestN(n)**

- Prunes graph to top n nodes per depth
- Manages token budget
- Focuses on high-quality paths

### Graph Traversal Strategy

```
Depth 0-2: Aggressive Generate(3) - Explore search space
Depth 2-3: Mixed Generate + Refine - Balance exploration/exploitation
Depth 3-4: Aggregate + Refine - Synthesize best paths
Termination: max_score > 9 OR depth > 4
```

## Tool Permissions

Configured in `.claude/settings.local.json`:

```json
{
  "tools": {
    "WebSearch": {
      "enabled": true,
      "description": "General web searches"
    },
    "WebFetch": {
      "enabled": true,
      "description": "Extract content from URLs"
    },
    "Task": {
      "enabled": true,
      "description": "Deploy autonomous agents"
    },
    "TodoWrite": {
      "enabled": true,
      "description": "Track research progress"
    },
    "Read": {
      "enabled": true,
      "description": "Read files"
    },
    "Write": {
      "enabled": true,
      "description": "Write files"
    }
  }
}
```

## Output Management

### File Organization

All research outputs go to `RESEARCH/[topic_name]/`:

```
RESEARCH/[topic_name]/
â”œâ”€â”€ README.md                    # Navigation and overview
â”œâ”€â”€ executive_summary.md         # 1-2 page key findings
â”œâ”€â”€ full_report.md               # Complete analysis (20-50 pages)
â”œâ”€â”€ data/
â”‚   â””â”€â”€ statistics.md            # Key numbers and facts
â”œâ”€â”€ visuals/
â”‚   â””â”€â”€ descriptions.md          # Chart/graph descriptions
â”œâ”€â”€ sources/
â”‚   â”œâ”€â”€ bibliography.md          # Complete citations
â”‚   â””â”€â”€ source_quality_table.md  # A-E quality ratings
â”œâ”€â”€ research_notes/
â”‚   â””â”€â”€ agent_findings_summary.md # Raw agent outputs
â””â”€â”€ appendices/
    â”œâ”€â”€ methodology.md           # Research methods used
    â””â”€â”€ limitations.md           # Unknowns and gaps
```

### Document Splitting Strategy

To avoid context limits:

- Break reports into sections (< 10,000 words each)
- Separate data files from narrative
- Keep agent outputs in research_notes/
- Link documents with cross-references

## Citation System

### Citation Format

**Inline**: `(Author, Year, p. XX)`
**Bibliography**: Full citation with URL/DOI

### Source Quality Ratings

- **A**: Peer-reviewed RCTs, systematic reviews, meta-analyses
- **B**: Cohort studies, clinical guidelines, reputable analysts
- **C**: Expert opinion, case reports, mechanistic studies
- **D**: Preprints, preliminary research, blogs
- **E**: Anecdotal, theoretical, speculative

### Validation Process

1. Check citation completeness
2. Verify source accessibility
3. Cross-reference claims
4. Rate source quality
5. Flag unreliable sources

## Extending the Framework

### Adding New Skills

1. Create skill directory in `.claude/skills/`
2. Add SKILL.md with YAML frontmatter
3. Write instructions.md with implementation details
4. Provide examples.md with usage patterns
5. Test with diverse research topics
6. Update documentation

### Adding New Commands

1. Create command file in `.claude/commands/`
2. Define command syntax and arguments
3. Map to appropriate skill(s)
4. Add usage examples
5. Update README.md

### Adding New Agent Types

1. Define agent role and focus
2. Specify required tools
3. Create agent prompt template
4. Define output format
5. Integrate with synthesizer
6. Test with multi-agent deployment

## Performance Considerations

### Token Budget Management

- Track tokens used per agent
- Set max_tokens limit (default: 50,000)
- Prune low-scoring branches early
- Cache intermediate results

### Parallel Execution

- Deploy agents in single response
- Use multiple Task calls
- Avoid sequential dependencies
- Aggregate results efficiently

### Quality vs Speed Tradeoffs

- Quick research: 3-4 agents, depth 2
- Standard research: 5-6 agents, depth 3
- Comprehensive research: 6-8 agents, depth 4

## Error Handling

### Common Issues

1. **Agent timeout**: Reduce scope or split task
2. **Citation missing**: Flag for manual review
3. **Source inaccessible**: Find alternative source
4. **Contradictory findings**: Document in report
5. **Token limit exceeded**: Split into smaller tasks

### Recovery Strategies

- Save intermediate results
- Resume from last checkpoint
- Retry failed operations
- Escalate to user when blocked

## Security Considerations

### Data Privacy

- No persistent storage of user data
- Research outputs saved locally only
- No external API calls (except web search)

### Source Validation

- Verify URL authenticity
- Check for malicious content
- Validate SSL certificates
- Flag suspicious sources

## Future Enhancements

### Planned Features

- [ ] Visual graph explorer for GoT
- [ ] Interactive research dashboard
- [ ] Real-time collaboration support
- [ ] Custom agent templates
- [ ] Advanced citation management
- [ ] Multi-language support

### Research Areas

- Improved scoring functions
- Better conflict resolution
- Automated fact-checking
- Source credibility prediction
- Dynamic agent allocation

---

**For implementation details, see [RESEARCH_METHODOLOGY.md](RESEARCH_METHODOLOGY.md)**
</file>

<file path="CLAUDE.md">
# CLAUDE.md

This file provides guidance to Claude Code (claude.ai/code) when working with this research framework.

## Project Overview

This is a **Claude Code Deep Research Agent** framework that implements sophisticated multi-agent research through:

- **Graph of Thoughts (GoT)** - Intelligent research path management with graph-based reasoning
- **7-Phase Research Process** - Structured methodology from question scoping to final output
- **Multi-Agent Architecture** - Parallel research agents (3-8 agents) with specialized roles
- **Citation Validation** - A-E source quality ratings with chain-of-verification

## Quick Commands Reference

### Primary Command

```bash
/deep-research [research topic]
```

Executes the complete 7-phase workflow:

1. Question refinement (asks clarifying questions)
2. Research planning with subtopic breakdown
3. Multi-agent parallel deployment
4. Source triangulation and cross-validation
5. Knowledge synthesis with citations
6. Quality assurance and validation
7. Output to `RESEARCH/[topic]/` directory

### Step-by-Step Commands

```bash
/refine-question [raw question]      # Transform into structured research prompt
/plan-research [structured prompt]   # Create detailed execution plan
/synthesize-findings [directory]     # Combine agent outputs
/validate-citations [file]           # Verify citation quality and accuracy
```

## Core Concepts

### Graph of Thoughts Operations

| Operation | Purpose | Example |
|-----------|---------|---------|
| **Generate(k)** | Spawn k parallel research paths | Generate(4) â†’ 4 diverse research angles |
| **Aggregate(k)** | Merge k findings into synthesis | Aggregate(3) â†’ 1 comprehensive report |
| **Refine(1)** | Improve existing finding | Refine(node_5) â†’ Enhanced quality |
| **Score** | Rate quality (0-10) | Based on citations, accuracy, completeness |
| **KeepBestN(n)** | Prune to top n nodes | KeepBestN(3) â†’ Retain best 3 paths |

### Multi-Agent Deployment

When executing research, deploy agents in a single response with multiple Task calls:

```
Phase 3: Iterative Querying (Parallel Execution)
â”œâ”€â”€ Web Research Agents (3-5): Current information, trends, news
â”œâ”€â”€ Academic/Technical Agents (1-2): Papers, specifications, methodologies
â””â”€â”€ Cross-Reference Agent (1): Fact-checking, verification
```

Each agent receives:

- Clear research focus description
- Specific search queries
- Expected output format
- Citation requirements

## Research Output Structure

All outputs go to `RESEARCH/[topic_name]/`:

```
RESEARCH/[topic_name]/
â”œâ”€â”€ README.md                    # Overview and navigation
â”œâ”€â”€ executive_summary.md         # 1-2 page key findings
â”œâ”€â”€ full_report.md               # Complete analysis (20-50 pages)
â”œâ”€â”€ data/
â”‚   â””â”€â”€ statistics.md            # Key numbers, facts
â”œâ”€â”€ visuals/
â”‚   â””â”€â”€ descriptions.md          # Chart/graph descriptions
â”œâ”€â”€ sources/
â”‚   â”œâ”€â”€ bibliography.md          # Complete citations
â”‚   â””â”€â”€ source_quality_table.md  # A-E quality ratings
â”œâ”€â”€ research_notes/
â”‚   â””â”€â”€ agent_findings_summary.md # Raw agent outputs
â””â”€â”€ appendices/
    â”œâ”€â”€ methodology.md           # Research methods used
    â””â”€â”€ limitations.md           # Unknowns, gaps
```

## Citation Requirements

**Every factual claim must include:**

1. Author/Organization name
2. Publication date
3. Source title
4. Direct URL/DOI
5. Page numbers (if applicable)

**Source Quality Ratings:**

- **A**: Peer-reviewed RCTs, systematic reviews, meta-analyses
- **B**: Cohort studies, clinical guidelines, reputable analysts
- **C**: Expert opinion, case reports, mechanistic studies
- **D**: Preprints, preliminary research, blogs
- **E**: Anecdotal, theoretical, speculative

**Never make claims without sources** - state "Source needed" if uncertain.

## Key Constraints

### Output Management

- All research outputs go in `RESEARCH/[topic]/` directories
- Break large documents into smaller files to avoid context limits
- Use TodoWrite to track task completion throughout execution

### Agent Deployment

- Use parallel agent deployment (single response, multiple Task calls)
- Deploy 3-8 agents depending on research scope
- Each agent should have distinct, non-overlapping focus

### Quality Standards

- Validate citations before finalizing reports
- Cross-reference claims across multiple sources
- Apply Chain-of-Verification to prevent hallucinations
- Use Graph of Thoughts to optimize research paths

## Skills System

Skills are located in `.claude/skills/`:

| Skill | Purpose |
|-------|---------|
| `question-refiner` | Transform vague questions into structured prompts |
| `research-executor` | Execute full 7-phase research process |
| `got-controller` | Manage Graph of Thoughts for complex research |
| `citation-validator` | Verify citation accuracy and source quality |
| `synthesizer` | Combine findings from multiple agents |

Each skill has:

- `SKILL.md`: YAML frontmatter + description
- `instructions.md`: Detailed implementation guidance
- `examples.md`: Usage examples

## Tool Permissions

Configured in `.claude/settings.local.json`:

- **WebSearch**: General web searches
- **WebFetch**: Extract content from specific URLs
- **Task**: Deploy autonomous research agents
- **TodoWrite**: Track research progress
- **Read/Write**: Manage research documents

## User Interaction Protocol

When user requests deep research:

1. **Ask clarifying questions** about:
   - Specific focus areas
   - Output format requirements
   - Geographic and time scope
   - Target audience
   - Special requirements

2. **Create research plan** showing:
   - Subtopic breakdown
   - Agent deployment strategy
   - Expected output structure

3. **Get user approval** before executing

4. **Execute research** with parallel agents

5. **Deliver structured output** to RESEARCH/ directory

## Documentation

For detailed information, refer to:

- **[ARCHITECTURE.md](ARCHITECTURE.md)** - System design, skills structure, technical details
- **[RESEARCH_METHODOLOGY.md](RESEARCH_METHODOLOGY.md)** - Complete 7-phase process, GoT implementation, agent templates
- **[README.md](README.md)** - Quick start guide for users

## Important Notes

- Always use TodoWrite to track tasks and show progress
- Deploy agents in parallel when possible (single response, multiple Task calls)
- Validate all citations before finalizing reports
- Break large reports into multiple smaller files
- Maintain graph state when using GoT Controller
- Cross-validate findings across multiple sources

---

**This is a quick reference. For complete implementation details, see [RESEARCH_METHODOLOGY.md](RESEARCH_METHODOLOGY.md).**
</file>

<file path="README.md">
# Claude Code Deep Research Agent

> A sophisticated multi-agent research framework that replicates OpenAI's Deep Research and Google Gemini's Deep Research capabilities using Claude Code.

## What is this?

A complete research automation system that uses Claude Code's Skills and Commands to conduct comprehensive, citation-backed research through **Graph of Thoughts (GoT)** reasoning and parallel multi-agent deployment.

## Quick Start

### Installation

```bash
git clone <repository-url>
cd template
```

The Skills and Commands are pre-configured in `.claude/` directory. No additional setup required.

### Basic Usage

Start deep research with a single command:

```bash
/deep-research [your research topic]
```

**Example:**

```bash
/deep-research AI applications in clinical diagnosis
```

This will:

1. Ask clarifying questions about your research needs
2. Create a structured research plan
3. Deploy 3-8 parallel research agents
4. Cross-validate findings across sources
5. Generate a comprehensive report with citations
6. Output to `RESEARCH/[topic]/` directory

## Core Features

- **Graph of Thoughts Framework** - Intelligent research path optimization
- **7-Phase Research Process** - Structured methodology from question to report
- **Multi-Agent Architecture** - Parallel agents with specialized roles
- **Citation Validation** - A-E source quality ratings with verification
- **Auto-Generated Reports** - Executive summary, full report, bibliography

## Available Commands

| Command | Description |
|---------|-------------|
| `/deep-research [topic]` | Execute complete research workflow |
| `/refine-question [question]` | Refine raw question into structured prompt |
| `/plan-research [prompt]` | Create detailed execution plan |
| `/synthesize-findings [dir]` | Combine research outputs from multiple agents |
| `/validate-citations [file]` | Verify citation accuracy and quality |

## Research Output

Each research creates a structured output:

```
RESEARCH/[topic_name]/
â”œâ”€â”€ README.md                    # Navigation guide
â”œâ”€â”€ executive_summary.md         # Key findings (1-2 pages)
â”œâ”€â”€ full_report.md               # Complete analysis (20-50 pages)
â”œâ”€â”€ data/                        # Statistics and raw data
â”œâ”€â”€ visuals/                     # Chart descriptions
â”œâ”€â”€ sources/                     # Bibliography and quality ratings
â”œâ”€â”€ research_notes/              # Agent outputs
â””â”€â”€ appendices/                  # Methodology and limitations
```

## Documentation

| Document | Audience | Description |
|----------|----------|-------------|
| [CLAUDE.md](CLAUDE.md) | Claude Code | Quick reference for AI assistant |
| [ARCHITECTURE.md](ARCHITECTURE.md) | Developers | System design and skills structure |
| [RESEARCH_METHODOLOGY.md](RESEARCH_METHODOLOGY.md) | Claude Code | Complete research implementation guide |
| [docs/reference/](docs/reference/) | Reference | Additional guides and references |

## Examples

### Market Research

```bash
/deep-research AI in healthcare market, focus on clinical diagnosis,
             comprehensive report, global scope, 2022-2024 data
```

### Technical Assessment

```bash
/deep-research WebAssembly vs JavaScript performance benchmarks
```

### Academic Literature Review

```bash
/deep-research Transformer architectures in AI,
             peer-reviewed sources only, 2017-present
```

## Performance

- **Quick research** (narrow topic): 10-15 minutes
- **Standard research** (moderate scope): 20-30 minutes
- **Comprehensive research** (broad scope): 30-60 minutes
- **Academic literature review**: 45-90 minutes

## Citation Standards

Every factual claim includes:

- Author/Organization name
- Publication date
- Source title
- Direct URL/DOI
- Source quality rating (A-E scale)

## Technology

- **Graph of Thoughts** - Graph-based reasoning for research optimization
- **Multi-Agent System** - Parallel task execution with Claude Code Task tool
- **Skills System** - Modular capabilities in `.claude/skills/`
- **Commands System** - User-facing shortcuts in `.claude/commands/`

## Contributing

To add new skills or improvements:

1. Follow the skill structure in `.claude/skills/`
2. Include `SKILL.md` with clear YAML frontmatter
3. Test with diverse research topics
4. Update documentation

## License

This project is provided as-is for educational and research purposes.

## Acknowledgments

- Graph of Thoughts framework inspired by [SPCL, ETH ZÃ¼rich](https://github.com/spcl/graph-of-thoughts)
- Built with [Claude Code](https://claude.ai/code)
- 7-Phase Research Process based on deep research best practices

---

**Need help?** See [ARCHITECTURE.md](ARCHITECTURE.md) for technical details or [RESEARCH_METHODOLOGY.md](RESEARCH_METHODOLOGY.md) for research implementation.
</file>

<file path="RESEARCH_METHODOLOGY.md">
# Deep Research Methodology with Graph of Thoughts

## Overview

This document provides comprehensive methodology for conducting AI-driven deep research using the Graph of Thoughts (GoT) framework. This approach autonomously conducts multi-step research through iterative searching, reading, analyzing, and synthesizing information with explicit citations.

**Key Features:**

- 7-phase structured research process
- Graph of Thoughts for complex reasoning and optimization
- Multi-agent parallel deployment (3-8 agents)
- Rigorous citation standards with A-E quality ratings
- Chain-of-Verification to prevent hallucinations
- Self-contained implementation using Task agents

**Important Notes:**

- All research outputs saved in `RESEARCH/[topic_name]/` directory
- Break down large documents into smaller files to avoid context limitations
- Use TodoWrite to track task completion throughout execution
- Maintain graph state when using GoT Controller

---

## Understanding Graph of Thoughts

Graph of Thoughts is a reasoning framework where:

- **Thoughts = Nodes**: Each research finding or synthesis is a node with a unique ID
- **Edges = Dependencies**: Connect parent thoughts to child thoughts
- **Transformations**: Operations that create (Generate), merge (Aggregate), or improve (Refine) thoughts
- **Scoring**: Every thought is evaluated 0-10 for quality based on citation density, source credibility, claim verification, comprehensiveness, and logical coherence
- **Pruning**: Low-scoring branches are abandoned using KeepBestN(n) operations
- **Frontier**: Active nodes available for expansion

The system explores multiple research paths in parallel, scores them, and finds optimal solutions through graph traversal.

### GoT Transformation Operations

| Operation | Purpose | Example |
|-----------|---------|---------|
| **Generate(k)** | Create k new thoughts from a parent | Generate(3) â†’ 3 diverse research angles |
| **Aggregate(k)** | Merge k thoughts into one stronger thought | Aggregate(3) â†’ 1 comprehensive synthesis |
| **Refine(1)** | Improve a thought without adding new content | Refine(node_5) â†’ Enhanced clarity and depth |
| **Score** | Evaluate thought quality (0-10) | Based on citations, accuracy, completeness |
| **KeepBestN(n)** | Prune to keep only top n nodes per level | KeepBestN(5) â†’ Retain best 5 paths |

### Graph State Structure

```json
{
  "nodes": {
    "n1": {
      "text": "Research finding with citations",
      "score": 8.5,
      "type": "root|generated|aggregated|refined",
      "depth": 0,
      "sources": ["url1", "url2"]
    }
  },
  "edges": [
    {"from": "n1", "to": "n2", "operation": "Generate"}
  ],
  "frontier": ["n2", "n3"],
  "budget": {
    "tokens_used": 15000,
    "max_tokens": 50000
  }
}
```

### Graph Traversal Strategy

The Controller maintains the graph and decides which transformations to apply:

1. **Early Depth (0-2)**: Aggressive Generate(3) to explore search space
2. **Mid Depth (2-3)**: Mix of Generate for promising paths + Refine for weak nodes
3. **Late Depth (3-4)**: Aggregate best branches + final Refine
4. **Pruning**: Keep only top 5 nodes per depth level
5. **Termination**: When best node scores 9+ or depth exceeds 4

---

## The 7-Phase Deep Research Process

### Phase 1: Question Scoping

**Objective:** Transform vague questions into structured research prompts

**Activities:**

- Clarify the research question with the user through structured dialogue
- Define output format and success criteria
- Identify constraints, scope boundaries, and desired tone
- Determine target audience and technical depth
- Create unambiguous query with clear parameters

**User Interaction - Ask clarifying questions about:**

- Specific focus areas
- Output format requirements (report, presentation, analysis)
- Geographic and time scope
- Target audience (technical team, executives, general)
- Special requirements (data, visualizations, comparisons)

**Deliverable:** Structured research prompt with clear objectives

---

### Phase 2: Retrieval Planning

**Objective:** Create a comprehensive research execution plan

**Activities:**

- Break main question into 3-5 subtopics
- Generate specific search queries for each subtopic
- Select appropriate data sources (academic, industry, news)
- Plan multi-agent deployment strategy (3-8 agents)
- Use GoT to model the research as a graph of operations
- Create research plan for user approval

**Subtopic Breakdown Template:**

1. Current state and trends
2. Key challenges and limitations
3. Future developments and predictions
4. Case studies and real-world applications
5. Expert opinions and industry perspectives

**Agent Deployment Planning:**

- 3-5 Web Research Agents: Current information, trends, news
- 1-2 Academic/Technical Agents: Papers, specifications, methodologies
- 1 Cross-Reference Agent: Fact-checking and verification

**Deliverable:** Detailed research plan with agent deployment strategy for user approval

---

### Phase 3: Iterative Querying

**Objective:** Execute systematic information gathering with parallel agents

**Activities:**

- Deploy 3-8 specialized research agents in parallel
- Execute searches systematically across multiple sources
- Navigate and extract relevant information
- Formulate new queries based on findings (ReAct pattern)
- Apply GoT operations for complex reasoning
- Use multiple search modalities (WebSearch, WebFetch, Puppeteer)

**Tools:**

- **WebSearch**: General web searches for finding relevant sources
- **WebFetch**: Extract and analyze content from specific URLs
- **mcp__puppeteer__**: Browser automation for JavaScript-heavy sites
- **Task**: Deploy autonomous agents for multi-step operations

**Agent Types:**

- **Web Research Agents (3-5)**: Current information, trends, news, real-world data
- **Academic/Technical Agents (1-2)**: Research papers, technical specifications, methodologies
- **Cross-Reference Agent (1)**: Fact-checking, verification, cross-validation

**Deliverable:** Raw research findings from all agents with source URLs and confidence ratings

---

### Phase 4: Source Triangulation

**Objective:** Validate and cross-reference all findings

**Activities:**

- Compare findings across multiple sources
- Validate claims with cross-references (minimum 2+ sources for critical claims)
- Handle inconsistencies and contradictions
- Assess source credibility using A-E rating system
- Use GoT scoring functions to evaluate information quality
- Apply Chain-of-Verification techniques

**Validation Protocol:**

1. **Primary Sources Only** - Link to original research, not secondary reporting
2. **Archive Links** - For time-sensitive content, include archive.org links
3. **Multiple Confirmations** - Critical claims need 2+ independent sources
4. **Conflicting Data** - Note when sources disagree and explain discrepancies
5. **Source Quality Ratings** - Apply A-E scale to every source

**Deliverable:** Validated findings with confidence ratings and source quality assessments

---

### Phase 5: Knowledge Synthesis

**Objective:** Combine findings into coherent narrative

**Activities:**

- Structure content logically with clear sections
- Write comprehensive sections with proper flow
- Include inline citations for every factual claim
- Add data visualizations when relevant
- Use GoT Aggregate operations to merge findings
- Apply Chain-of-Density for information compression

**Synthesis Process:**

1. Collect all agent findings
2. Identify overlaps and contradictions
3. Resolve conflicts with evidence
4. Create unified narrative
5. Maintain source attribution from each agent

**Chain-of-Density Approach:**

1. First pass: Extract key points (low density)
2. Second pass: Add supporting details and context
3. Third pass: Compress while preserving all critical information
4. Final pass: Maximum density with all essential facts and citations

**Deliverable:** Draft research report with inline citations

---

### Phase 6: Quality Assurance

**Objective:** Ensure accuracy and completeness

**Activities:**

- Check for hallucinations and unsupported claims
- Verify all citations match content
- Ensure completeness and clarity
- Apply Chain-of-Verification techniques
- Use GoT ground truth operations for validation
- Run citation validator on final document

**Quality Checklist:**

- [ ] Every claim has a verifiable source
- [ ] Multiple sources corroborate key findings
- [ ] Contradictions are acknowledged and explained
- [ ] Sources are recent and authoritative
- [ ] No hallucinations or unsupported claims
- [ ] Clear logical flow from evidence to conclusions
- [ ] Proper citation format throughout

**Deliverable:** Quality-assured research report

---

### Phase 7: Output & Packaging

**Objective:** Format and deliver final research

**Activities:**

- Format for optimal readability
- Create executive summary (1-2 pages)
- Generate proper bibliography with source quality ratings
- Organize into folder structure
- Export in requested format
- Include methodology and limitations documentation

**Output Structure:**

```
RESEARCH/[topic_name]/
â”œâ”€â”€ README.md                    # Overview and navigation guide
â”œâ”€â”€ executive_summary.md         # 1-2 page key findings
â”œâ”€â”€ full_report.md               # Complete analysis (20-50 pages)
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw_data.csv
â”‚   â”œâ”€â”€ processed_data.json
â”‚   â””â”€â”€ statistics_summary.md
â”œâ”€â”€ visuals/
â”‚   â”œâ”€â”€ charts/
â”‚   â”œâ”€â”€ graphs/
â”‚   â””â”€â”€ descriptions.md
â”œâ”€â”€ sources/
â”‚   â”œâ”€â”€ bibliography.md
â”‚   â”œâ”€â”€ source_quality_table.md  # A-E ratings
â”‚   â””â”€â”€ screenshots/
â”œâ”€â”€ research_notes/
â”‚   â”œâ”€â”€ agent_1_findings.md
â”‚   â”œâ”€â”€ agent_2_findings.md
â”‚   â””â”€â”€ synthesis_notes.md
â””â”€â”€ appendices/
    â”œâ”€â”€ methodology.md
    â”œâ”€â”€ limitations.md
    â””â”€â”€ future_research.md
```

**Deliverable:** Complete research package in RESEARCH/[topic_name]/ directory

---

## Multi-Agent Deployment Strategy

### Overview

Deploy multiple Task agents in parallel to maximize research efficiency and coverage. This approach mirrors how a research team would divide work among specialists.

### Agent Deployment Protocol

**Step 1: Create Research Plan**

- Break down main question into specific subtopics
- Assign one agent per subtopic/research angle
- Define clear task boundaries to minimize redundancy

**Step 2: Launch Parallel Agents**
Use multiple Task tool invocations in a single response. Each agent receives:

- Clear description of their research focus
- Specific instructions on what to find
- Expected output format with citation requirements
- List of tools to use (WebSearch, WebFetch, Puppeteer)

**Step 3: Coordinate Results**
After agents complete their tasks:

- Compile findings from all agents
- Identify overlaps and contradictions
- Synthesize into coherent narrative
- Maintain source attribution from each agent

### Agent Prompt Templates

#### General Research Agent Template

```
Research [specific aspect] of [main topic]. Use the following tools:
1. Start with WebSearch to find relevant sources
2. Use WebFetch to extract content from promising URLs
3. If sites require JavaScript, use mcp__puppeteer__puppeteer_navigate and screenshot

Focus on finding:
- Recent information (prioritize last 2 years)
- Authoritative sources
- Specific data/statistics
- Multiple perspectives

For every factual claim, provide:
1. Direct quote or specific data point
2. Author/organization name
3. Publication year
4. Full title
5. Direct URL/DOI
6. Confidence rating (High/Medium/Low)

Never make claims without sources. If uncertain, state 'Source needed' rather than guessing.

Provide a structured summary with all source URLs.
```

#### Technical Research Agent Template

```
Find technical/academic information about [topic aspect].

Tools to use:
1. WebSearch for academic papers and technical resources
2. WebFetch for PDF extraction and content analysis
3. mcp__filesystem__ tools to save important findings

Look for:
- Peer-reviewed papers
- Technical specifications
- Methodologies and frameworks
- Scientific evidence

Include proper academic citations with DOI/PMID when available.
```

#### Verification Agent Template

```
Verify the following claims about [topic]:
[List key claims to verify]

Use multiple search queries with WebSearch to find:
- Supporting evidence
- Contradicting information
- Original sources

Rate confidence: High/Medium/Low for each claim.
Explain any contradictions found.
Cross-reference at least 2 independent sources for critical claims.
```

### Best Practices

1. **Clear Task Boundaries**: Each agent should have distinct, non-overlapping focus
2. **Comprehensive Prompts**: Include all necessary context and citation requirements
3. **Parallel Execution**: Launch all agents in one response for maximum efficiency
4. **Result Integration**: Plan synthesis strategy before launching agents
5. **Quality Control**: Always include at least one verification agent

### Example Multi-Agent Deployment

When researching "AI in Healthcare", deploy agents as follows:

**Agent 1**: "Research current AI applications in healthcare - focus on clinical diagnosis and treatment"
**Agent 2**: "Find challenges and ethical concerns in medical AI - regulatory and privacy issues"
**Agent 3**: "Investigate future AI healthcare innovations - emerging technologies and predictions"
**Agent 4**: "Gather case studies of successful AI healthcare implementations - ROI and outcomes"
**Agent 5**: "Cross-reference and verify key statistics about AI healthcare impact - validate claims"

---

## Graph of Thoughts Implementation

### Core GoT Implementation

When deep research is requested, deploy a GoT Controller that maintains graph state and orchestrates transformations:

#### GoT Execution Loop

```
repeat until DONE {
    1. Select frontier thoughts with Ranker R (top-3 highest scoring)
    2. For each selected thought, choose Transformation T:
       - If depth < 2: Generate(3) to explore branches
       - If score < 7: Refine(1) to improve quality
       - If multiple good paths: Aggregate(k) to merge
    3. Deploy transformation agents and await results
    4. Update graph with new nodes, edges, and scores
    5. Prune: KeepBestN(5) at each depth level
    6. Exit when max_score > 9 or depth > 4
}
```

### Transformation Agent Templates

#### Generate Agent Template

```
Task: "GoT Generate - Node [ID] Branch [k]"

You are Generate transformation creating branch [k] from parent thought:
"[PARENT_THOUGHT]"

Your specific exploration angle: [ANGLE]
- Angle 1: Current state and evidence
- Angle 2: Challenges and limitations
- Angle 3: Future implications

Execute:
1. WebSearch for "[TOPIC] [ANGLE]" - find 5 sources
2. Score each source quality (1-10)
3. WebFetch top 3 sources
4. Synthesize findings into coherent thought (200-400 words)
5. Self-score your thought (0-10) based on:
   - Claim accuracy
   - Citation density
   - Novel insights
   - Coherence

Return:
{
  "thought": "your synthesized findings with inline citations",
  "score": float,
  "sources": ["url1", "url2", "url3"],
  "operation": "Generate",
  "parent": "[PARENT_ID]"
}
```

#### Aggregate Agent Template

```
Task: "GoT Aggregate - Nodes [IDs]"

You are Aggregate transformation combining these [k] thoughts:

[THOUGHT_1]
Score: [SCORE_1]

[THOUGHT_2]
Score: [SCORE_2]

Combine into ONE stronger unified thought that:
- Preserves all important claims
- Resolves contradictions
- Maintains all citations
- Achieves higher quality than any input

Self-score the result (0-10).

Return:
{
  "thought": "aggregated synthesis",
  "score": float,
  "operation": "Aggregate",
  "parents": [parent_ids]
}
```

#### Refine Agent Template

```
Task: "GoT Refine - Node [ID]"

You are Refine transformation improving this thought:
"[CURRENT_THOUGHT]"
Current score: [SCORE]

Improve by:
1. Fact-check claims using WebSearch
2. Add missing context/nuance
3. Strengthen weak arguments
4. Fix citation issues
5. Enhance clarity

Do NOT add new major points - only refine existing content.

Self-score improvement (0-10).

Return refined thought with updated score.
```

### Complete GoT Research Example

**User Request:** "Deep research CRISPR gene editing safety"

**Iteration 1: Initialize and Explore**

1. Controller Agent creates root node: "Research CRISPR gene editing safety"
2. Generate(3) deploys 3 parallel agents exploring:
   - Current evidence and success rates
   - Safety concerns and limitations
   - Future implications and regulations
3. Results: 3 thoughts with scores (6.8, 8.2, 7.5)
4. Graph state saved with frontier = [n3(8.2), n2(7.5), n4(6.8)]

**Iteration 2: Deepen Best Paths**

1. Controller examines frontier, decides:
   - n3 (8.2): High score â†’ Generate(3) for deeper exploration
   - n2 (7.5): Medium â†’ Generate(2)
   - n4 (6.8): Low â†’ Refine(1) to improve
2. 6 agents deployed in parallel
3. Best result: "High-fidelity SpCas9 variants reduce off-targets by 95%" (Score: 9.1)

**Iteration 3: Aggregate Strong Branches**

1. Controller sees multiple high scores
2. Aggregate(3) merges best thoughts into comprehensive synthesis
3. Score: 9.3 - exceeds threshold

**Iteration 4: Final Polish**

1. Refine(1) enhances clarity and completeness
2. Final thought scores 9.5
3. Output: Best path through graph becomes research report

**What Makes This True GoT:**

- Graph maintained throughout with nodes, edges, scores
- Multiple paths explored in parallel
- Pruning drops weak branches
- Scoring guides exploration vs exploitation
- Optimal solution found through graph traversal

---

## Citation Requirements & Source Traceability

### Mandatory Citation Standards

**Every factual claim must include:**

1. **Author/Organization** - Who made this claim
2. **Date** - When the information was published
3. **Source Title** - Name of paper, article, or report
4. **URL/DOI** - Direct link to verify the source
5. **Page Numbers** - For lengthy documents (when applicable)

### Citation Formats

**Academic Papers:**

```
(Author et al., Year, p. XX) with full citation in references
Example: (Smith et al., 2023, p. 145)
Full: Smith, J., Johnson, K., & Lee, M. (2023). "Title of Paper." Journal Name, 45(3), 140-156. https://doi.org/10.xxxx/xxxxx
```

**Web Sources:**

```
(Organization, Year, Section Title)
Example: (NIH, 2024, "Treatment Guidelines")
Full: National Institutes of Health. (2024). "Treatment Guidelines for Metabolic Syndrome." Retrieved [date] from https://www.nih.gov/specific-page
```

**Direct Quotes:**

```
"Exact quote from source" (Author, Year, p. XX)
```

### Source Quality Ratings

Rate every source using this A-E scale:

- **A**: Peer-reviewed RCTs, systematic reviews, meta-analyses
- **B**: Cohort studies, case-control studies, clinical guidelines, reputable analysts
- **C**: Expert opinion, case reports, mechanistic studies
- **D**: Preliminary research, preprints, conference abstracts, blogs
- **E**: Anecdotal, theoretical, or speculative

### Source Verification Protocol

1. **Primary Sources Only** - Link to original research, not secondary reporting
2. **Archive Links** - For time-sensitive content, include archive.org links
3. **Multiple Confirmations** - Critical claims need 2+ independent sources
4. **Conflicting Data** - Note when sources disagree and explain discrepancies
5. **Recency Check** - Prioritize sources from last 2 years when relevant

### Traceability Requirements

**For Medical/Health Information:**

- PubMed ID (PMID) when available
- Clinical trial registration numbers
- FDA/regulatory body references
- Version/update dates for guidelines

**For Genetic Information:**

- dbSNP rs numbers
- Gene database links (OMIM, GeneCards)
- Population frequency sources (gnomAD, 1000 Genomes)
- Effect size sources with confidence intervals

**For Statistical Claims:**

- Sample sizes
- P-values and confidence intervals
- Statistical methods used
- Data availability statements

### Source Documentation Structure

Each research output must include:

1. **Inline Citations** - Throughout the text
2. **References Section** - Full bibliography at end
3. **Source Quality Table** - Rating each source A-E
4. **Verification Checklist** - Confirming each claim is sourced
5. **Data Availability** - Where raw data can be accessed

### Example Implementation

**Poor Citation:**
"Studies show that metformin reduces diabetes risk."

**Proper Citation:**
"The Diabetes Prevention Program demonstrated that metformin reduces diabetes incidence by 31% over 2.8 years in high-risk individuals (Knowler et al., 2002, NEJM, PMID: 11832527, <https://doi.org/10.1056/NEJMoa012512>)"

### Red Flags for Unreliable Sources

- No author attribution
- Missing publication dates
- Broken or suspicious URLs
- Claims without data
- Conflicts of interest not disclosed
- Predatory journals
- Retracted papers (check RetractionWatch)

---

## Advanced Research Methodologies

### Chain-of-Verification (CoVe)

To prevent hallucinations:

1. Generate initial research findings
2. Create verification questions for each claim
3. Search for evidence to answer verification questions
4. Revise findings based on verification results
5. Repeat until all claims are verified

### Chain-of-Density (CoD) Summarization

When processing sources, use iterative refinement to increase information density:

1. First pass: Extract key points (low density)
2. Second pass: Add supporting details and context
3. Third pass: Compress while preserving all critical information
4. Final pass: Maximum density with all essential facts and citations

### ReAct Pattern (Reason + Act)

Agents should follow this loop:

1. **Reason**: Analyze what information is needed
2. **Act**: Execute search or retrieval action
3. **Observe**: Process the results
4. **Reason**: Determine if more information needed
5. **Repeat**: Continue until sufficient evidence gathered

### Multi-Agent Orchestration Roles

For complex topics, deploy specialized agents:

- **Planner Agent**: Decomposes research into subtopics
- **Search Agents**: Execute queries and retrieve sources
- **Synthesis Agents**: Combine findings from multiple sources
- **Critic Agents**: Verify claims and check for errors
- **Editor Agent**: Polishes final output

### Human-in-the-Loop Checkpoints

Critical intervention points:

1. **After Planning**: Approve research strategy
2. **During Verification**: Expert review of technical claims
3. **Before Finalization**: Stakeholder sign-off
4. **Post-Delivery**: Feedback incorporation

---

## Implementation Tools

### Core Tools

1. **WebSearch**: Built-in web search capability for finding relevant sources
2. **WebFetch**: For extracting and analyzing content from specific URLs
3. **Read/Write**: For managing research documents locally
4. **Task**: For spawning autonomous agents for complex multi-step operations
5. **TodoWrite**: For tracking research progress

### MCP Server Tools

1. **mcp__filesystem__**: File system operations (read, write, search files)
2. **mcp__puppeteer__**: Browser automation for dynamic web content
   - Navigate to pages requiring JavaScript
   - Take screenshots of web content
   - Extract data from interactive websites
   - Fill forms and interact with web elements

### Web Research Strategy

- **Primary**: Use WebSearch tool for general web searches
- **Secondary**: Use WebFetch for extracting content from specific URLs
- **Advanced**: Use mcp__puppeteer__ for sites requiring interaction or JavaScript rendering
- **Note**: When MCP web fetch tools become available, prefer them over WebFetch

### Tool Usage Instructions

**WebSearch Usage:**

```
Use WebSearch with specific queries:
- Include key terms in quotes for exact matches
- Use domain filtering for authoritative sources
- Try multiple query variations
```

**WebFetch Usage:**

```
After WebSearch identifies URLs:
1. Use WebFetch with targeted prompts
2. Ask for specific information extraction
3. Request summaries of long content
```

**Puppeteer MCP Usage:**

```
For JavaScript-heavy sites:
1. mcp__puppeteer__puppeteer_navigate to URL
2. mcp__puppeteer__puppeteer_screenshot for visual content
3. mcp__puppeteer__puppeteer_evaluate to extract dynamic data
```

---

## Mitigation Strategies

### Hallucination Prevention

- Always ground statements in source material
- Use Chain-of-Verification for critical claims
- Cross-reference multiple sources
- Explicitly state uncertainty when appropriate
- Never make claims without sources - state "Source needed" if uncertain

### Coverage Optimization

- Use diverse search queries
- Check multiple perspectives
- Include recent sources (check dates)
- Acknowledge limitations and gaps
- Search across different source types (academic, industry, news)

### Citation Management

- Track source URLs and access dates
- Quote relevant passages verbatim when needed
- Maintain source-to-statement mapping
- Use consistent citation format
- Create bibliography as research progresses

---

## User Interaction Protocol

### Initial Question Gathering Phase

When a user requests deep research, engage in structured dialogue to gather all necessary information before beginning research.

### Required Information Checklist

Before starting research, clarify:

**1. Core Research Question**

- Main topic or question to investigate
- Specific aspects or angles of interest
- What problem are you trying to solve?

**2. Output Requirements**

- Desired format (report, presentation, analysis, etc.)
- Length expectations (executive summary vs comprehensive report)
- File structure preferences (single document vs folder with multiple files)
- Visual requirements (charts, graphs, diagrams, images)

**3. Scope & Boundaries**

- Geographic focus (global, specific countries/regions)
- Time period (current, historical, future projections)
- Industry or domain constraints
- What should be excluded from research?

**4. Sources & Credibility**

- Preferred source types (academic, industry, news, etc.)
- Any sources to prioritize or avoid
- Required credibility level (peer-reviewed only, industry reports ok, etc.)

**5. Deliverable Structure**

- Folder organization preferences
- Naming conventions for files
- Whether to include:
  - Raw research notes
  - Source PDFs/screenshots
  - Data files (CSV, JSON)
  - Visualization source files

**6. Special Requirements**

- Specific data or statistics needed
- Comparison frameworks to use
- Regulatory or compliance considerations
- Target audience for the research

### Question Templates

**1. Topic Clarification**

- "What specific aspects of [topic] are most important for your needs?"
- "Are you looking for current state analysis, historical trends, or future predictions?"

**2. Output Specification**

- "Would you prefer a single comprehensive report or multiple focused documents?"
- "Do you need visualizations? If so, what types would be most helpful?"

**3. Scope Definition**

- "Are there any geographic regions or time periods I should focus on?"
- "What level of technical detail is appropriate for your audience?"

**4. Source Preferences**

- "Do you have any preferred sources or databases I should prioritize?"
- "Are there any sources or viewpoints I should avoid?"

**5. Delivery Format**

- "How would you like the files organized?"
- "Do you need the raw research data or just the final analysis?"

### Research Plan Approval

Before executing research:

1. Present subtopic breakdown
2. Show agent deployment strategy
3. Describe expected output structure
4. Get user approval to proceed

---

## Key Principles of Deep Research

### Iterative Refinement

Deep research is not linear - it's a continuous loop of:

1. **Search**: Find relevant information
2. **Read**: Extract key insights
3. **Refine**: Generate new queries based on findings
4. **Verify**: Cross-check claims across sources
5. **Synthesize**: Combine into coherent narrative
6. **Repeat**: Continue until comprehensive coverage

### Why This Outperforms Manual Research

- **Breadth**: AI can process 20+ sources in minutes vs days for humans
- **Depth**: Multi-step reasoning uncovers non-obvious connections
- **Consistency**: Systematic approach ensures no gaps
- **Traceability**: Every claim linked to source
- **Efficiency**: Handles low-level tasks, freeing humans for analysis
- **Parallel Processing**: Multiple agents work simultaneously

### State Management

Throughout the research process, maintain:

- Current research questions
- Sources visited and their quality scores
- Extracted claims and verification status
- Graph state (for GoT implementation)
- Progress tracking against original plan
- Agent findings and synthesis notes

---

## Ready to Begin

This methodology provides everything needed for Graph of Thoughts deep research:

- **Self-contained** - No external files or dependencies required
- **Automatic execution** - Deploys immediately when you request research
- **True GoT implementation** - Graph state, scoring, pruning, and optimization
- **Uses available tools** - WebSearch, WebFetch, Task agents, Puppeteer
- **Transparent process** - Saves graph states and execution traces
- **Rigorous quality** - Citation validation and verification protocols

### To Start Deep Research

Simply say: **"Deep research [your topic]"**

**The system will:**

1. Ask clarifying questions if needed
2. Deploy a GoT Controller to manage the graph
3. Launch transformation agents (Generate, Refine, Aggregate)
4. Explore multiple research paths with scoring
5. Deliver the optimal research findings with complete citations

**No Python setup, no API keys, no external frameworks needed** - everything runs using the Task agent system to implement proper Graph of Thoughts reasoning.

---

*For quick reference, see [CLAUDE.md](CLAUDE.md). For system architecture details, see [ARCHITECTURE.md](ARCHITECTURE.md).*
</file>

</files>
