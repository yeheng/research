---
name: research-executor
description: æ‰§è¡Œå®Œæ•´çš„ 7 é˜¶æ®µæ·±åº¦ç ”ç©¶æµç¨‹ã€‚æŽ¥æ”¶ç»“æž„åŒ–ç ”ç©¶ä»»åŠ¡ï¼Œè‡ªåŠ¨éƒ¨ç½²å¤šä¸ªå¹¶è¡Œç ”ç©¶æ™ºèƒ½ä½“ï¼Œç”Ÿæˆå¸¦å®Œæ•´å¼•ç”¨çš„ç»¼åˆç ”ç©¶æŠ¥å‘Šã€‚å½“ç”¨æˆ·æœ‰ç»“æž„åŒ–çš„ç ”ç©¶æç¤ºè¯æ—¶ä½¿ç”¨æ­¤æŠ€èƒ½ã€‚
---

# Research Executor

## Overview

The Research Executor conducts comprehensive, multi-phase research using the 7-stage deep research methodology and Graph of Thoughts (GoT) framework.

## When to Use

- User provides a structured research prompt (from question-refiner)
- Need to execute systematic research with multiple agents
- Require comprehensive report with verified citations
- Research involves 3+ subtopics requiring parallel investigation

## Core Capabilities

1. **7-Phase Research Process**: Question scoping â†’ Planning â†’ Querying â†’ Triangulation â†’ Synthesis â†’ QA â†’ Output
2. **Multi-Agent Deployment**: 3-8 parallel research agents based on complexity
3. **Citation Management**: A-E source quality ratings, 100% citation coverage
4. **GoT Integration**: Optional Graph of Thoughts for complex topics

## Quick Start

```markdown
Execute research on: [structured research prompt]

The executor will:
1. Verify prompt completeness
2. Create research plan with subtopics
3. Deploy parallel agents (web, academic, verification)
4. Triangulate sources and validate claims
5. Synthesize findings with inline citations
6. Generate structured output in RESEARCH/[topic]/
```

## Output Structure

```
RESEARCH/[topic]/
â”œâ”€â”€ README.md
â”œâ”€â”€ executive_summary.md
â”œâ”€â”€ full_report.md
â”œâ”€â”€ data/
â”œâ”€â”€ sources/
â””â”€â”€ appendices/
```

## Key Features

- **Task Complexity Assessment**: Automatic agent count and model selection
- **Parallel Execution**: All agents launch simultaneously
- **Source Quality Control**: A-E rating system
- **Hallucination Prevention**: Chain-of-Verification process

## âš ï¸ Critical: Token Optimization

> ðŸ“‹ **Reference**: `.claude/shared/constants/token_optimization.md`

**ALWAYS use the Download â†’ Clean â†’ Read pipeline:**

1. WebFetch â†’ Save to `data/raw/[source].html`
2. Preprocess â†’ Clean to `data/processed/[source].md`
3. Read from processed file (60-90% token savings)

**FORBIDDEN**: Direct WebFetch content in context for pages >5KB

**Per-Agent Budget**: 15k tokens max
- 5k for instructions
- 10k for source content (processed only)

## Agent Communication

> ðŸ“‹ **Reference**: `.claude/shared/constants/agent_communication.md`

**Before fetching URLs**: Check `data/url_manifest.json` for cached sources

**Progress tracking**: Update `research_notes/agent_status.json` every 5 minutes

**Deduplication**: Register findings in `research_notes/findings_registry.json`

## Error Handling

> ðŸ“‹ **Reference**: `.claude/shared/constants/error_codes.md`

**Common Errors**:
- **E101**: Web fetch timeout â†’ Retry once, then skip
- **E201**: Token limit exceeded â†’ Use preprocessing pipeline
- **E203**: Citation validation failed â†’ Add missing citations (mandatory)
- **E301**: Agent spawn failed â†’ Reduce agent count and retry

**Quality Threshold**: Output must score â‰¥8.0 or trigger refinement (max 2 attempts)

## Shared Resources

> ðŸ“‹ **Source Ratings**: `.claude/shared/constants/source_quality_ratings.md`
> ðŸ“‹ **Report Templates**: `.claude/shared/templates/report_structure.md`
> ðŸ“‹ **Citation Format**: `.claude/shared/templates/citation_format.md`

## Safety Limits

| Limit | Value |
|-------|-------|
| Max parallel agents | 8 |
| Max research time | 90 minutes |
| Min quality score | 8.0 to pass |
| Max token per source | 10,000 |

## Examples

See [examples.md](./examples.md) for detailed usage scenarios.

## Detailed Instructions

See [instructions.md](./instructions.md) for complete implementation guide.
